import cv2
import os
from ultralytics import YOLO

# --- 1. –ù–ê–õ–ê–®–¢–£–í–ê–ù–ù–Ø ---
# –®–ª—è—Ö –¥–æ –≤–∞—à–æ—ó –Ω–∞—Ç—Ä–µ–Ω–æ–≤–∞–Ω–æ—ó –º–æ–¥–µ–ª—ñ YOLO
MODEL_PATH = r"C:\Users\serhi\OneDrive\CD_DSST\Article_syntetic_data\Trained models\Zala-Supercum-detector-YOLO8n-640-480-2000-samples-40Epochs-with-117-Hard-Negatives_and_DeFocus.pt"
# –ö–∞—Ç–∞–ª–æ–≥ –∑ –≤—ñ–¥–µ–æ –¥–ª—è –∞–Ω–∞–ª—ñ–∑—É
VIDEO_DIR = r"C:\Users\serhi\OneDrive\CD_DSST\Article_syntetic_data\Data_for_tests\Video_interceptors\src"
# –í–∏—Ö—ñ–¥–Ω–∏–π –∫–∞—Ç–∞–ª–æ–≥ –¥–ª—è –∑–±–µ—Ä–µ–∂–µ–Ω–Ω—è –Ω–µ–≥–∞—Ç–∏–≤–Ω–∏—Ö –ø—Ä–∏–∫–ª–∞–¥—ñ–≤
OUTPUT_DIR = r"C:\Users\serhi\OneDrive\CD_DSST\Article_syntetic_data\Data_for_tests\Hard_Negatives"
# –§—ñ–ª—å—Ç—Ä –¥–ª—è —ñ–º–µ–Ω –≤—ñ–¥–µ–æ—Ñ–∞–π–ª—ñ–≤ (–Ω–∞–ø—Ä–∏–∫–ª–∞–¥, "Drone" –∞–±–æ "*" –¥–ª—è –≤—Å—ñ—Ö)
FILENAME_FILTER = "*"
CROP_SIZE = (640, 480)
FRAME_STEP = 40

crop_center = None
WINDOW_NAME = "Hard Negative Mining Tool"
# –ö–æ–¥–∏ –∫–ª–∞–≤—ñ—à
KEY_ENTER = 13
KEY_SPACE = 32
KEY_ESC = 27
KEY_Q = ord('q')

def mouse_callback(event, x, y, flags, param):
    """–û–±—Ä–æ–±–ª—è—î –∫–ª—ñ–∫–∏ –º–∏—à—ñ, –∑–±–µ—Ä—ñ–≥–∞—é—á–∏ —Ü–µ–Ω—Ç—Ä –¥–ª—è –º–∞–π–±—É—Ç–Ω—å–æ—ó –æ–±—Ä—ñ–∑–∫–∏."""
    global crop_center
    if event == cv2.EVENT_LBUTTONDOWN:
        crop_center = (x, y)
        print(f"–û–±—Ä–∞–Ω–æ –Ω–æ–≤–∏–π —Ü–µ–Ω—Ç—Ä –¥–ª—è –≤–∏—Ä—ñ–∑–∞–Ω–Ω—è: {crop_center}")

def save_negative_sample(frame, rect, output_dir, counter):
    """–ó–±–µ—Ä—ñ–≥–∞—î –≤–∏—Ä—ñ–∑–∞–Ω—É –æ–±–ª–∞—Å—Ç—å —Ç–∞ —Å—Ç–≤–æ—Ä—é—î –ø–æ—Ä–æ–∂–Ω—ñ–π —Ñ–∞–π–ª —Ä–æ–∑–º—ñ—Ç–∫–∏."""
    x1, y1, x2, y2 = rect
    cropped_image = frame[y1:y2, x1:x2]

    base_filename = f"Negative_{counter}"
    image_path = os.path.join(output_dir, f"{base_filename}.jpg")
    label_path = os.path.join(output_dir, f"{base_filename}.txt")

    cv2.imwrite(image_path, cropped_image)
    with open(label_path, 'w') as f:
        pass

    print(f"‚úÖ –ó–±–µ—Ä–µ–∂–µ–Ω–æ: {image_path} —Ç–∞ {label_path}")
    return counter + 1

def find_video_files(video_dir, filename_filter):
    """–ó–Ω–∞—Ö–æ–¥–∏—Ç—å –≤—ñ–¥–µ–æ—Ñ–∞–π–ª–∏ —É –≤–∫–∞–∑–∞–Ω–æ–º—É –∫–∞—Ç–∞–ª–æ–∑—ñ, —â–æ –≤—ñ–¥–ø–æ–≤—ñ–¥–∞—é—Ç—å —Ñ—ñ–ª—å—Ç—Ä—É."""
    supported_formats = ['.mp4', '.avi', '.mov', '.mkv']
    video_files = []
    print(f"üîç –ü–æ—à—É–∫ –≤—ñ–¥–µ–æ —É '{video_dir}'...")
    if not os.path.isdir(video_dir):
        print(f"‚ùå –ö–∞—Ç–∞–ª–æ–≥ –Ω–µ –∑–Ω–∞–π–¥–µ–Ω–æ: {video_dir}")
        return []
        
    for f in os.listdir(video_dir):
        name_matches = (filename_filter == '*') or (filename_filter in f)
        
        if os.path.splitext(f)[1].lower() in supported_formats and name_matches:
            video_files.append(os.path.join(video_dir, f))
            
    print(f"–ó–Ω–∞–π–¥–µ–Ω–æ {len(video_files)} –≤—ñ–¥–µ–æ—Ñ–∞–π–ª—ñ–≤.")
    return video_files

def get_start_counter(output_dir):
    """–í–∏–∑–Ω–∞—á–∞—î –ø–æ—á–∞—Ç–∫–æ–≤–∏–π –Ω–æ–º–µ—Ä –¥–ª—è —ñ–º–µ–Ω—É–≤–∞–Ω–Ω—è —Ñ–∞–π–ª—ñ–≤, —â–æ–± —É–Ω–∏–∫–Ω—É—Ç–∏ –ø–µ—Ä–µ–∑–∞–ø–∏—Å—É."""
    os.makedirs(output_dir, exist_ok=True)
    existing_files = os.listdir(output_dir)
    max_num = 0
    if existing_files:
        for f in existing_files:
            if f.startswith("Negative_") and f.endswith(".jpg"):
                try:
                    num = int(f.replace("Negative_", "").replace(".jpg", ""))
                    if num > max_num:
                        max_num = num
                except ValueError:
                    continue
    start_counter = max_num + 1
    print(f"–ü—Ä–æ–¥–æ–≤–∂—É—î–º–æ –Ω—É–º–µ—Ä–∞—Ü—ñ—é –∑ {start_counter}")
    return start_counter

def initialize_app():
    """–ó–∞–≤–∞–Ω—Ç–∞–∂—É—î –º–æ–¥–µ–ª—å —Ç–∞ –Ω–∞–ª–∞—à—Ç–æ–≤—É—î –≤—ñ–∫–Ω–æ OpenCV."""
    print("–ó–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è –º–æ–¥–µ–ª—ñ YOLO...")
    try:
        model = YOLO(MODEL_PATH)
        print("‚úÖ –ú–æ–¥–µ–ª—å —É—Å–ø—ñ—à–Ω–æ –∑–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–∞.")
    except Exception as e:
        print(f"‚ùå –ü–æ–º–∏–ª–∫–∞ –∑–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è –º–æ–¥–µ–ª—ñ: {e}")
        return None
    
    cv2.namedWindow(WINDOW_NAME)
    cv2.setMouseCallback(WINDOW_NAME, mouse_callback)
    return model

def handle_frame_interaction(original_frame, annotated_frame, counter):
    """–û–±—Ä–æ–±–ª—è—î –≤–∑–∞—î–º–æ–¥—ñ—é –∫–æ—Ä–∏—Å—Ç—É–≤–∞—á–∞ –∑ –æ–¥–Ω–∏–º –∫–∞–¥—Ä–æ–º (–∫–ª—ñ–∫–∏, –Ω–∞—Ç–∏—Å–∫–∞–Ω–Ω—è –∫–ª–∞–≤—ñ—à)."""
    global crop_center
    
    while True:
        display_frame = annotated_frame.copy()
        rect_to_save = None

        if crop_center:
            h, w, _ = display_frame.shape
            crop_w, crop_h = CROP_SIZE
            x1 = max(0, crop_center[0] - crop_w // 2)
            y1 = max(0, crop_center[1] - crop_h // 2)
            x2 = min(w, x1 + crop_w)
            y2 = min(h, y1 + crop_h)
            if x2 == w: x1 = w - crop_w
            if y2 == h: y1 = h - crop_h
            rect_to_save = (x1, y1, x2, y2)
            
            cv2.rectangle(display_frame, (x1, y1), (x2, y2), (0, 255, 255), 2)
            cv2.putText(display_frame, "Selected Area", (x1, y1 - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 255), 2)

        cv2.imshow(WINDOW_NAME, display_frame)
        key = cv2.waitKey(1) & 0xFF

        if key == KEY_ENTER:
            if rect_to_save:
                counter = save_negative_sample(original_frame, rect_to_save, OUTPUT_DIR, counter)
                crop_center = None
                return counter, "next_frame" 
            else:
                print("‚ö†Ô∏è –°–ø–æ—á–∞—Ç–∫—É –∫–ª—ñ–∫–Ω—ñ—Ç—å –º–∏—à–µ—é, —â–æ–± –æ–±—Ä–∞—Ç–∏ –æ–±–ª–∞—Å—Ç—å!")
        elif key == KEY_SPACE:
            crop_center = None
            return counter, "next_frame"
        elif key in [KEY_Q, KEY_ESC]:
            return counter, "quit"

def process_video(video_path, model, counter):
    """–û–±—Ä–æ–±–ª—è—î –æ–¥–∏–Ω –≤—ñ–¥–µ–æ—Ñ–∞–π–ª –∫–∞–¥—Ä –∑–∞ –∫–∞–¥—Ä–æ–º."""
    global crop_center
    print(f"\n‚ñ∂Ô∏è –û–±—Ä–æ–±–∫–∞ –≤—ñ–¥–µ–æ: {video_path}")
    cap = cv2.VideoCapture(video_path)
    if not cap.isOpened():
        print(f"‚ö†Ô∏è –ù–µ –≤–¥–∞–ª–æ—Å—è –≤—ñ–¥–∫—Ä–∏—Ç–∏ –≤—ñ–¥–µ–æ: {video_path}")
        return counter, False

    frame_idx = 0
    while True:
        cap.set(cv2.CAP_PROP_POS_FRAMES, frame_idx)
        success, frame = cap.read()
        if not success:
            print("üèÅ –ó–∞–≤–µ—Ä—à–µ–Ω–æ –æ–±—Ä–æ–±–∫—É –≤—ñ–¥–µ–æ.")
            break

        print(f"--- –û–±—Ä–æ–±–∫–∞ –∫–∞–¥—Ä—É {frame_idx} ---")
        results = model(frame, verbose=False)
        annotated_frame = results[0].plot()

        counter, status = handle_frame_interaction(frame, annotated_frame, counter)
        
        if status == "next_frame":
            frame_idx += FRAME_STEP
        elif status == "quit":
            cap.release()
            return counter, True

    cap.release()
    return counter, False

def main():
    """–ì–æ–ª–æ–≤–Ω–∞ —Ç–æ—á–∫–∞ –≤—Ö–æ–¥—É. –ö–µ—Ä—É—î –∑–∞–≥–∞–ª—å–Ω–∏–º –ø—Ä–æ—Ü–µ—Å–æ–º."""
    model = initialize_app()
    if not model:
        return

    video_files = find_video_files(VIDEO_DIR, FILENAME_FILTER)
    if not video_files:
        return
        
    negative_counter = get_start_counter(OUTPUT_DIR)

    should_quit = False
    for video_path in video_files:
        negative_counter, should_quit = process_video(video_path, model, negative_counter)
        if should_quit:
            break
    
    if should_quit:
        print("‚èπÔ∏è –í–∏—Ö—ñ–¥ –∑ –ø—Ä–æ–≥—Ä–∞–º–∏.")
    else:
        print("\nüéâ –í—Å—ñ –≤—ñ–¥–µ–æ—Ñ–∞–π–ª–∏ –æ–±—Ä–æ–±–ª–µ–Ω–æ.")
        
    cv2.destroyAllWindows()

if __name__ == "__main__":
    main()