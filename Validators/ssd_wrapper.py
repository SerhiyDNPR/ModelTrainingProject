# Validators/ssd_wrapper.py

import torch
import torchvision
from torchvision.transforms import functional as F
from torchvision.models.detection.anchor_utils import DefaultBoxGenerator
from torchvision.models.detection.ssd import SSDClassificationHead, SSDRegressionHead
from torchvision.ops import Conv2dNormActivation
# ---------------------------------------------------------------
from sahi import AutoDetectionModel
from sahi.predict import get_sliced_prediction

from .model_wrapper import ModelWrapper
from .prediction import Prediction

class SSDWrapper(ModelWrapper):
    """
    –£–Ω—ñ–≤–µ—Ä—Å–∞–ª—å–Ω–∞ –æ–±–≥–æ—Ä—Ç–∫–∞ –¥–ª—è –º–æ–¥–µ–ª–µ–π SSD –∑ –æ–ø—Ü—ñ–æ–Ω–∞–ª—å–Ω–æ—é –ø—ñ–¥—Ç—Ä–∏–º–∫–æ—é SAHI.
    –ê–¥–∞–ø—Ç–æ–≤–∞–Ω–æ –¥–ª—è –∑–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è –º–æ–¥–µ–ª–µ–π, –Ω–∞–≤—á–µ–Ω–∏—Ö –∑–∞ –¥–æ–ø–æ–º–æ–≥–æ—é SSDTrainer.
    """

    def _select_backbone(self):
        """
        –í—ñ–¥–æ–±—Ä–∞–∂–∞—î –º–µ–Ω—é –≤–∏–±–æ—Ä—É backbone —ñ –ø–æ–≤–µ—Ä—Ç–∞—î –≤–∏–±—ñ—Ä –∫–æ—Ä–∏—Å—Ç—É–≤–∞—á–∞.
        """
        print("\n–ë—É–¥—å –ª–∞—Å–∫–∞, –æ–±–µ—Ä—ñ—Ç—å '—Ö—Ä–µ–±–µ—Ç' (backbone) –¥–ª—è –º–æ–¥–µ–ª—ñ SSD, —â–æ –∑–∞–≤–∞–Ω—Ç–∞–∂—É—î—Ç—å—Å—è:")
        print("  1: VGG16 (–¥–ª—è —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–æ—ó –º–æ–¥–µ–ª—ñ SSD300)")
        print("  2: MobileNetV3-Large (–¥–ª—è –º–æ–¥–µ–ª—ñ SSDLite320)")
        
        while True:
            choice = input("–í–∞—à –≤–∏–±—ñ—Ä (1 –∞–±–æ 2): ").strip()
            if choice == '1':
                print("‚úÖ –û–±—Ä–∞–Ω–æ –∞—Ä—Ö—ñ—Ç–µ–∫—Ç—É—Ä—É –Ω–∞ –±–∞–∑—ñ VGG16.")
                return 'vgg16'
            elif choice == '2':
                print("‚úÖ –û–±—Ä–∞–Ω–æ –∞—Ä—Ö—ñ—Ç–µ–∫—Ç—É—Ä—É –Ω–∞ –±–∞–∑—ñ MobileNetV3-Large.")
                return 'mobilenet'
            else:
                print("‚ùå –ù–µ–≤—ñ—Ä–Ω–∏–π –≤–∏–±—ñ—Ä. –ë—É–¥—å –ª–∞—Å–∫–∞, –≤–≤–µ–¥—ñ—Ç—å 1 –∞–±–æ 2.")

    def _build_model(self, backbone_type, num_classes):
        """
        –°—Ç–≤–æ—Ä—é—î –µ–∫–∑–µ–º–ø–ª—è—Ä –º–æ–¥–µ–ª—ñ –∑ –ø—Ä–∞–≤–∏–ª—å–Ω–æ—é –∞—Ä—Ö—ñ—Ç–µ–∫—Ç—É—Ä–æ—é "–≥–æ–ª–æ–≤–∏",
        –∞–Ω–∞–ª–æ–≥—ñ—á–Ω–æ –¥–æ —Ç–æ–≥–æ, —è–∫ —Ü–µ —Ä–æ–±–∏—Ç—å—Å—è –≤ SSDTrainer.
        """
        print("üîß –°—Ç–≤–æ—Ä–µ–Ω–Ω—è –∞—Ä—Ö—ñ—Ç–µ–∫—Ç—É—Ä–∏ –º–æ–¥–µ–ª—ñ –¥–ª—è –∑–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è –≤–∞–≥...")
        # 1. –ó–∞–≤–∞–Ω—Ç–∞–∂—É—î–º–æ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—É –º–æ–¥–µ–ª—å –±–µ–∑ –ø–µ—Ä–µ–¥-–Ω–∞–≤—á–µ–Ω–∏—Ö –≤–∞–≥
        if backbone_type == 'vgg16':
            model = torchvision.models.detection.ssd300_vgg16(weights=None, num_classes=num_classes)
        else: # mobilenet
            model = torchvision.models.detection.ssdlite320_mobilenet_v3_large(weights=None, num_classes=num_classes)

        # --- ‚ÄºÔ∏è –í–ê–ñ–õ–ò–í–û: –¢–£–¢  –ú–ê–Ñ –ë–£–¢–ò –¢–ê–ö–ò–ô –ñ–ï –ì–ï–ù–ï–†–ê–¢–û–† –Ø–ö–û–†–Ü–í –Ø–ö –ü–†–ò –ù–ê–í–ß–ê–ù–ù–Ü –ú–û–î–ï–õ–Ü ‚ÄºÔ∏è ---
        model.anchor_generator = DefaultBoxGenerator(
            [
                # –ö–∞—Ä—Ç–∞ –æ–∑–Ω–∞–∫ 1 (–¥–ª—è –Ω–∞–π–º–µ–Ω—à–∏—Ö –æ–±'—î–∫—Ç—ñ–≤)
                # –ü–æ–∫—Ä–∏–≤–∞—î —Ä–æ–∑–º—ñ—Ä–∏ ~28-64 –ø—ñ–∫—Å–µ–ª—ñ–≤
                [0.045, 0.07, 0.1],
                
                # –ö–∞—Ä—Ç–∞ –æ–∑–Ω–∞–∫ 2 
                # –ü–æ–∫—Ä–∏–≤–∞—î —Ä–æ–∑–º—ñ—Ä–∏ ~64-160 –ø—ñ–∫—Å–µ–ª—ñ–≤
                [0.1, 0.18, 0.25],
                
                # –ö–∞—Ä—Ç–∞ –æ–∑–Ω–∞–∫ 3 (–¥–ª—è —Å–µ—Ä–µ–¥–Ω—ñ—Ö –æ–±'—î–∫—Ç—ñ–≤)
                # –ü–æ–∫—Ä–∏–≤–∞—î —Ä–æ–∑–º—ñ—Ä–∏ ~160-320 –ø—ñ–∫—Å–µ–ª—ñ–≤
                [0.25, 0.4, 0.5],
                
                # –ö–∞—Ä—Ç–∞ –æ–∑–Ω–∞–∫ 4
                # –ü–æ–∫—Ä–∏–≤–∞—î —Ä–æ–∑–º—ñ—Ä–∏ ~320-450 –ø—ñ–∫—Å–µ–ª—ñ–≤
                [0.5, 0.6, 0.7],
                
                # –ö–∞—Ä—Ç–∞ –æ–∑–Ω–∞–∫ 5 (–¥–ª—è –≤–µ–ª–∏–∫–∏—Ö –æ–±'—î–∫—Ç—ñ–≤)
                # –ü–æ–∫—Ä–∏–≤–∞—î —Ä–æ–∑–º—ñ—Ä–∏ ~450-575 –ø—ñ–∫—Å–µ–ª—ñ–≤
                [0.7, 0.8, 0.9],
                
                # –ö–∞—Ä—Ç–∞ –æ–∑–Ω–∞–∫ 6 (–¥–ª—è –Ω–∞–π–±—ñ–ª—å—à–∏—Ö –æ–±'—î–∫—Ç—ñ–≤)
                # –ü–æ–∫—Ä–∏–≤–∞—î —Ä–æ–∑–º—ñ—Ä–∏ ~575-608 –ø—ñ–∫—Å–µ–ª—ñ–≤
                [0.9, 0.93, 0.95] 
            ]
        )
        # -------------------------------------------------------------------------
        
        # 2. –í–∏—Ç—è–≥—É—î–º–æ –ø–∞—Ä–∞–º–µ—Ç—Ä–∏ –∑—ñ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–æ—ó –º–æ–¥–µ–ª—ñ –¥–ª—è –ø–æ–±—É–¥–æ–≤–∏ –Ω–æ–≤–æ—ó "–≥–æ–ª–æ–≤–∏"
        in_channels = []
        for layer in model.head.classification_head.module_list:
            if isinstance(layer, torch.nn.Sequential) and isinstance(layer[0], ConvdNormActivation):
                in_channels.append(layer[0][0].in_channels)
            else:
                in_channels.append(layer.in_channels)
        
        num_anchors = model.anchor_generator.num_anchors_per_location()
        
        # 3. –°—Ç–≤–æ—Ä—é—î–º–æ —Ç–∞ –≤—Å—Ç–∞–Ω–æ–≤–ª—é—î–º–æ –Ω–æ–≤—ñ –∫–ª–∞—Å–∏—Ñ—ñ–∫–∞—Ü—ñ–π–Ω—É —Ç–∞ —Ä–µ–≥—Ä–µ—Å—ñ–π–Ω—É –≥–æ–ª–æ–≤–∏
        model.head.classification_head = SSDClassificationHead(in_channels, num_anchors, num_classes)
        model.head.regression_head = SSDRegressionHead(in_channels, num_anchors)
        
        return model

    def load(self, model_path, use_sahi=False):
        self.use_sahi = use_sahi
        try:
            backbone_type = self._select_backbone()
            # –ö—ñ–ª—å–∫—ñ—Å—Ç—å –∫–ª–∞—Å—ñ–≤ –¥–ª—è "–≥–æ–ª–æ–≤–∏" = –∫—ñ–ª—å–∫—ñ—Å—Ç—å –æ–±'—î–∫—Ç—ñ–≤ + 1 (—Ñ–æ–Ω)
            num_classes_for_head = len(self.class_names) + 1
            
            # –°—Ç–≤–æ—Ä—é—î–º–æ –º–æ–¥–µ–ª—å –∑ –ø—Ä–∞–≤–∏–ª—å–Ω–æ—é –∞—Ä—Ö—ñ—Ç–µ–∫—Ç—É—Ä–æ—é
            model_instance = self._build_model(backbone_type, num_classes_for_head)
            
            # –ó–∞–≤–∞–Ω—Ç–∞–∂—É—î–º–æ –≤–∞–≥–∏ –∑ —á–µ–∫–ø–æ—ñ–Ω—Ç–∞
            state_dict = torch.load(model_path, map_location=self.device).get('model_state_dict', torch.load(model_path, map_location=self.device))
            model_instance.load_state_dict(state_dict)
            model_instance = model_instance.to(self.device).eval()

            print(f"DEBUG: –¢–∏–ø AutoDetectionModel: {AutoDetectionModel}")

            if self.use_sahi:
                print("‚ú® SAHI slicing ENABLED. –Ü–Ω—ñ—Ü—ñ–∞–ª—ñ–∑–∞—Ü—ñ—è AutoDetectionModel –∑ –≥–æ—Ç–æ–≤–æ—é –º–æ–¥–µ–ª–ª—é...")
                self.model = AutoDetectionModel(
                    model=model_instance,
                    model_type='torchvision',
                    device=self.device,
                    class_names=self.class_names,
                )
            else:
                print("‚ú® SAHI slicing DISABLED. –í–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î—Ç—å—Å—è —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–∞ –º–æ–¥–µ–ª—å.")
                self.model = model_instance

            print(f"‚úÖ –ú–æ–¥–µ–ª—å SSD ({backbone_type.upper()}) '{model_path}' —É—Å–ø—ñ—à–Ω–æ –∑–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–∞.")
        except Exception as e:
            print(f"‚ùå –ü–æ–º–∏–ª–∫–∞ –∑–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è –º–æ–¥–µ–ª—ñ SSD: {e}")
            raise

    def predict(self, frame, conf_threshold):
        if hasattr(self, 'use_sahi') and self.use_sahi:
            # –õ–æ–≥—ñ–∫–∞ –ø–µ—Ä–µ–¥–±–∞—á–µ–Ω–Ω—è –∑ –Ω–∞—Ä—ñ–∑–∫–æ—é SAHI
            self.model.confidence_threshold = conf_threshold
            
            result = get_sliced_prediction(
                frame,
                detection_model=self.model,
                slice_height=512,
                slice_width=512,
                overlap_height_ratio=0.2,
                overlap_width_ratio=0.2,
            )
            
            predictions = []
            for pred in result.object_prediction_list:
                box = pred.bbox.to_xyxy()
                score = pred.score.value
                # SAHI –∑–∞ –∑–∞–º–æ–≤—á—É–≤–∞–Ω–Ω—è–º –Ω—É–º–µ—Ä—É—î –∫–ª–∞—Å–∏ –∑ 0, —â–æ –Ω–∞–º —ñ –ø–æ—Ç—Ä—ñ–±–Ω–æ
                class_id = pred.category.id
                class_name = pred.category.name
                
                predictions.append(
                    Prediction(box, score, class_id, class_name, track_id=None)
                )
            return predictions

        else:
            # –°—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–∞ –ª–æ–≥—ñ–∫–∞ –ø–µ—Ä–µ–¥–±–∞—á–µ–Ω–Ω—è –±–µ–∑ –Ω–∞—Ä—ñ–∑–∫–∏
            predictions = []
            # –ö–æ–Ω–≤–µ—Ä—Ç–∞—Ü—ñ—è BGR (OpenCV) -> RGB -> Tensor
            rgb_frame = frame[:, :, ::-1].copy()
            tensor_frame = F.to_tensor(rgb_frame).to(self.device)
            
            with torch.no_grad():
                results = self.model([tensor_frame])[0]

            for box, label, score in zip(results["boxes"], results["labels"], results["scores"]):
                if score.item() >= conf_threshold:
                    # –ú–æ–¥–µ–ª—å –ø–æ–≤–µ—Ä—Ç–∞—î –º—ñ—Ç–∫–∏ –≤—ñ–¥ 1 –¥–æ N. –ù–∞–º –ø–æ—Ç—Ä—ñ–±–Ω—ñ —ñ–Ω–¥–µ–∫—Å–∏ –≤—ñ–¥ 0 –¥–æ N-1.
                    class_id = label.item() - 1 
                    if 0 <= class_id < len(self.class_names):
                        class_name = self.class_names[class_id]
                        predictions.append(
                            Prediction(box.cpu().numpy(), score.item(), class_id, class_name, track_id=None)
                        )
            return predictions