# Validators/ssd_wrapper.py

import torch
import os
import torchvision
from torchvision.transforms import functional as F
from torchvision.models.detection.anchor_utils import DefaultBoxGenerator
from torchvision.models.detection.ssd import SSDClassificationHead, SSDRegressionHead
from torchvision.ops import Conv2dNormActivation
# ---------------------------------------------------------------
from sahi import AutoDetectionModel
from sahi.predict import get_sliced_prediction

from .model_wrapper import ModelWrapper
from .prediction import Prediction

class SSDWrapper(ModelWrapper):
    """
    –£–Ω—ñ–≤–µ—Ä—Å–∞–ª—å–Ω–∞ –æ–±–≥–æ—Ä—Ç–∫–∞ –¥–ª—è –º–æ–¥–µ–ª–µ–π SSD –∑ –æ–ø—Ü—ñ–æ–Ω–∞–ª—å–Ω–æ—é –ø—ñ–¥—Ç—Ä–∏–º–∫–æ—é SAHI.
    –ê–¥–∞–ø—Ç–æ–≤–∞–Ω–æ –¥–ª—è –∑–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è –º–æ–¥–µ–ª–µ–π, –Ω–∞–≤—á–µ–Ω–∏—Ö –∑–∞ –¥–æ–ø–æ–º–æ–≥–æ—é SSDTrainer.
    """

    def _select_backbone(self):
        """
        –í—ñ–¥–æ–±—Ä–∞–∂–∞—î –º–µ–Ω—é –≤–∏–±–æ—Ä—É backbone —ñ –ø–æ–≤–µ—Ä—Ç–∞—î –≤–∏–±—ñ—Ä –∫–æ—Ä–∏—Å—Ç—É–≤–∞—á–∞.
        """
        print("\n–ë—É–¥—å –ª–∞—Å–∫–∞, –æ–±–µ—Ä—ñ—Ç—å '—Ö—Ä–µ–±–µ—Ç' (backbone) –¥–ª—è –º–æ–¥–µ–ª—ñ SSD, —â–æ –∑–∞–≤–∞–Ω—Ç–∞–∂—É—î—Ç—å—Å—è:")
        print("  1: VGG16 (–¥–ª—è —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–æ—ó –º–æ–¥–µ–ª—ñ SSD300)")
        print("  2: MobileNetV3-Large (–¥–ª—è –º–æ–¥–µ–ª—ñ SSDLite320)")
        
        while True:
            choice = input("–í–∞—à –≤–∏–±—ñ—Ä (1 –∞–±–æ 2): ").strip()
            if choice == '1':
                print("‚úÖ –û–±—Ä–∞–Ω–æ –∞—Ä—Ö—ñ—Ç–µ–∫—Ç—É—Ä—É –Ω–∞ –±–∞–∑—ñ VGG16.")
                return 'vgg16'
            elif choice == '2':
                print("‚úÖ –û–±—Ä–∞–Ω–æ –∞—Ä—Ö—ñ—Ç–µ–∫—Ç—É—Ä—É –Ω–∞ –±–∞–∑—ñ MobileNetV3-Large.")
                return 'mobilenet'
            else:
                print("‚ùå –ù–µ–≤—ñ—Ä–Ω–∏–π –≤–∏–±—ñ—Ä. –ë—É–¥—å –ª–∞—Å–∫–∞, –≤–≤–µ–¥—ñ—Ç—å 1 –∞–±–æ 2.")

    def _build_model(self, backbone_type, num_classes):
        """
        –°—Ç–≤–æ—Ä—é—î –µ–∫–∑–µ–º–ø–ª—è—Ä –º–æ–¥–µ–ª—ñ –∑ –ø—Ä–∞–≤–∏–ª—å–Ω–æ—é –∞—Ä—Ö—ñ—Ç–µ–∫—Ç—É—Ä–æ—é "–≥–æ–ª–æ–≤–∏",
        –∞–Ω–∞–ª–æ–≥—ñ—á–Ω–æ –¥–æ —Ç–æ–≥–æ, —è–∫ —Ü–µ —Ä–æ–±–∏—Ç—å—Å—è –≤ SSDTrainer.
        """
        print("üîß –°—Ç–≤–æ—Ä–µ–Ω–Ω—è –∞—Ä—Ö—ñ—Ç–µ–∫—Ç—É—Ä–∏ –º–æ–¥–µ–ª—ñ –¥–ª—è –∑–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è –≤–∞–≥...")
        if backbone_type == 'vgg16':
            model = torchvision.models.detection.ssd300_vgg16(weights=None, num_classes=num_classes)
        else: # mobilenet
            model = torchvision.models.detection.ssdlite320_mobilenet_v3_large(weights=None, num_classes=num_classes)

        # --- –í–ê–ñ–õ–ò–í–û: –ì–µ–Ω–µ—Ä–∞—Ç–æ—Ä —è–∫–æ—Ä—ñ–≤ –º–∞—î –±—É—Ç–∏ —ñ–¥–µ–Ω—Ç–∏—á–Ω–∏–º —Ç–æ–º—É, —â–æ –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É–≤–∞–≤—Å—è –ø—Ä–∏ –Ω–∞–≤—á–∞–Ω–Ω—ñ ---
        model.anchor_generator = DefaultBoxGenerator(
            [
                [0.045, 0.07, 0.1], [0.1, 0.18, 0.25], [0.25, 0.4, 0.5],
                [0.5, 0.6, 0.7], [0.7, 0.8, 0.9], [0.9, 0.93, 0.95] 
            ]
        )
        
        in_channels = []
        for layer in model.head.classification_head.module_list:
            if isinstance(layer, torch.nn.Sequential) and isinstance(layer[0], Conv2dNormActivation):
                in_channels.append(layer[0][0].in_channels)
            else:
                in_channels.append(layer.in_channels)
        
        num_anchors = model.anchor_generator.num_anchors_per_location()
        
        model.head.classification_head = SSDClassificationHead(in_channels, num_anchors, num_classes)
        model.head.regression_head = SSDRegressionHead(in_channels, num_anchors)
        
        return model

    def load(self, model_path, use_sahi=False):
        self.use_sahi = use_sahi
        try:
            backbone_type = self._select_backbone()
            num_classes_for_head = len(self.class_names) + 1
            
            # 1. –°—Ç–≤–æ—Ä—é—î–º–æ –∞—Ä—Ö—ñ—Ç–µ–∫—Ç—É—Ä—É –º–æ–¥–µ–ª—ñ
            model_instance = self._build_model(backbone_type, num_classes_for_head)
            
            # 2. –ó–∞–≤–∞–Ω—Ç–∞–∂—É—î–º–æ –≤–∞–≥–∏ –∑ —á–µ–∫–ø–æ—ñ–Ω—Ç–∞
            state_dict = torch.load(model_path, map_location=self.device).get('model_state_dict', torch.load(model_path, map_location=self.device))
            model_instance.load_state_dict(state_dict)
            model_instance = model_instance.to(self.device).eval()

            # --- –û–°–¨ –¢–£–¢ –ë–£–õ–ê –ü–û–ú–ò–õ–ö–ê ---
            if self.use_sahi:
                print("‚ú® SAHI slicing ENABLED. –Ü–Ω—ñ—Ü—ñ–∞–ª—ñ–∑–∞—Ü—ñ—è AutoDetectionModel –∑ –≥–æ—Ç–æ–≤–æ—é –º–æ–¥–µ–ª–ª—é...")
                # –ü–†–ê–í–ò–õ–¨–ù–û: –ø–µ—Ä–µ–¥–∞—î–º–æ –≥–æ—Ç–æ–≤–∏–π –æ–±'—î–∫—Ç –º–æ–¥–µ–ª—ñ, –∞ –Ω–µ —à–ª—è—Ö
                self.model = AutoDetectionModel.from_pretrained(
                    model_type='torchvision',  # <--- –ö–õ–Æ–ß–û–í–ò–ô –î–û–î–ê–ù–ò–ô –†–Ø–î–û–ö
                    model=model_instance,
                    category_mapping={i: name for i, name in enumerate(self.class_names)},
                    device=self.device,
                )
            else:
                print("‚ú® SAHI slicing DISABLED. –í–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î—Ç—å—Å—è —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–∞ –º–æ–¥–µ–ª—å.")
                self.model = model_instance

            print(f"‚úÖ –ú–æ–¥–µ–ª—å SSD ({backbone_type.upper()}) '{os.path.basename(model_path)}' —É—Å–ø—ñ—à–Ω–æ –∑–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–∞.")
        except Exception as e:
            print(f"‚ùå –ü–æ–º–∏–ª–∫–∞ –∑–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è –º–æ–¥–µ–ª—ñ SSD: {e}")
            raise

    def predict(self, frame, conf_threshold):
        if hasattr(self, 'use_sahi') and self.use_sahi:
            # –õ–æ–≥—ñ–∫–∞ –ø–µ—Ä–µ–¥–±–∞—á–µ–Ω–Ω—è –∑ –Ω–∞—Ä—ñ–∑–∫–æ—é SAHI
            self.model.confidence_threshold = conf_threshold
            
            result = get_sliced_prediction(
                frame,
                detection_model=self.model,
                slice_height=512,
                slice_width=512,
                overlap_height_ratio=0.2,
                overlap_width_ratio=0.2,
            )
            
            predictions = []
            for pred in result.object_prediction_list:
                box = pred.bbox.to_xyxy()
                score = pred.score.value
                class_id = pred.category.id
                class_name = pred.category.name
                
                predictions.append(
                    Prediction(box, score, class_id, class_name, track_id=None)
                )
            return predictions

        else:
            # –°—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–∞ –ª–æ–≥—ñ–∫–∞ –ø–µ—Ä–µ–¥–±–∞—á–µ–Ω–Ω—è –±–µ–∑ –Ω–∞—Ä—ñ–∑–∫–∏
            predictions = []
            rgb_frame = frame[:, :, ::-1].copy()
            tensor_frame = F.to_tensor(rgb_frame).to(self.device)
            
            with torch.no_grad():
                results = self.model([tensor_frame])[0]

            for box, label, score in zip(results["boxes"], results["labels"], results["scores"]):
                if score.item() >= conf_threshold:
                    class_id = label.item() - 1 
                    if 0 <= class_id < len(self.class_names):
                        class_name = self.class_names[class_id]
                        predictions.append(
                            Prediction(box.cpu().numpy(), score.item(), class_id, class_name, track_id=None)
                        )
            return predictions