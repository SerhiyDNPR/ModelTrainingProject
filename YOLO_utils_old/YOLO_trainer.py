import os
import shutil
import json
import re
from glob import glob
from pathlib import Path
from sklearn.model_selection import train_test_split
from tqdm import tqdm

def natural_sort_key(s):
    """–ö–ª—é—á –¥–ª—è –ø—Ä–∏—Ä–æ–¥–Ω–æ–≥–æ —Å–æ—Ä—Ç—É–≤–∞–Ω–Ω—è (solo_1, solo_2, solo_10)."""
    s = s.name
    return [int(text) if text.isdigit() else text.lower() for text in re.split('([0-9]+)', s)]

def discover_classes(annotated_dirs):
    """–°–∫–∞–Ω—É—î –≤—Å—ñ JSON-—Ñ–∞–π–ª–∏ –¥–ª—è –≤–∏—è–≤–ª–µ–Ω–Ω—è —É–Ω—ñ–∫–∞–ª—å–Ω–∏—Ö –Ω–∞–∑–≤ –∫–ª–∞—Å—ñ–≤."""
    print("üîç –°–∫–∞–Ω—É–≤–∞–Ω–Ω—è –∫–ª–∞—Å—ñ–≤ —É –≤–∏—Ö—ñ–¥–Ω–∏—Ö –¥–∞–Ω–∏—Ö Perception...")
    class_names = set()
    for directory in tqdm(annotated_dirs, desc="–ü–æ—à—É–∫ –∫–ª–∞—Å—ñ–≤", unit="–ø–∞–ø–∫–∞"):
        json_files = [p.parent / "step0.frame_data.json" for p in directory.glob("sequence.*/step0.camera.png") if (p.parent / "step0.frame_data.json").exists()]
        for frame_file in json_files:
            with open(frame_file) as f:
                frame_data = json.load(f)
            capture = frame_data.get("capture") or frame_data.get("captures", [{}])[0]
            annotations_list = frame_data.get("annotations", capture.get("annotations", []))
            for annotation in annotations_list:
                if "BoundingBox2DAnnotation" in annotation.get("@type", ""):
                    for value in annotation.get("values", []):
                        label_name = value.get("label_name") or value.get("labelName")
                        if label_name:
                            class_names.add(label_name)
    sorted_names = sorted(list(class_names))
    print(f"‚úÖ –ó–Ω–∞–π–¥–µ–Ω–æ {len(sorted_names)} —É–Ω—ñ–∫–∞–ª—å–Ω–∏—Ö –∫–ª–∞—Å—ñ–≤: {sorted_names}")
    return sorted_names

def convert_and_prepare_data(perception_source_dir, final_dataset_dir):
    """
    –ö–æ–Ω–≤–µ—Ä—Ç—É—î –¥–∞–Ω—ñ –∑ —Ñ–æ—Ä–º–∞—Ç—É Perception –Ω–∞–ø—Ä—è–º—É –≤ —Ñ—ñ–Ω–∞–ª—å–Ω—É —Å—Ç—Ä—É–∫—Ç—É—Ä—É YoloDataset,
    —Ä–æ–∑–ø–æ–¥—ñ–ª—è—é—á–∏ –Ω–∞ train, val, —ñ test.
    """
    print("\n--- –†–æ–∑–ø–æ—á–∞—Ç–æ –ø—Ä–æ—Ü–µ—Å –∫–æ–Ω–≤–µ—Ä—Ç–∞—Ü—ñ—ó —Ç–∞ –ø—ñ–¥–≥–æ—Ç–æ–≤–∫–∏ –¥–∞–Ω–∏—Ö ---")
    
    if os.path.exists(final_dataset_dir):
        print(f"üßπ –û—á–∏—â–µ–Ω–Ω—è —ñ—Å–Ω—É—é—á–æ—ó –¥–∏—Ä–µ–∫—Ç–æ—Ä—ñ—ó: {final_dataset_dir}")
        shutil.rmtree(final_dataset_dir)

    base_input_path = Path(perception_source_dir)
    source_dirs = sorted([p for p in base_input_path.glob("solo*") if p.is_dir()], key=natural_sort_key)

    if not source_dirs:
        print(f"–ü–û–ú–ò–õ–ö–ê: –ù–µ –∑–Ω–∞–π–¥–µ–Ω–æ –∂–æ–¥–Ω–æ—ó –¥–∏—Ä–µ–∫—Ç–æ—Ä—ñ—ó 'solo*' –∑–∞ —à–ª—è—Ö–æ–º '{base_input_path}'")
        return None, None

    images_dir = Path(final_dataset_dir) / "images"
    labels_dir = Path(final_dataset_dir) / "labels"
    for subset in ["train", "val", "test"]:
        os.makedirs(images_dir / subset, exist_ok=True)
        os.makedirs(labels_dir / subset, exist_ok=True)

    annotated_dirs = source_dirs[:-1] if len(source_dirs) > 1 else source_dirs
    negative_dir = source_dirs[-1] if len(source_dirs) > 1 else None

    class_names = discover_classes(annotated_dirs)
    class_map = {name: i for i, name in enumerate(class_names)}

    positive_examples = []
    negative_examples = []
    imgsz = (640, 480)

    print("\nüîé –ó–±—ñ—Ä —Ç–∞ –∞–Ω–∞–ª—ñ–∑ —Ñ–∞–π–ª—ñ–≤ –∑ –∞–Ω–æ—Ç–∞—Ü—ñ—è–º–∏...")
    for directory in tqdm(annotated_dirs, desc="–ê–Ω–∞–ª—ñ–∑ –ø–æ–∑–∏—Ç–∏–≤–Ω–∏—Ö –ø—Ä–∏–∫–ª–∞–¥—ñ–≤", unit="–ø–∞–ø–∫–∞"):
        all_files = sorted(
            [(p.parent / "step0.frame_data.json", p.parent / "step0.camera.png")
             for p in directory.glob("sequence.*/step0.camera.png") if (p.parent / "step0.frame_data.json").exists()],
            key=lambda x: int(x[1].parent.name.split('.')[-1])
        )
        
        print(f"   -> –í –ø–∞–ø—Ü—ñ '{directory.name}' –∑–Ω–∞–π–¥–µ–Ω–æ {len(all_files)} —Ñ–∞–π–ª—ñ–≤ –∑ —Ä–æ–∑–º—ñ—Ç–∫–æ—é.")

        for json_path, img_path in all_files:
            with open(json_path) as f:
                frame_data = json.load(f)
            
            capture = frame_data.get("capture") or frame_data.get("captures", [{}])[0]
            if capture.get("dimension"):
                img_w, img_h = capture["dimension"]
                imgsz = (int(img_w), int(img_h))
            
            yolo_annotations = []
            annotations_list = frame_data.get("annotations", capture.get("annotations", []))
            for annotation in annotations_list:
                if "BoundingBox2DAnnotation" in annotation.get("@type", ""):
                    for value in annotation.get("values", []):
                        class_name = value.get("label_name") or value.get("labelName")
                        class_id = class_map.get(class_name)
                        if class_id is None: continue
                        
                        px_x, px_y = value["origin"]
                        px_w, px_h = value["dimension"]
                        x_center_norm = (px_x + px_w / 2) / img_w
                        y_center_norm = (px_y + px_h / 2) / img_h
                        width_norm = px_w / img_w
                        height_norm = px_h / img_h
                        yolo_annotations.append(f"{class_id} {x_center_norm:.6f} {y_center_norm:.6f} {width_norm:.6f} {height_norm:.6f}")
            
            if yolo_annotations:
                positive_examples.append({"img_path": img_path, "annotations": yolo_annotations})

    print(f"\n–ó–Ω–∞–π–¥–µ–Ω–æ {len(positive_examples)} –ø–æ–∑–∏—Ç–∏–≤–Ω–∏—Ö –ø—Ä–∏–∫–ª–∞–¥—ñ–≤ –∑ –∞–Ω–æ—Ç–∞—Ü—ñ—è–º–∏.")

    if negative_dir:
        print("üîé –ó–±—ñ—Ä —Ñ–∞–π–ª—ñ–≤ –∑ –Ω–µ–≥–∞—Ç–∏–≤–Ω–∏–º–∏ –ø—Ä–∏–∫–ª–∞–¥–∞–º–∏...")
        all_negative_files = [p for p in negative_dir.glob("sequence.*/step0.camera.png")]
        for img_path in tqdm(all_negative_files, desc="–ê–Ω–∞–ª—ñ–∑ –Ω–µ–≥–∞—Ç–∏–≤–Ω–∏—Ö –ø—Ä–∏–∫–ª–∞–¥—ñ–≤"):
            negative_examples.append({"img_path": img_path, "annotations": []})
        print(f"–ó–Ω–∞–π–¥–µ–Ω–æ {len(negative_examples)} –Ω–µ–≥–∞—Ç–∏–≤–Ω–∏—Ö –ø—Ä–∏–∫–ª–∞–¥—ñ–≤.")

    train_files, test_files = train_test_split(positive_examples, test_size=0.2, random_state=42)
    val_files, test_files = train_test_split(test_files, test_size=0.5, random_state=42)
    
    train_files.extend(negative_examples)
    
    splits = {"train": train_files, "val": val_files, "test": test_files}

    print("\nüì¶ –§–æ—Ä–º—É–≤–∞–Ω–Ω—è —Ñ—ñ–Ω–∞–ª—å–Ω–æ–≥–æ –¥–∞—Ç–∞—Å–µ—Ç—É...")
    for split_name, files in splits.items():
        img_dest_dir = images_dir / split_name
        lbl_dest_dir = labels_dir / split_name
        
        for item in tqdm(files, desc=f"–ö–æ–ø—ñ—é–≤–∞–Ω–Ω—è '{split_name}'", unit="file"):
            parent_folder_name = item['img_path'].parent.parent.name
            sequence_folder_name = item['img_path'].parent.name
            unique_base_name = f"{parent_folder_name}_{sequence_folder_name}"
            
            shutil.copy(item['img_path'], img_dest_dir / f"{unique_base_name}.png")

            label_filepath = lbl_dest_dir / f"{unique_base_name}.txt"
            with open(label_filepath, "w") as f_label:
                if item['annotations']:
                    f_label.write("\n".join(item['annotations']))
    
    # --- –î–û–î–ê–ù–û: –í–∏–≤–µ–¥–µ–Ω–Ω—è —Ñ—ñ–Ω–∞–ª—å–Ω–æ—ó —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ ---
    total_train = len(train_files)
    total_val = len(val_files)
    total_test = len(test_files)
    total_images = total_train + total_val + total_test
    print("\n--- ‚úÖ –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø—ñ—Å–ª—è –∫–æ–Ω–≤–µ—Ä—Ç–∞—Ü—ñ—ó ---")
    print(f"–¢—Ä–µ–Ω—É–≤–∞–ª—å–Ω–∞ –≤–∏–±—ñ—Ä–∫–∞: {total_train} –∑–æ–±—Ä–∞–∂–µ–Ω—å")
    print(f"–í–∞–ª—ñ–¥–∞—Ü—ñ–π–Ω–∞ –≤–∏–±—ñ—Ä–∫–∞: {total_val} –∑–æ–±—Ä–∞–∂–µ–Ω—å")
    print(f"–¢–µ—Å—Ç–æ–≤–∞ –≤–∏–±—ñ—Ä–∫–∞:    {total_test} –∑–æ–±—Ä–∞–∂–µ–Ω—å")
    print("-----------------------------------------")
    print(f"üéâ –ó–∞–≥–∞–ª—å–Ω–∞ –∫—ñ–ª—å–∫—ñ—Å—Ç—å –∑–æ–±—Ä–∞–∂–µ–Ω—å: {total_images}")
    # ----------------------------------------
    
    return imgsz, class_names

def check_for_unfinished_training():
    """–ü–µ—Ä–µ–≤—ñ—Ä—è—î –Ω–∞—è–≤–Ω—ñ—Å—Ç—å –Ω–µ–∑–∞–≤–µ—Ä—à–µ–Ω–æ–≥–æ –Ω–∞–≤—á–∞–Ω–Ω—è."""
    train_dirs = sorted(glob(os.path.join("runs", "detect", "train*")))
    if not train_dirs:
        return None
    last_train_dir = train_dirs[-1]
    last_model_path = os.path.join(last_train_dir, "weights", "last.pt")
    if os.path.exists(last_model_path):
        print(f"\n‚úÖ –í–∏—è–≤–ª–µ–Ω–æ –Ω–µ–∑–∞–≤–µ—Ä—à–µ–Ω–µ –Ω–∞–≤—á–∞–Ω–Ω—è: {last_train_dir}")
        answer = input("–ë–∞–∂–∞—î—Ç–µ –ø—Ä–æ–¥–æ–≤–∂–∏—Ç–∏ –Ω–∞–≤—á–∞–Ω–Ω—è –∑ –æ—Å—Ç–∞–Ω–Ω—å–æ—ó —Ç–æ—á–∫–∏ –∑–±–µ—Ä–µ–∂–µ–Ω–Ω—è? (y/n): ").strip().lower()
        if answer not in ['y', 'Y', '–Ω', '–ù']:
            print(f"üöÄ –ù–∞–≤—á–∞–Ω–Ω—è –±—É–¥–µ –ø—Ä–æ–¥–æ–≤–∂–µ–Ω–æ –∑ —Ñ–∞–π–ª—É: {last_model_path}")
            return last_model_path
        else:
            print("üóëÔ∏è –ü–æ–ø–µ—Ä–µ–¥–Ω—ñ–π –ø—Ä–æ–≥—Ä–µ—Å –±—É–¥–µ –ø—Ä–æ—ñ–≥–Ω–æ—Ä–æ–≤–∞–Ω–æ. –ù–∞–≤—á–∞–Ω–Ω—è —Ä–æ–∑–ø–æ—á–Ω–µ—Ç—å—Å—è –∑ –Ω—É–ª—è.")
            return None
    return None