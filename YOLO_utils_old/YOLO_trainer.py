import datetime as dt
import os
import shutil
import torch
import cv2
import yaml
import json
import re
from glob import glob
from pathlib import Path
from sklearn.model_selection import train_test_split
from ultralytics import YOLO
from multiprocessing import freeze_support
from tqdm import tqdm
import tkinter as tk
from tkinter import filedialog

# --- –§–£–ù–ö–¶–Ü–á, –ü–ï–†–ï–ù–ï–°–ï–ù–Ü –ó –ö–û–ù–í–ï–†–¢–ï–†–ê ---

def natural_sort_key(s):
    """–ö–ª—é—á –¥–ª—è –ø—Ä–∏—Ä–æ–¥–Ω–æ–≥–æ —Å–æ—Ä—Ç—É–≤–∞–Ω–Ω—è (solo_1, solo_2, solo_10)."""
    s = s.name
    return [int(text) if text.isdigit() else text.lower() for text in re.split('([0-9]+)', s)]

def discover_classes(annotated_dirs):
    """–°–∫–∞–Ω—É—î –≤—Å—ñ JSON-—Ñ–∞–π–ª–∏ –¥–ª—è –≤–∏—è–≤–ª–µ–Ω–Ω—è —É–Ω—ñ–∫–∞–ª—å–Ω–∏—Ö –Ω–∞–∑–≤ –∫–ª–∞—Å—ñ–≤."""
    print("üîç –°–∫–∞–Ω—É–≤–∞–Ω–Ω—è –∫–ª–∞—Å—ñ–≤ —É –≤–∏—Ö—ñ–¥–Ω–∏—Ö –¥–∞–Ω–∏—Ö Perception...")
    class_names = set()
    for directory in tqdm(annotated_dirs, desc="–ü–æ—à—É–∫ –∫–ª–∞—Å—ñ–≤", unit="–ø–∞–ø–∫–∞"):
        json_files = [p.parent / "step0.frame_data.json" for p in directory.glob("sequence.*/step0.camera.png") if (p.parent / "step0.frame_data.json").exists()]
        for frame_file in json_files:
            with open(frame_file) as f:
                frame_data = json.load(f)
            capture = frame_data.get("capture") or frame_data.get("captures", [{}])[0]
            annotations_list = frame_data.get("annotations", capture.get("annotations", []))
            for annotation in annotations_list:
                if "BoundingBox2DAnnotation" in annotation.get("@type", ""):
                    for value in annotation.get("values", []):
                        label_name = value.get("label_name") or value.get("labelName")
                        if label_name:
                            class_names.add(label_name)
    sorted_names = sorted(list(class_names))
    print(f"‚úÖ –ó–Ω–∞–π–¥–µ–Ω–æ {len(sorted_names)} —É–Ω—ñ–∫–∞–ª—å–Ω–∏—Ö –∫–ª–∞—Å—ñ–≤: {sorted_names}")
    return sorted_names

def convert_and_prepare_data(perception_source_dir, final_dataset_dir):
    """
    –ö–æ–Ω–≤–µ—Ä—Ç—É—î –¥–∞–Ω—ñ –∑ —Ñ–æ—Ä–º–∞—Ç—É Perception –Ω–∞–ø—Ä—è–º—É –≤ —Ñ—ñ–Ω–∞–ª—å–Ω—É —Å—Ç—Ä—É–∫—Ç—É—Ä—É YoloDataset,
    —Ä–æ–∑–ø–æ–¥—ñ–ª—è—é—á–∏ –Ω–∞ train, val, —ñ test.
    """
    print("\n--- –†–æ–∑–ø–æ—á–∞—Ç–æ –ø—Ä–æ—Ü–µ—Å –∫–æ–Ω–≤–µ—Ä—Ç–∞—Ü—ñ—ó —Ç–∞ –ø—ñ–¥–≥–æ—Ç–æ–≤–∫–∏ –¥–∞–Ω–∏—Ö ---")
    
    if os.path.exists(final_dataset_dir):
        print(f"üßπ –û—á–∏—â–µ–Ω–Ω—è —ñ—Å–Ω—É—é—á–æ—ó –¥–∏—Ä–µ–∫—Ç–æ—Ä—ñ—ó: {final_dataset_dir}")
        shutil.rmtree(final_dataset_dir)

    base_input_path = Path(perception_source_dir)
    source_dirs = sorted([p for p in base_input_path.glob("solo*") if p.is_dir()], key=natural_sort_key)

    if not source_dirs:
        print(f"–ü–û–ú–ò–õ–ö–ê: –ù–µ –∑–Ω–∞–π–¥–µ–Ω–æ –∂–æ–¥–Ω–æ—ó –¥–∏—Ä–µ–∫—Ç–æ—Ä—ñ—ó 'solo*' –∑–∞ —à–ª—è—Ö–æ–º '{base_input_path}'")
        return None, None

    images_dir = Path(final_dataset_dir) / "images"
    labels_dir = Path(final_dataset_dir) / "labels"
    for subset in ["train", "val", "test"]:
        os.makedirs(images_dir / subset, exist_ok=True)
        os.makedirs(labels_dir / subset, exist_ok=True)

    annotated_dirs = source_dirs[:-1] if len(source_dirs) > 1 else source_dirs
    negative_dir = source_dirs[-1] if len(source_dirs) > 1 else None

    class_names = discover_classes(annotated_dirs)
    class_map = {name: i for i, name in enumerate(class_names)}

    positive_examples = []
    negative_examples = []
    imgsz = (640, 480)

    print("\nüîé –ó–±—ñ—Ä —Ç–∞ –∞–Ω–∞–ª—ñ–∑ —Ñ–∞–π–ª—ñ–≤ –∑ –∞–Ω–æ—Ç–∞—Ü—ñ—è–º–∏...")
    for directory in tqdm(annotated_dirs, desc="–ê–Ω–∞–ª—ñ–∑ –ø–æ–∑–∏—Ç–∏–≤–Ω–∏—Ö –ø—Ä–∏–∫–ª–∞–¥—ñ–≤", unit="–ø–∞–ø–∫–∞"):
        all_files = sorted(
            [(p.parent / "step0.frame_data.json", p.parent / "step0.camera.png")
             for p in directory.glob("sequence.*/step0.camera.png") if (p.parent / "step0.frame_data.json").exists()],
            key=lambda x: int(x[1].parent.name.split('.')[-1])
        )
        
        print(f"   -> –í –ø–∞–ø—Ü—ñ '{directory.name}' –∑–Ω–∞–π–¥–µ–Ω–æ {len(all_files)} —Ñ–∞–π–ª—ñ–≤ –∑ —Ä–æ–∑–º—ñ—Ç–∫–æ—é.")

        for json_path, img_path in all_files:
            with open(json_path) as f:
                frame_data = json.load(f)
            
            capture = frame_data.get("capture") or frame_data.get("captures", [{}])[0]
            if capture.get("dimension"):
                img_w, img_h = capture["dimension"]
                imgsz = (int(img_w), int(img_h))
            
            yolo_annotations = []
            annotations_list = frame_data.get("annotations", capture.get("annotations", []))
            for annotation in annotations_list:
                if "BoundingBox2DAnnotation" in annotation.get("@type", ""):
                    for value in annotation.get("values", []):
                        class_name = value.get("label_name") or value.get("labelName")
                        class_id = class_map.get(class_name)
                        if class_id is None: continue
                        
                        px_x, px_y = value["origin"]
                        px_w, px_h = value["dimension"]
                        x_center_norm = (px_x + px_w / 2) / img_w
                        y_center_norm = (px_y + px_h / 2) / img_h
                        width_norm = px_w / img_w
                        height_norm = px_h / img_h
                        yolo_annotations.append(f"{class_id} {x_center_norm:.6f} {y_center_norm:.6f} {width_norm:.6f} {height_norm:.6f}")
            
            if yolo_annotations:
                positive_examples.append({"img_path": img_path, "annotations": yolo_annotations})

    print(f"\n–ó–Ω–∞–π–¥–µ–Ω–æ {len(positive_examples)} –ø–æ–∑–∏—Ç–∏–≤–Ω–∏—Ö –ø—Ä–∏–∫–ª–∞–¥—ñ–≤ –∑ –∞–Ω–æ—Ç–∞—Ü—ñ—è–º–∏.")

    if negative_dir:
        print("üîé –ó–±—ñ—Ä —Ñ–∞–π–ª—ñ–≤ –∑ –Ω–µ–≥–∞—Ç–∏–≤–Ω–∏–º–∏ –ø—Ä–∏–∫–ª–∞–¥–∞–º–∏...")
        all_negative_files = [p for p in negative_dir.glob("sequence.*/step0.camera.png")]
        for img_path in tqdm(all_negative_files, desc="–ê–Ω–∞–ª—ñ–∑ –Ω–µ–≥–∞—Ç–∏–≤–Ω–∏—Ö –ø—Ä–∏–∫–ª–∞–¥—ñ–≤"):
            negative_examples.append({"img_path": img_path, "annotations": []})
        print(f"–ó–Ω–∞–π–¥–µ–Ω–æ {len(negative_examples)} –Ω–µ–≥–∞—Ç–∏–≤–Ω–∏—Ö –ø—Ä–∏–∫–ª–∞–¥—ñ–≤.")

    train_files, test_files = train_test_split(positive_examples, test_size=0.2, random_state=42)
    val_files, test_files = train_test_split(test_files, test_size=0.5, random_state=42)
    
    train_files.extend(negative_examples)
    
    splits = {"train": train_files, "val": val_files, "test": test_files}

    print("\nüì¶ –§–æ—Ä–º—É–≤–∞–Ω–Ω—è —Ñ—ñ–Ω–∞–ª—å–Ω–æ–≥–æ –¥–∞—Ç–∞—Å–µ—Ç—É...")
    for split_name, files in splits.items():
        img_dest_dir = images_dir / split_name
        lbl_dest_dir = labels_dir / split_name
        
        for item in tqdm(files, desc=f"–ö–æ–ø—ñ—é–≤–∞–Ω–Ω—è '{split_name}'", unit="file"):
            parent_folder_name = item['img_path'].parent.parent.name
            sequence_folder_name = item['img_path'].parent.name
            unique_base_name = f"{parent_folder_name}_{sequence_folder_name}"
            
            shutil.copy(item['img_path'], img_dest_dir / f"{unique_base_name}.png")

            label_filepath = lbl_dest_dir / f"{unique_base_name}.txt"
            with open(label_filepath, "w") as f_label:
                if item['annotations']:
                    f_label.write("\n".join(item['annotations']))
    
    # --- –î–û–î–ê–ù–û: –í–∏–≤–µ–¥–µ–Ω–Ω—è —Ñ—ñ–Ω–∞–ª—å–Ω–æ—ó —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ ---
    total_train = len(train_files)
    total_val = len(val_files)
    total_test = len(test_files)
    total_images = total_train + total_val + total_test
    print("\n--- ‚úÖ –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø—ñ—Å–ª—è –∫–æ–Ω–≤–µ—Ä—Ç–∞—Ü—ñ—ó ---")
    print(f"–¢—Ä–µ–Ω—É–≤–∞–ª—å–Ω–∞ –≤–∏–±—ñ—Ä–∫–∞: {total_train} –∑–æ–±—Ä–∞–∂–µ–Ω—å")
    print(f"–í–∞–ª—ñ–¥–∞—Ü—ñ–π–Ω–∞ –≤–∏–±—ñ—Ä–∫–∞: {total_val} –∑–æ–±—Ä–∞–∂–µ–Ω—å")
    print(f"–¢–µ—Å—Ç–æ–≤–∞ –≤–∏–±—ñ—Ä–∫–∞:    {total_test} –∑–æ–±—Ä–∞–∂–µ–Ω—å")
    print("-----------------------------------------")
    print(f"üéâ –ó–∞–≥–∞–ª—å–Ω–∞ –∫—ñ–ª—å–∫—ñ—Å—Ç—å –∑–æ–±—Ä–∞–∂–µ–Ω—å: {total_images}")
    # ----------------------------------------
    
    return imgsz, class_names

def check_for_unfinished_training():
    """–ü–µ—Ä–µ–≤—ñ—Ä—è—î –Ω–∞—è–≤–Ω—ñ—Å—Ç—å –Ω–µ–∑–∞–≤–µ—Ä—à–µ–Ω–æ–≥–æ –Ω–∞–≤—á–∞–Ω–Ω—è."""
    train_dirs = sorted(glob(os.path.join("runs", "detect", "train*")))
    if not train_dirs:
        return None
    last_train_dir = train_dirs[-1]
    last_model_path = os.path.join(last_train_dir, "weights", "last.pt")
    if os.path.exists(last_model_path):
        print(f"\n‚úÖ –í–∏—è–≤–ª–µ–Ω–æ –Ω–µ–∑–∞–≤–µ—Ä—à–µ–Ω–µ –Ω–∞–≤—á–∞–Ω–Ω—è: {last_train_dir}")
        answer = input("–ë–∞–∂–∞—î—Ç–µ –ø—Ä–æ–¥–æ–≤–∂–∏—Ç–∏ –Ω–∞–≤—á–∞–Ω–Ω—è –∑ –æ—Å—Ç–∞–Ω–Ω—å–æ—ó —Ç–æ—á–∫–∏ –∑–±–µ—Ä–µ–∂–µ–Ω–Ω—è? (y/n): ").strip().lower()
        if answer not in ['y', 'yes', '–Ω', '—Ç–∞–∫']:
            print(f"üöÄ –ù–∞–≤—á–∞–Ω–Ω—è –±—É–¥–µ –ø—Ä–æ–¥–æ–≤–∂–µ–Ω–æ –∑ —Ñ–∞–π–ª—É: {last_model_path}")
            return last_model_path
        else:
            print("üóëÔ∏è –ü–æ–ø–µ—Ä–µ–¥–Ω—ñ–π –ø—Ä–æ–≥—Ä–µ—Å –±—É–¥–µ –ø—Ä–æ—ñ–≥–Ω–æ—Ä–æ–≤–∞–Ω–æ. –ù–∞–≤—á–∞–Ω–Ω—è —Ä–æ–∑–ø–æ—á–Ω–µ—Ç—å—Å—è –∑ –Ω—É–ª—è.")
            return None
    return None

def add_hard_negatives(dataset_dir):
    """–î–æ–¥–∞—î '—Å–∫–ª–∞–¥–Ω—ñ –Ω–µ–≥–∞—Ç–∏–≤–Ω—ñ' –ø—Ä–∏–∫–ª–∞–¥–∏ –¥–æ —Ç—Ä–µ–Ω—É–≤–∞–ª—å–Ω–æ—ó –≤–∏–±—ñ—Ä–∫–∏."""
    answer = input("\n–ë–∞–∂–∞—î—Ç–µ –¥–æ–¥–∞—Ç–∏ Hard Negative –ø—Ä–∏–∫–ª–∞–¥–∏ –¥–æ –Ω–∞–≤—á–∞–ª—å–Ω–æ—ó –≤–∏–±—ñ—Ä–∫–∏? (y/n): ").strip().lower()
    if answer not in ['y', 'yes', '–Ω', '—Ç–∞–∫']:
        print("–ü—Ä–æ–ø—É—Å–∫–∞—î–º–æ –¥–æ–¥–∞–≤–∞–Ω–Ω—è Hard Negative –ø—Ä–∏–∫–ª–∞–¥—ñ–≤.")
        return

    root = tk.Tk()
    root.withdraw()
    print("–ë—É–¥—å –ª–∞—Å–∫–∞, –æ–±–µ—Ä—ñ—Ç—å –¥–∏—Ä–µ–∫—Ç–æ—Ä—ñ—é –∑ Hard Negative –ø—Ä–∏–∫–ª–∞–¥–∞–º–∏...")
    hard_negatives_dir = filedialog.askdirectory(title="–û–±–µ—Ä—ñ—Ç—å –¥–∏—Ä–µ–∫—Ç–æ—Ä—ñ—é –∑ Hard Negative –ø—Ä–∏–∫–ª–∞–¥–∞–º–∏")
    if not hard_negatives_dir:
        print("–î–∏—Ä–µ–∫—Ç–æ—Ä—ñ—é –Ω–µ –æ–±—Ä–∞–Ω–æ. –ü—Ä–æ–ø—É—Å–∫–∞—î–º–æ.")
        return

    train_images_dir = os.path.join(dataset_dir, "images", "train")
    train_labels_dir = os.path.join(dataset_dir, "labels", "train")
    hn_images = glob(os.path.join(hard_negatives_dir, "*.jpg")) + glob(os.path.join(hard_negatives_dir, "*.png"))
    if not hn_images:
        print("‚ö†Ô∏è –£ –≤–∫–∞–∑–∞–Ω—ñ–π –¥–∏—Ä–µ–∫—Ç–æ—Ä—ñ—ó –Ω–µ –∑–Ω–∞–π–¥–µ–Ω–æ –∑–æ–±—Ä–∞–∂–µ–Ω—å (.jpg –∞–±–æ .png).")
        return
        
    print(f"–ö–æ–ø—ñ—é–≤–∞–Ω–Ω—è {len(hn_images)} Hard Negative —Ñ–∞–π–ª—ñ–≤...")
    for img_path in tqdm(hn_images, desc="Hard Negatives", unit="file"):
        shutil.copy(img_path, os.path.join(train_images_dir, os.path.basename(img_path)))
        base_name, _ = os.path.splitext(os.path.basename(img_path))
        open(os.path.join(train_labels_dir, f"{base_name}.txt"), 'w').close()
    print(f"‚úÖ –£—Å–ø—ñ—à–Ω–æ –¥–æ–¥–∞–Ω–æ {len(hn_images)} —Ñ–∞–π–ª—ñ–≤ –¥–æ —Ç—Ä–µ–Ω—É–≤–∞–ª—å–Ω–æ—ó –≤–∏–±—ñ—Ä–∫–∏.")

if __name__ == '__main__':
    freeze_support()
    perception_source_dir = r"C:\Users\serhi\AppData\LocalLow\DefaultCompany\GenerateSynteticData"
    final_dataset_dir = "YoloDataset"

    do_conversion = input("–ë–∞–∂–∞—î—Ç–µ –∑–∞–ø—É—Å—Ç–∏—Ç–∏ –∫–æ–Ω–≤–µ—Ä—Ç–∞—Ü—ñ—é –¥–∞–Ω–∏—Ö –∑ Unity Perception? (y/n): ").strip().lower()
    if do_conversion in ['y', 'yes', '–Ω', '—Ç–∞–∫']:
        if not os.path.isdir(perception_source_dir):
            print(f"–ü–û–ú–ò–õ–ö–ê: –í–∫–∞–∑–∞–Ω–∏–π —à–ª—è—Ö –¥–æ –¥–∞–Ω–∏—Ö Perception '{perception_source_dir}' –Ω–µ —ñ—Å–Ω—É—î.")
            exit(1)
            
        imgsz, class_names = convert_and_prepare_data(perception_source_dir, final_dataset_dir)
        if imgsz is None:
            print("–ü–æ–º–∏–ª–∫–∞ –ø—ñ–¥ —á–∞—Å –ø—ñ–¥–≥–æ—Ç–æ–≤–∫–∏ –¥–∞–Ω–∏—Ö. –†–æ–±–æ—Ç—É –∑—É–ø–∏–Ω–µ–Ω–æ.")
            exit(1)

        add_hard_negatives(final_dataset_dir)

        print("\n–°—Ç–≤–æ—Ä–µ–Ω–Ω—è —Ñ–∞–π–ª—É yolo_config.yaml –¥–ª—è —Ç—Ä–µ–Ω—É–≤–∞–Ω–Ω—è...")
        with open("yolo_config.yaml", "w", encoding='utf-8') as f:
            f.write(f"path: {os.path.abspath(final_dataset_dir)}\n")
            f.write("train: images/train\nval: images/val\ntest: images/test\n\n")
            f.write(f"nc: {len(class_names)}\nnames: {class_names}\n")
    else:
        print("–ö–æ–Ω–≤–µ—Ä—Ç–∞—Ü—ñ—é –ø—Ä–æ–ø—É—â–µ–Ω–æ. –ü–µ—Ä–µ–≤—ñ—Ä–∫–∞ –Ω–∞—è–≤–Ω–æ—Å—Ç—ñ —ñ—Å–Ω—É—é—á–æ–≥–æ –¥–∞—Ç–∞—Å–µ—Ç—É...")
        if not os.path.exists(final_dataset_dir) or not os.path.exists("yolo_config.yaml"):
             print(f"ERROR: –ù–µ–º–æ–∂–ª–∏–≤–æ –ø—Ä–æ–¥–æ–≤–∂–∏—Ç–∏. –ü–∞–ø–∫–∞ '{final_dataset_dir}' –∞–±–æ —Ñ–∞–π–ª 'yolo_config.yaml' –≤—ñ–¥—Å—É—Ç–Ω—ñ.")
             exit(1)
        try:
            existing_image = glob(os.path.join(final_dataset_dir, "images", "train", "*.*"))[0]
            img = cv2.imread(existing_image)
            imgsz = (int(img.shape[1]), int(img.shape[0]))
        except IndexError:
            print("ERROR: –ù–µ –∑–Ω–∞–π–¥–µ–Ω–æ –∑–æ–±—Ä–∞–∂–µ–Ω—å —É —Ç—Ä–µ–Ω—É–≤–∞–ª—å–Ω—ñ–π –≤–∏–±—ñ—Ä—Ü—ñ –¥–ª—è –≤–∏–∑–Ω–∞—á–µ–Ω–Ω—è —Ä–æ–∑–º—ñ—Ä—É.")
            exit(1)
    
    resume_path = check_for_unfinished_training()
    should_resume = resume_path is not None
    model_to_load = resume_path if should_resume else "yolov8n.pt"
    model = YOLO(model_to_load)

    print("\nüöÄ –†–æ–∑–ø–æ—á–∏–Ω–∞—î–º–æ —Ç—Ä–µ–Ω—É–≤–∞–Ω–Ω—è –º–æ–¥–µ–ª—ñ...")
    model.train(
        data='yolo_config.yaml',
        epochs=40,
        imgsz=imgsz[0],
        batch=16,
        project='runs/detect',
        name=f'train_{dt.datetime.now().strftime("%Y%m%d_%H%M%S")}',
        exist_ok=False,
        device='cuda' if torch.cuda.is_available() else 'cpu',
        patience=10,
        resume=should_resume
    )
    res_str = f"{imgsz[0]}x{imgsz[1]}"
    model.save(f"Final-YOLOv8n-{res_str}.pt")
    print(f"\n‚úÖ –¢—Ä–µ–Ω—É–≤–∞–Ω–Ω—è –∑–∞–≤–µ—Ä—à–µ–Ω–æ. –§—ñ–Ω–∞–ª—å–Ω—É –º–æ–¥–µ–ª—å –∑–±–µ—Ä–µ–∂–µ–Ω–æ —è–∫ Final-YOLOv8n-{res_str}.pt")