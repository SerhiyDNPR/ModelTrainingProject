# trainers.py

import os
import pprint
from abc import ABC, abstractmethod
from glob import glob
import numpy as np
from tqdm import tqdm
from torch.utils.tensorboard import SummaryWriter
import torch

class BaseTrainer(ABC):
    """–ê–±—Å—Ç—Ä–∞–∫—Ç–Ω–∏–π –±–∞–∑–æ–≤–∏–π –∫–ª–∞—Å –¥–ª—è –≤—Å—ñ—Ö —Ç—Ä–µ–Ω–µ—Ä—ñ–≤."""
    def __init__(self, training_params, dataset_dir):
        self.params = training_params
        self.dataset_dir = dataset_dir

    def display_hyperparameters(self):
        """–í–∏–≤–æ–¥–∏—Ç—å –≥—ñ–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä–∏ –Ω–∞–≤—á–∞–Ω–Ω—è —É –∫–æ–Ω—Å–æ–ª—å —É –≤—ñ–¥—Ñ–æ—Ä–º–∞—Ç–æ–≤–∞–Ω–æ–º—É –≤–∏–≥–ª—è–¥—ñ."""
        print("\n--- –ì—ñ–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä–∏ –Ω–∞–≤—á–∞–Ω–Ω—è ---")
        pprint.pprint(self.params)
        print("---------------------------------")

    def _get_model_name(self):
        """
        –ü–æ–≤–µ—Ä—Ç–∞—î –Ω–∞–∑–≤—É –º–æ–¥–µ–ª—ñ –¥–ª—è –ª–æ–≥—É–≤–∞–Ω–Ω—è, –±–∞–∑—É—é—á–∏—Å—å –Ω–∞ –Ω–∞–∑–≤—ñ –∫–ª–∞—Å—É.
        –ú–æ–∂–µ –±—É—Ç–∏ –ø–µ—Ä–µ–≤–∏–∑–Ω–∞—á–µ–Ω–∏–π —É –¥–æ—á—ñ—Ä–Ω—ñ—Ö –∫–ª–∞—Å–∞—Ö –¥–ª—è –±—ñ–ª—å—à —Ç–æ—á–Ω–æ—ó –Ω–∞–∑–≤–∏.
        """
        return self.__class__.__name__.replace("Trainer", "")

    @abstractmethod
    def start_or_resume_training(self, dataset_stats):
        """
        –ó–∞–ø—É—Å–∫–∞—î –Ω–æ–≤–µ –Ω–∞–≤—á–∞–Ω–Ω—è –∞–±–æ –≤—ñ–¥–Ω–æ–≤–ª—é—î —ñ—Å–Ω—É—é—á–µ.
        
        Args:
            dataset_stats (dict): –°–ª–æ–≤–Ω–∏–∫ –∑—ñ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–æ—é –ø—Ä–æ –¥–∞—Ç–∞—Å–µ—Ç.

        Returns:
            dict: –°–ª–æ–≤–Ω–∏–∫ –∑ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏ –Ω–∞–≤—á–∞–Ω–Ω—è –¥–ª—è –∑–∞–ø–∏—Å—É –≤ –ª–æ–≥-—Ñ–∞–π–ª.
        """
        pass

    def _check_for_resume(self, project_path):
        """–ü–µ—Ä–µ–≤—ñ—Ä—è—î –Ω–∞—è–≤–Ω—ñ—Å—Ç—å –Ω–µ–∑–∞–≤–µ—Ä—à–µ–Ω–æ–≥–æ –Ω–∞–≤—á–∞–Ω–Ω—è."""
        train_dirs = sorted(glob(os.path.join(project_path, "*")))
        if not train_dirs:
            return None, False
        
        last_train_dir = train_dirs[-1]
        last_model_path = os.path.join(last_train_dir, "weights", "last.pt")

        if os.path.exists(last_model_path):
            print(f"\n‚úÖ –í–∏—è–≤–ª–µ–Ω–æ –Ω–µ–∑–∞–≤–µ—Ä—à–µ–Ω–µ –Ω–∞–≤—á–∞–Ω–Ω—è: {last_train_dir}")
            answer = input("–ë–∞–∂–∞—î—Ç–µ –ø—Ä–æ–¥–æ–≤–∂–∏—Ç–∏ –Ω–∞–≤—á–∞–Ω–Ω—è –∑ –æ—Å—Ç–∞–Ω–Ω—å–æ—ó —Ç–æ—á–∫–∏ –∑–±–µ—Ä–µ–∂–µ–Ω–Ω—è? (y/n): ").strip().lower()
            if answer in ['y', 'Y', '–Ω', '–ù']:
                print(f"üöÄ –ù–∞–≤—á–∞–Ω–Ω—è –±—É–¥–µ –ø—Ä–æ–¥–æ–≤–∂–µ–Ω–æ –∑ —Ñ–∞–π–ª—É: {last_model_path}")
                return last_model_path, True
        
        print("üóëÔ∏è –ü–æ–ø–µ—Ä–µ–¥–Ω—ñ–π –ø—Ä–æ–≥—Ä–µ—Å –±—É–¥–µ –ø—Ä–æ—ñ–≥–Ω–æ—Ä–æ–≤–∞–Ω–æ. –ù–∞–≤—á–∞–Ω–Ω—è —Ä–æ–∑–ø–æ—á–Ω–µ—Ç—å—Å—è –∑ –Ω—É–ª—è.")
        return None, False

def collate_fn(batch):
    """–°–ø–µ—Ü—ñ–∞–ª—å–Ω–∞ —Ñ—É–Ω–∫—Ü—ñ—è –¥–ª—è –æ–±'—î–¥–Ω–∞–Ω–Ω—è –±–∞—Ç—á—ñ–≤ —É DataLoader."""
    return tuple(zip(*batch))

def log_dataset_statistics_to_tensorboard(dataset, writer: SummaryWriter):
    print("\nüìä –ü—Ä–æ–≤–æ–¥–∏—Ç—å—Å—è –∞–Ω–∞–ª—ñ–∑ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ —Ç—Ä–µ–Ω—É–≤–∞–ª—å–Ω–æ–≥–æ –¥–∞—Ç–∞—Å–µ—Ç—É...")
    
    # 1. –ó–±—ñ—Ä —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ (–±–µ–∑ –∑–º—ñ–Ω)
    bbox_widths, bbox_heights, bbox_areas, aspect_ratios = [], [], [], []
    for _, target in tqdm(dataset, desc="–ê–Ω–∞–ª—ñ–∑ –¥–∞—Ç–∞—Å–µ—Ç—É"):
        boxes = target.get('boxes')
        if boxes is None or len(boxes) == 0:
            continue
        if not isinstance(boxes, torch.Tensor):
            boxes = torch.tensor(boxes)
        widths_tensor = boxes[:, 2] - boxes[:, 0]
        heights_tensor = boxes[:, 3] - boxes[:, 1]
        for w, h in zip(widths_tensor, heights_tensor):
            w_item, h_item = w.item(), h.item()
            if w_item > 0 and h_item > 0:
                bbox_widths.append(w_item)
                bbox_heights.append(h_item)
                bbox_areas.append(w_item * h_item)
                aspect_ratios.append(w_item / h_item)

    if not bbox_widths:
        print("‚ö†Ô∏è –ù–µ –∑–Ω–∞–π–¥–µ–Ω–æ –∂–æ–¥–Ω–æ—ó —Ä–∞–º–∫–∏ –≤ –¥–∞—Ç–∞—Å–µ—Ç—ñ –¥–ª—è –∞–Ω–∞–ª—ñ–∑—É.")
        return

    print("üìà –ú–∞–ª—é–≤–∞–Ω–Ω—è 6 —Ä–æ–∑–ø–æ–¥—ñ–ª—ñ–≤ —É –≤–∏–≥–ª—è–¥—ñ –ª—ñ–Ω—ñ–π–Ω–∏—Ö –≥—Ä–∞—Ñ—ñ–∫—ñ–≤...")
    
    num_bins = 50 # –ö—ñ–ª—å–∫—ñ—Å—Ç—å —Ç–æ—á–æ–∫ –Ω–∞ –≥—Ä–∞—Ñ—ñ–∫—É

    # --- –ì—Ä–∞—Ñ—ñ–∫ 1: –õ—ñ–Ω—ñ—è —Ä–æ–∑–ø–æ–¥—ñ–ª—É —à–∏—Ä–∏–Ω ---
    counts, bin_edges = np.histogram(bbox_widths, bins=num_bins)
    bin_centers = (bin_edges[:-1] + bin_edges[1:]) / 2
    for i, count in enumerate(counts):
        writer.add_scalar('Dataset_Distribution_Lines/1_Widths', count, bin_centers[i])

    # --- –ì—Ä–∞—Ñ—ñ–∫ 2: –õ—ñ–Ω—ñ—è —Ä–æ–∑–ø–æ–¥—ñ–ª—É —à–∏—Ä–∏–Ω (–ª–æ–≥–∞—Ä–∏—Ñ–º—ñ—á–Ω–∞ —à–∫–∞–ª–∞) ---
    log_widths = np.log1p(np.array(bbox_widths))
    counts, bin_edges = np.histogram(log_widths, bins=num_bins)
    bin_centers = (bin_edges[:-1] + bin_edges[1:]) / 2
    for i, count in enumerate(counts):
        writer.add_scalar('Dataset_Distribution_Lines/2_Widths_Log_Scale', count, bin_centers[i])

    # --- –ì—Ä–∞—Ñ—ñ–∫ 3: –õ—ñ–Ω—ñ—è —Ä–æ–∑–ø–æ–¥—ñ–ª—É –≤–∏—Å–æ—Ç ---
    counts, bin_edges = np.histogram(bbox_heights, bins=num_bins)
    bin_centers = (bin_edges[:-1] + bin_edges[1:]) / 2
    for i, count in enumerate(counts):
        writer.add_scalar('Dataset_Distribution_Lines/3_Heights', count, bin_centers[i])

    # --- –ì—Ä–∞—Ñ—ñ–∫ 4: –õ—ñ–Ω—ñ—è —Ä–æ–∑–ø–æ–¥—ñ–ª—É –≤–∏—Å–æ—Ç (–ª–æ–≥–∞—Ä–∏—Ñ–º—ñ—á–Ω–∞ —à–∫–∞–ª–∞) ---
    log_heights = np.log1p(np.array(bbox_heights))
    counts, bin_edges = np.histogram(log_heights, bins=num_bins)
    bin_centers = (bin_edges[:-1] + bin_edges[1:]) / 2
    for i, count in enumerate(counts):
        writer.add_scalar('Dataset_Distribution_Lines/4_Heights_Log_Scale', count, bin_centers[i])

    # --- –ì—Ä–∞—Ñ—ñ–∫ 5: –õ—ñ–Ω—ñ—è —Ä–æ–∑–ø–æ–¥—ñ–ª—É –ø–ª–æ—â ---
    counts, bin_edges = np.histogram(bbox_areas, bins=num_bins)
    bin_centers = (bin_edges[:-1] + bin_edges[1:]) / 2
    for i, count in enumerate(counts):
        writer.add_scalar('Dataset_Distribution_Lines/5_Areas', count, bin_centers[i])

    # --- –ì—Ä–∞—Ñ—ñ–∫ 6: –õ—ñ–Ω—ñ—è —Ä–æ–∑–ø–æ–¥—ñ–ª—É —Å–ø—ñ–≤–≤—ñ–¥–Ω–æ—à–µ–Ω—å —Å—Ç–æ—Ä—ñ–Ω ---
    counts, bin_edges = np.histogram(aspect_ratios, bins=num_bins)
    bin_centers = (bin_edges[:-1] + bin_edges[1:]) / 2
    for i, count in enumerate(counts):
        writer.add_scalar('Dataset_Distribution_Lines/6_Aspect_Ratios', count, bin_centers[i])

    print("‚úÖ –ê–Ω–∞–ª—ñ–∑ –¥–∞—Ç–∞—Å–µ—Ç—É —É—Å–ø—ñ—à–Ω–æ –∑–∞–≤–µ—Ä—à–µ–Ω–æ.")
