# trainers/EfficientDet_trainer.py

import os
import datetime as dt
import shutil
import torch
import torch.optim as optim
from torch.utils.data import DataLoader
from torchvision.datasets import CocoDetection
import torchvision.transforms.functional as F
import random
from tqdm import tqdm
from trainers.trainers import BaseTrainer, collate_fn, log_dataset_statistics_to_tensorboard
from torchmetrics.detection import MeanAveragePrecision
from torch.utils.tensorboard import SummaryWriter

# –°–ø—Ä–æ–±–∞ —ñ–º–ø–æ—Ä—Ç—É inputimeout –¥–ª—è –∑–∞–ø–∏—Ç—É –∑ —Ç–∞–π–º–∞—É—Ç–æ–º
try:
    from inputimeout import inputimeout, TimeoutOccurred
except ImportError:
    class TimeoutOccurred(Exception):
        pass
    def inputimeout(prompt, timeout):
        return input(prompt) # –ó–∞–≥–ª—É—à–∫–∞, —è–∫—â–æ inputimeout –Ω–µ –≤—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–æ

# EfficientDet –≤–∏–º–∞–≥–∞—î —Å—Ç–æ—Ä–æ–Ω–Ω—å–æ—ó –±—ñ–±–ª—ñ–æ—Ç–µ–∫–∏ effdet.
try:
    # –í–∞–∂–ª–∏–≤–æ: —ñ–º–ø–æ—Ä—Ç—É—î–º–æ DetBenchPredict –¥–ª—è –≤–∞–ª—ñ–¥–∞—Ü—ñ—ó
    from effdet import get_efficientdet_config, EfficientDet, DetBenchTrain, DetBenchPredict
    from effdet.efficientdet import HeadNet
except ImportError:
    print("–ü–æ–º–∏–ª–∫–∞: –±—ñ–±–ª—ñ–æ—Ç–µ–∫—É 'effdet' –Ω–µ –∑–Ω–∞–π–¥–µ–Ω–æ. –í—Å—Ç–∞–Ω–æ–≤—ñ—Ç—å —ó—ó: pip install effdet")
    exit(1)

# –°–ª–æ–≤–Ω–∏–∫ –∑ –∫–æ–Ω—Ñ—ñ–≥—É—Ä–∞—Ü—ñ—è–º–∏ –º–æ–¥–µ–ª–µ–π: –Ω–∞–∑–≤–∞ —Ç–∞ —Ä–µ–∫–æ–º–µ–Ω–¥–æ–≤–∞–Ω–∏–π —Ä–æ–∑–º—ñ—Ä –∑–æ–±—Ä–∞–∂–µ–Ω–Ω—è (H, W)
BACKBONE_CONFIGS = {
    '1': ('tf_efficientdet_d0', (512, 512)),
    '2': ('tf_efficientdet_d1', (640, 640)),
    '3': ('tf_efficientdet_d2', (768, 768)),
    '4': ('tf_efficientdet_d3', (896, 896)),
    '5': ('tf_efficientdet_d4', (1024, 1024)),
    '6': ('tf_efficientdet_d5', (1280, 1280)),
    '7': ('tf_efficientdet_d6', (1536, 1536)),
    '8': ('tf_efficientdet_d7', (1536, 1536)),
}


class DetectionTransforms:
    """–¢—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∞—Ü—ñ—ó, —è–∫—ñ –º–∞—Å—à—Ç–∞–±—É—é—Ç—å –±–æ–∫—Å–∏ —Ç–∞ –∑–æ–±—Ä–∞–∂–µ–Ω–Ω—è."""
    def __init__(self, is_train=False, cat_id_map=None, imgsz=None):
        self.is_train = is_train
        self.cat_id_map = cat_id_map
        if isinstance(imgsz, int):
            self.imgsz = (imgsz, imgsz) # (H, W)
        elif isinstance(imgsz, (tuple, list)) and len(imgsz) == 2:
            self.imgsz = imgsz
        else:
            raise ValueError("imgsz –ø–æ–≤–∏–Ω–µ–Ω –±—É—Ç–∏ int –∞–±–æ (height, width).")
        if self.cat_id_map is None:
            raise ValueError("cat_id_map –ø–æ–≤–∏–Ω–µ–Ω –±—É—Ç–∏ –Ω–∞–¥–∞–Ω–∏–π.")

    def __call__(self, image, target):
        w_orig, h_orig = image.size
        image = F.resize(image, (self.imgsz[0], self.imgsz[1]))

        hflip = self.is_train and random.random() > 0.5
        if hflip:
            image = F.hflip(image)

        image = F.to_tensor(image)
        # –°—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–∞ –Ω–æ—Ä–º–∞–ª—ñ–∑–∞—Ü—ñ—è (EffDet —á–∞—Å—Ç–æ –º–∞—î —Ü–µ –≤–±—É–¥–æ–≤–∞–Ω–æ, –∞–ª–µ –∫—Ä–∞—â–µ –¥–æ–¥–∞—Ç–∏ —Ç—É—Ç)
        image = F.normalize(image, mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])

        boxes, labels = [], []
        w_scale = self.imgsz[1] / w_orig
        h_scale = self.imgsz[0] / h_orig

        if target:
            for ann in target:
                label = self.cat_id_map.get(ann['category_id'])
                if label is None: continue
                x_min, y_min, w, h = ann['bbox']
                if w < 1 or h < 1: continue
                x_max, y_max = x_min + w, y_min + h
                x_min, x_max = x_min * w_scale, x_max * w_scale
                y_min, y_max = y_min * h_scale, y_max * h_scale

                if hflip:
                    img_w = self.imgsz[1]
                    x_min, x_max = img_w - x_max, img_w - x_min # –û–±–º—ñ–Ω –º—ñ—Å—Ü—è–º–∏

                x_min = max(0, x_min); y_min = max(0, y_min)
                x_max = min(self.imgsz[1], x_max); y_max = min(self.imgsz[0], y_max)

                if x_max > x_min and y_max > y_min:
                    boxes.append([x_min, y_min, x_max, y_max])
                    labels.append(label)

        boxes = torch.as_tensor(boxes, dtype=torch.float32)
        labels = torch.as_tensor(labels, dtype=torch.int64)

        final_target = {"boxes": boxes, "labels": labels}
        return image, final_target


def _create_model(num_classes, model_name='tf_efficientdet_d0', image_size=(512, 512), pretrained=True):
    """–°—Ç–≤–æ—Ä—é—î –º–æ–¥–µ–ª—å EfficientDet, –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—é—á–∏ –≤–Ω—É—Ç—Ä—ñ—à–Ω—ñ —Ñ—É–Ω–∫—Ü—ñ—ó effdet."""
    config = get_efficientdet_config(model_name)
    config.num_classes = num_classes
    config.image_size = image_size
    
    # –í–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î–º–æ EfficientDet –≤—ñ–¥ effdet
    model = EfficientDet(config, pretrained_backbone=pretrained)
    # –ó–∞–º—ñ–Ω—é—î–º–æ classification head –¥–ª—è –Ω–æ–≤–æ–≥–æ num_classes
    model.class_net = HeadNet(config, num_outputs=num_classes)
    return model


class EfficientDetTrainer(BaseTrainer):
    """–ö–µ—Ä—É—î –ø—Ä–æ—Ü–µ—Å–æ–º –Ω–∞–≤—á–∞–Ω–Ω—è –º–æ–¥–µ–ª—ñ EfficientDet."""
    def __init__(self, training_params, dataset_dir):
        super().__init__(training_params, dataset_dir)
        self.backbone_choice = None
        self.training_mode = None
        self.image_size = None

    def _select_configuration(self):
        # ... (–õ–æ–≥—ñ–∫–∞ –≤–∏–±–æ—Ä—É backbone —Ç–∞ —Ä–µ–∂–∏–º—É –Ω–∞–≤—á–∞–Ω–Ω—è)
        print("\n   –û–±–µ—Ä—ñ—Ç—å '—Ö—Ä–µ–±–µ—Ç' (backbone) –¥–ª—è EfficientDet:")
        for key, (name, size) in BACKBONE_CONFIGS.items():
            model_id = name.replace('tf_efficientdet_', '').upper()
            print(f"     {key}: {model_id:<4} (—Ä–µ–∫–æ–º–µ–Ω–¥–æ–≤–∞–Ω–∏–π —Ä–æ–∑–º—ñ—Ä: {size[0]}x{size[1]} [H x W])")
        
        while self.backbone_choice is None:
            choice = input(f"   –í–∞—à –≤–∏–±—ñ—Ä backbone (1-{len(BACKBONE_CONFIGS)}): ").strip()
            if choice in BACKBONE_CONFIGS:
                self.backbone_choice, self.image_size = BACKBONE_CONFIGS[choice]
                print(f"‚úÖ –û–±—Ä–∞–Ω–æ backbone: {self.backbone_choice} –∑ —Ä–æ–∑–º—ñ—Ä–æ–º –∑–æ–±—Ä–∞–∂–µ–Ω–Ω—è {self.image_size} (H x W)")
            else:
                print(f"   ‚ùå –ù–µ–≤—ñ—Ä–Ω–∏–π –≤–∏–±—ñ—Ä. –ë—É–¥—å –ª–∞—Å–∫–∞, –≤–≤–µ–¥—ñ—Ç—å —á–∏—Å–ª–æ –≤—ñ–¥ 1 –¥–æ {len(BACKBONE_CONFIGS)}.")

        print("\n   –û–±–µ—Ä—ñ—Ç—å —Ä–µ–∂–∏–º –Ω–∞–≤—á–∞–Ω–Ω—è:")
        print("     1: Fine-tuning (–Ω–∞–≤—á–∞—Ç–∏ —Ç—ñ–ª—å–∫–∏ '–≥–æ–ª–æ–≤—É', —à–≤–∏–¥—à–µ, —Ä–µ–∫–æ–º–µ–Ω–¥–æ–≤–∞–Ω–æ)")
        print("     2: Full training (–Ω–∞–≤—á–∞—Ç–∏ –≤—Å—é –º–æ–¥–µ–ª—å, –¥–æ–≤—à–µ)")
        while self.training_mode is None:
            sub_choice = input("   –í–∞—à –≤–∏–±—ñ—Ä —Ä–µ–∂–∏–º—É (1 –∞–±–æ 2): ").strip()
            if sub_choice == '1':
                self.training_mode = '_finetune'
                print("‚úÖ –û–±—Ä–∞–Ω–æ —Ä–µ–∂–∏–º: Fine-tuning.")
            elif sub_choice == '2':
                self.training_mode = '_full'
                print("‚úÖ –û–±—Ä–∞–Ω–æ —Ä–µ–∂–∏–º: Full training.")
            else:
                print("   ‚ùå –ù–µ–≤—ñ—Ä–Ω–∏–π –≤–∏–±—ñ—Ä. –ë—É–¥—å –ª–∞—Å–∫–∞, –≤–≤–µ–¥—ñ—Ç—å 1 –∞–±–æ 2.")
        
    def _get_model_name(self):
        if not self.backbone_choice:
            return "EfficientDet"
        backbone_str = self.backbone_choice.replace('tf_efficientdet_', '').upper()
        mode_str = "Fine-tune" if self.training_mode == '_finetune' else "Full"
        return f"EfficientDet ({backbone_str} {mode_str})"

    def _get_model(self, num_classes):
        """–ó–∞–≤–∞–Ω—Ç–∞–∂—É—î –º–æ–¥–µ–ª—å EfficientDet."""
        print(f"üîß –°—Ç–≤–æ—Ä–µ–Ω–Ω—è –º–æ–¥–µ–ª—ñ: {self._get_model_name()}")
        
        # –í–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î–º–æ –æ—Ä–∏–≥—ñ–Ω–∞–ª—å–Ω–∏–π _create_model, —è–∫–∏–π –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î effdet.get_efficientdet_config
        model = _create_model(
            num_classes,
            self.backbone_choice,
            image_size=self.image_size, 
            pretrained=True
        )

        if self.training_mode == '_finetune':
            print("‚ùÑÔ∏è –ó–∞–º–æ—Ä–æ–∂—É–≤–∞–Ω–Ω—è –≤–∞–≥ backbone. –ù–∞–≤—á–∞–Ω–Ω—è —Ç—ñ–ª—å–∫–∏ –≥–æ–ª–æ–≤–∏.")
            # –ó–∞–º–æ—Ä–æ–∂—É—î–º–æ –ø–∞—Ä–∞–º–µ—Ç—Ä–∏ backbone, —è–∫—ñ –º–∞—é—Ç—å –ø—Ä–µ—Ñ—ñ–∫—Å 'model.backbone.'
            for name, param in model.named_parameters():
                if name.startswith('backbone.'):
                    param.requires_grad = False
                else:
                    param.requires_grad = True
        else:
            print("üî• –£—Å—ñ –≤–∞–≥–∏ –º–æ–¥–µ–ª—ñ —Ä–æ–∑–º–æ—Ä–æ–∂–µ–Ω–æ –¥–ª—è –ø–æ–≤–Ω–æ–≥–æ –Ω–∞–≤—á–∞–Ω–Ω—è.")
            for param in model.parameters():
                param.requires_grad = True
        
        return model

    def start_or_resume_training(self, dataset_stats):
        if self.training_mode is None or self.backbone_choice is None:
            self._select_configuration()

        imgsz = self.image_size # <--- imgsz –≤–∏–∑–Ω–∞—á–µ–Ω–æ
        print(f"\n--- –ó–∞–ø—É—Å–∫ —Ç—Ä–µ–Ω—É–≤–∞–Ω–Ω—è –¥–ª—è {self._get_model_name()} ---")
        
        device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        print(f"üîå –û–±—Ä–∞–Ω–æ –ø—Ä–∏—Å—Ç—Ä—ñ–π –¥–ª—è –Ω–∞–≤—á–∞–Ω–Ω—è: {str(device).upper()}")
        
        # ... (–Ω–∞—Å—Ç—Ä–æ–π–∫–∞ –≥—ñ–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä—ñ–≤)
        project_dir = os.path.join(self.params.get('project', 'runs/efficientdet'),
                                   f"{self.backbone_choice}{self.training_mode}")
        epochs = self.params.get('epochs', 30)
        batch_size = self.params.get('batch', 2)
        learning_rate = self.params.get('lr', 0.0005)
        step_size = self.params.get('lr_scheduler_step_size', 10)
        gamma = self.params.get('lr_scheduler_gamma', 0.1)
        self.accumulation_steps = self.params.get('accumulation_steps', 8)

        # ... (–ó–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è –¥–∞–Ω–∏—Ö —Ç–∞ –º–æ–¥–µ–ª—ñ)
        train_loader, val_loader, num_classes = self._prepare_dataloaders(batch_size, imgsz=imgsz)
        
        # 1. –°—Ç–≤–æ—Ä–µ–Ω–Ω—è –±–∞–∑–æ–≤–æ—ó –º–æ–¥–µ–ª—ñ
        base_model = self._get_model(num_classes)
        # 2. –û–±–≥–æ—Ä—Ç–∞–Ω–Ω—è –¥–ª—è —Ç—Ä–µ–Ω—É–≤–∞–Ω–Ω—è (–∑ –≤—Ç—Ä–∞—Ç–∞–º–∏ —Ç–∞ –∞–Ω–∫–æ—Ä–∞–º–∏)
        model = DetBenchTrain(base_model, create_labeler=True).to(device)

        # ... (–û–ø—Ç–∏–º—ñ–∑–∞—Ç–æ—Ä —Ç–∞ –ø–ª–∞–Ω—É–≤–∞–ª—å–Ω–∏–∫)
        optimizer = optim.AdamW(filter(lambda p: p.requires_grad, model.parameters()), lr=learning_rate, weight_decay=1e-4)
        lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=step_size, gamma=gamma)
        
        # ... (Warm-up, Checkpoints, TensorBoard)
        run_name, checkpoint_path = self._check_for_resume(project_dir)
        start_epoch, best_map, global_step = 0, 0.0, 0
        run_dir = os.path.join(project_dir, run_name)
        os.makedirs(run_dir, exist_ok=True)
        writer = SummaryWriter(log_dir=os.path.join(run_dir, 'tensorboard_logs'))
        
        warmup_epochs = 1
        warmup_steps = warmup_epochs * len(train_loader)
        warmup_start_lr = 1e-7
        target_lr = learning_rate

        print(f"\nüöÄ –†–æ–∑–ø–æ—á–∏–Ω–∞—î–º–æ —Ç—Ä–µ–Ω—É–≤–∞–Ω–Ω—è –Ω–∞ {epochs} –µ–ø–æ—Ö...")
        for epoch in range(start_epoch, epochs):
            global_step = self._train_one_epoch(model, optimizer, train_loader, device, epoch, writer, global_step)
            
            # --- –í–ò–ü–†–ê–í–õ–ï–ù–ù–Ø –í–ò–ö–õ–ò–ö–£: –î–æ–¥–∞–Ω–æ imgsz ---
            val_map = self._validate_one_epoch(model, val_loader, device, imgsz) 
            # ----------------------------------------
            
            lr_scheduler.step()
            
            # ... (–õ–æ–≥—É–≤–∞–Ω–Ω—è —Ç–∞ –∑–±–µ—Ä–µ–∂–µ–Ω–Ω—è —á–µ–∫–ø–æ—ñ–Ω—Ç—ñ–≤)
            writer.add_scalar('Validation/mAP', val_map, epoch)

            is_best = val_map > best_map
            if is_best:
                best_map = val_map
            self.save_checkpoint({
                'epoch': epoch + 1, 'model_state_dict': model.state_dict(),
                'optimizer_state_dict': optimizer.state_dict(), 'best_map': best_map,
                'lr_scheduler_state_dict': lr_scheduler.state_dict()
            }, is_best, run_dir)

        writer.close()
        # ... (–§–æ—Ä–º—É–≤–∞–Ω–Ω—è summary)
        summary = {
            "model_name": self._get_model_name(),
            "best_map": f"{best_map:.4f}",
            "best_model_path": os.path.join(run_dir, "best_model.pth"),
            "hyperparameters": self.params
        }
        return summary

    def _prepare_dataloaders(self, batch_size, imgsz=None):
        train_img_dir = os.path.join(self.dataset_dir, 'train')
        train_ann_file = os.path.join(self.dataset_dir, 'annotations', 'instances_train.json')
        val_img_dir = os.path.join(self.dataset_dir, 'val')
        val_ann_file = os.path.join(self.dataset_dir, 'annotations', 'instances_val.json')

        temp_dataset = CocoDetection(root=train_img_dir, annFile=train_ann_file)
        coco_cat_ids = sorted(temp_dataset.coco.cats.keys())
        cat_id_to_label = {cat_id: i for i, cat_id in enumerate(coco_cat_ids)}
        num_classes = len(coco_cat_ids)

        train_dataset = CocoDetection(root=train_img_dir, annFile=train_ann_file,
                                      transforms=DetectionTransforms(is_train=True, cat_id_map=cat_id_to_label, imgsz=imgsz))
        val_dataset = CocoDetection(root=val_img_dir, annFile=val_ann_file,
                                    transforms=DetectionTransforms(is_train=False, cat_id_map=cat_id_to_label, imgsz=imgsz))

        train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_fn, num_workers=0, pin_memory=True)
        val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, collate_fn=collate_fn, num_workers=0, pin_memory=True)
        return train_loader, val_loader, num_classes

    def _train_one_epoch(self, model, optimizer, data_loader, device, epoch, writer, global_step):
        model.train()
        progress_bar = tqdm(data_loader, desc=f"Epoch {epoch + 1} [Train]")
        optimizer.zero_grad()
        
        # –ü–∞—Ä–∞–º–µ—Ç—Ä–∏ Warm-up
        target_lr = self.params.get('lr', 0.0005)
        warmup_steps = 1 * len(data_loader)
        warmup_start_lr = 1e-7

        for i, (images, targets) in enumerate(progress_bar):
            if global_step < warmup_steps:
                lr_scale = global_step / warmup_steps
                new_lr = warmup_start_lr + lr_scale * (target_lr - warmup_start_lr)
                for g in optimizer.param_groups: g['lr'] = new_lr

            images_tensor = torch.stack(images).to(device)
            
            # --- –§–Ü–ù–ê–õ–¨–ù–ï –í–ò–ü–†–ê–í–õ–ï–ù–ù–Ø: –ê–≥—Ä–µ—Å–∏–≤–Ω–µ –û–±'—î–¥–Ω–∞–Ω–Ω—è –¶—ñ–ª–µ–π —É –°–ª–æ–≤–Ω–∏–∫ ---
            
            batch_bboxes = []
            batch_classes = []
            img_scales = []
            img_sizes = []
            
            # targets - —Ü–µ List[Dict] (–¥–æ–≤–∂–∏–Ω–∞ = batch_size)
            for target_item in targets:
                if target_item['boxes'].numel() == 0:
                    bx = torch.zeros((0, 4), dtype=torch.float32, device=device)
                    cl = torch.zeros((0,), dtype=torch.int64, device=device)
                else:
                    bx = target_item['boxes'].to(device).float()
                    cl = target_item['labels'].to(device).long()
                
                batch_bboxes.append(bx)
                batch_classes.append(cl)
                
                # –î–æ–¥–∞—î–º–æ –µ–ª–µ–º–µ–Ω—Ç–∏, —â–æ –º–∞—é—Ç—å –±—É—Ç–∏ float —Ç–µ–Ω–∑–æ—Ä–∞–º–∏
                img_scales.append(torch.tensor(1.0, dtype=torch.float32, device=device))
                # –†–æ–∑–º—ñ—Ä –æ–¥–Ω–æ–≥–æ –∑–æ–±—Ä–∞–∂–µ–Ω–Ω—è, –ø–æ–≤—Ç–æ—Ä–µ–Ω–∏–π –¥–ª—è –∫–æ–∂–Ω–æ–≥–æ –µ–ª–µ–º–µ–Ω—Ç–∞ –±–∞—Ç—á—É
                img_sizes.append(torch.tensor(images_tensor[0].shape[1:], dtype=torch.float32, device=device))

            # –°—Ç–≤–æ—Ä—é—î–º–æ –û–î–ò–ù –°–õ–û–í–ù–ò–ö –ó–Ü –°–ü–ò–°–ö–ê–ú–ò –¢–ï–ù–ó–û–†–Ü–í –¥–ª—è DetBenchTrain
            targets_for_bench = {
                'bbox': batch_bboxes, 
                'cls': batch_classes, 
                'img_scale': img_scales, 
                'img_size': img_sizes
            }

            if all(t.numel() == 0 for t in batch_bboxes):
                optimizer.zero_grad()
                continue
            
            try:
                loss_dict = model(images_tensor, targets_for_bench)
            except Exception as e:
                # –í–∫–ª—é—á–∞—î TypeError —Ç–∞ RuntimeError
                print(f"[DEBUG] ‚ùå –ü–æ–º–∏–ª–∫–∞ –≤ –º–æ–¥–µ–ª—ñ –Ω–∞ batch {i}: {e}. –ü—Ä–æ–ø—É—Å–∫.")
                optimizer.zero_grad()
                continue
            
            cls_loss, box_loss = loss_dict['class_loss'].item(), loss_dict['box_loss'].item()
            loss = loss_dict['loss']

            if not torch.isfinite(loss):
                print("‚ö†Ô∏è –ù–µ–∫–æ—Ä–µ–∫—Ç–Ω–∏–π loss, –ø—Ä–æ–ø—É—Å–∫–∞—î–º–æ.")
                optimizer.zero_grad()
                continue

            if self.accumulation_steps > 1:
                loss = loss / self.accumulation_steps

            loss.backward()
            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0) # Gradient Clipping
            
            if (i + 1) % self.accumulation_steps == 0 or (i + 1) == len(data_loader):
                optimizer.step()
                optimizer.zero_grad()
                
                writer.add_scalar('Train/Loss_step', loss.item() * self.accumulation_steps, global_step)
                writer.add_scalar('Train/Classification_Loss', cls_loss, global_step)
                writer.add_scalar('Train/Box_Regression_Loss', box_loss, global_step)
                
                progress_bar.set_postfix(loss=loss.item() * self.accumulation_steps, cls=cls_loss, box=box_loss)
                global_step += 1
            else:
                 progress_bar.set_postfix(loss=loss.item() * self.accumulation_steps, cls=cls_loss, box=box_loss)

        return global_step


    def _validate_one_epoch(self, model, data_loader, device, imgsz):
        model.eval()
        # –°—Ç–≤–æ—Ä—é—î–º–æ –º–æ–¥–µ–ª—å –¥–ª—è –ø–µ—Ä–µ–¥–±–∞—á–µ–Ω–Ω—è
        pred_model = DetBenchPredict(model.model).to(device)
        pred_model.eval()

        metric = MeanAveragePrecision(box_format='xyxy').to(device)
        
        with torch.no_grad():
            progress_bar = tqdm(data_loader, desc="Validating")
            for images, targets in progress_bar:
                images_tensor = torch.stack(images).to(device)
                detections = pred_model(images_tensor)

                preds = []
                for det in detections:
                    keep = det[:, 4] > 0.05 
                    preds.append({
                        'boxes': det[keep, :4],
                        'scores': det[keep, 4],
                        'labels': det[keep, 5].int()
                    })
                
                targets_for_metric = [{k: v.to(device) for k, v in t.items()} for t in targets]
                metric.update(preds, targets_for_metric)
        
        try:
            mAP_dict = metric.compute()
            return mAP_dict['map'].item()
        except Exception as e:
            print(f"–ü–æ–º–∏–ª–∫–∞ –ø—Ä–∏ –æ–±—á–∏—Å–ª–µ–Ω–Ω—ñ mAP: {e}")
            return 0.0
    # === –ö–Ü–ù–ï–¶–¨ _validate_one_epoch ===

    # –í–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î–º–æ –º–µ—Ç–æ–¥ —ñ–∑ FasterRCNNTrainer –¥–ª—è –≤—ñ–¥–Ω–æ–≤–ª–µ–Ω–Ω—è –Ω–∞–≤—á–∞–Ω–Ω—è
    from trainers.FasterRCNNTrainer import FasterRCNNTrainer
    _check_for_resume = FasterRCNNTrainer._check_for_resume_rcnn

    def _load_checkpoint(self, path, model, optimizer, device, lr_scheduler=None):
        checkpoint = torch.load(path, map_location=device)
        model.load_state_dict(checkpoint['model_state_dict'])
        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])
        start_epoch = checkpoint['epoch']
        best_map = checkpoint.get('best_map', 0.0)
        
        if lr_scheduler and 'lr_scheduler_state_dict' in checkpoint:
            lr_scheduler.load_state_dict(checkpoint['lr_scheduler_state_dict'])
            
        return model, optimizer, start_epoch, best_map, lr_scheduler