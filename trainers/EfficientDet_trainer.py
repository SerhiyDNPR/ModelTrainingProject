# trainers/EfficientDet_trainer.py

import os
import datetime as dt
import shutil
import torch
import torch.optim as optim
from torch.utils.data import DataLoader
from torchvision.datasets import CocoDetection
import torchvision.transforms as T
import torchvision.transforms.functional as F # –ü–æ—Ç—Ä—ñ–±–Ω–æ –¥–ª—è –∫–∞—Å—Ç–æ–º–Ω–∏—Ö —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∞—Ü—ñ–π
import random # –ü–æ—Ç—Ä—ñ–±–Ω–æ –¥–ª—è –∞—É–≥–º–µ–Ω—Ç–∞—Ü—ñ–π
from tqdm import tqdm
from trainers.trainers import BaseTrainer, collate_fn, log_dataset_statistics_to_tensorboard
from torchmetrics.detection import MeanAveragePrecision
from torch.utils.tensorboard import SummaryWriter

# –°–ø—Ä–æ–±–∞ —ñ–º–ø–æ—Ä—Ç—É inputimeout –¥–ª—è –∑–∞–ø–∏—Ç—É –∑ —Ç–∞–π–º–∞—É—Ç–æ–º
try:
    from inputimeout import inputimeout, TimeoutOccurred
except ImportError:
    # –ó–∞–≥–ª—É—à–∫–∞, —è–∫—â–æ –±—ñ–±–ª—ñ–æ—Ç–µ–∫–∞ –Ω–µ –≤—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∞
    class TimeoutOccurred(Exception):
        pass
    def inputimeout(prompt, timeout):
        print(prompt.replace(f"[–∞–≤—Ç–æ–º–∞—Ç–∏—á–Ω–æ '1' —á–µ—Ä–µ–∑ {timeout}—Å]", "(–î–ª—è —Ä–æ–±–æ—Ç–∏ —Ç–∞–π–º–∞—É—Ç—É –≤—Å—Ç–∞–Ω–æ–≤—ñ—Ç—å 'inputimeout')"))
        return input()

# EfficientDet –≤–∏–º–∞–≥–∞—î —Å—Ç–æ—Ä–æ–Ω–Ω—å–æ—ó –±—ñ–±–ª—ñ–æ—Ç–µ–∫–∏.
# –í—Å—Ç–∞–Ω–æ–≤—ñ—Ç—å —ó—ó –∫–æ–º–∞–Ω–¥–æ—é: pip install effdet
try:
    from effdet import get_efficientdet_config, EfficientDet, DetBenchTrain, DetBenchPredict
    from effdet.efficientdet import HeadNet
except ImportError:
    print("–ü–æ–º–∏–ª–∫–∞: –±—ñ–±–ª—ñ–æ—Ç–µ–∫—É 'effdet' –Ω–µ –∑–Ω–∞–π–¥–µ–Ω–æ.")
    print("–ë—É–¥—å –ª–∞—Å–∫–∞, –≤—Å—Ç–∞–Ω–æ–≤—ñ—Ç—å —ó—ó –∫–æ–º–∞–Ω–¥–æ—é: pip install effdet")
    exit(1)

# –°–ª–æ–≤–Ω–∏–∫ –∑ –∫–æ–Ω—Ñ—ñ–≥—É—Ä–∞—Ü—ñ—è–º–∏ –º–æ–¥–µ–ª–µ–π: –Ω–∞–∑–≤–∞ —Ç–∞ —Ä–µ–∫–æ–º–µ–Ω–¥–æ–≤–∞–Ω–∏–π —Ä–æ–∑–º—ñ—Ä –∑–æ–±—Ä–∞–∂–µ–Ω–Ω—è (–≤–∏—Å–æ—Ç–∞, —à–∏—Ä–∏–Ω–∞)
# –í–ê–ñ–õ–ò–í–û: effdet –æ—á—ñ–∫—É—î —Ä–æ–∑–º—ñ—Ä —É —Ñ–æ—Ä–º–∞—Ç—ñ (height, width)
BACKBONE_CONFIGS = {
    '1': ('tf_efficientdet_d0', (512, 512)),
    '2': ('tf_efficientdet_d1', (640, 640)),
    '3': ('tf_efficientdet_d2', (768, 768)),
    '4': ('tf_efficientdet_d3', (896, 896)),
    '5': ('tf_efficientdet_d4', (1024, 1024)),
    '6': ('tf_efficientdet_d5', (1280, 1280)),
    '7': ('tf_efficientdet_d6', (1536, 1536)),
    '8': ('tf_efficientdet_d7', (1536, 1536)),
}


class DetectionTransforms:
    """
    –í–ª–∞—Å–Ω–∏–π –∫–ª–∞—Å —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∞—Ü—ñ–π, —â–æ –Ω–µ –∑–∞–ª–µ–∂–∏—Ç—å –≤—ñ–¥ FCOS_trainer.
    –í–∏–∫–æ–Ω—É—î:
    1. –ó–º—ñ–Ω—É —Ä–æ–∑–º—ñ—Ä—É –∑–æ–±—Ä–∞–∂–µ–Ω–Ω—è –¥–æ —Ñ—ñ–∫—Å–æ–≤–∞–Ω–æ–≥–æ `imgsz`.
    2. –ú–∞—Å—à—Ç–∞–±—É–≤–∞–Ω–Ω—è bounding boxes –≤—ñ–¥–ø–æ–≤—ñ–¥–Ω–æ –¥–æ –Ω–æ–≤–æ–≥–æ —Ä–æ–∑–º—ñ—Ä—É.
    3. –ì–æ—Ä–∏–∑–æ–Ω—Ç–∞–ª—å–Ω–µ –≤—ñ–¥–¥–∑–µ—Ä–∫–∞–ª–µ–Ω–Ω—è (–∞—É–≥–º–µ–Ω—Ç–∞—Ü—ñ—è) –¥–ª—è –Ω–∞–≤—á–∞–Ω–Ω—è.
    4. –ü–µ—Ä–µ—Ç–≤–æ—Ä–µ–Ω–Ω—è PIL Image –≤ Tensor.
    5. –ü–µ—Ä–µ—Ç–≤–æ—Ä–µ–Ω–Ω—è ID –∫–∞—Ç–µ–≥–æ—Ä—ñ–π COCO –≤ –ø–æ—Å–ª—ñ–¥–æ–≤–Ω—ñ ID (0, 1, 2...).
    """
    def __init__(self, is_train=False, cat_id_map=None, imgsz=None):
        self.is_train = is_train
        self.cat_id_map = cat_id_map

        if isinstance(imgsz, int):
            self.imgsz = (imgsz, imgsz) # (H, W)
        elif isinstance(imgsz, (tuple, list)) and len(imgsz) == 2:
            self.imgsz = imgsz # –û—á—ñ–∫—É—î–º–æ (H, W)
        else:
            raise ValueError("imgsz –ø–æ–≤–∏–Ω–µ–Ω –±—É—Ç–∏ int –∞–±–æ (height, width) tuple/list.")

        if self.cat_id_map is None:
            raise ValueError("cat_id_map –ø–æ–≤–∏–Ω–µ–Ω –±—É—Ç–∏ –Ω–∞–¥–∞–Ω–∏–π.")

    def __call__(self, image, target):
        # 1. –û—Ç—Ä–∏–º—É—î–º–æ –æ—Ä–∏–≥—ñ–Ω–∞–ª—å–Ω–∏–π —Ä–æ–∑–º—ñ—Ä
        w_orig, h_orig = image.size # PIL.size –ø–æ–≤–µ—Ä—Ç–∞—î (width, height)

        # 2. –ó–º—ñ–Ω—é—î–º–æ —Ä–æ–∑–º—ñ—Ä –∑–æ–±—Ä–∞–∂–µ–Ω–Ω—è
        # F.resize –æ—á—ñ–∫—É—î (H, W)
        image = F.resize(image, (self.imgsz[0], self.imgsz[1]))

        # 3. –ì–æ—Ä–∏–∑–æ–Ω—Ç–∞–ª—å–Ω–µ –≤—ñ–¥–¥–∑–µ—Ä–∫–∞–ª–µ–Ω–Ω—è (–∞—É–≥–º–µ–Ω—Ç–∞—Ü—ñ—è)
        hflip = self.is_train and random.random() > 0.5
        if hflip:
            image = F.hflip(image)

        # 4. –ü–µ—Ä–µ—Ç–≤–æ—Ä—é—î–º–æ –∑–æ–±—Ä–∞–∂–µ–Ω–Ω—è –≤ —Ç–µ–Ω–∑–æ—Ä ([0, 255] -> [0.0, 1.0])
        image = F.to_tensor(image)

        # 5. –û–±—Ä–æ–±–ª—è—î–º–æ —Ü—ñ–ª—ñ (targets)
        boxes = []
        labels = []

        # –†–æ–∑—Ä–∞—Ö–æ–≤—É—î–º–æ –∫–æ–µ—Ñ—ñ—Ü—ñ—î–Ω—Ç–∏ –º–∞—Å—à—Ç–∞–±—É–≤–∞–Ω–Ω—è
        # self.imgsz = (H, W)
        w_scale = self.imgsz[1] / w_orig
        h_scale = self.imgsz[0] / h_orig

        if target: # target - —Ü–µ —Å–ø–∏—Å–æ–∫ dict'—ñ–≤ –∞–Ω–æ—Ç–∞—Ü—ñ–π
            for ann in target:
                # –û—Ç—Ä–∏–º—É—î–º–æ –ø–æ—Å–ª—ñ–¥–æ–≤–Ω–∏–π ID –∫–ª–∞—Å—É
                label = self.cat_id_map.get(ann['category_id'])
                if label is None:
                    continue # –Ü–≥–Ω–æ—Ä—É—î–º–æ –∫–ª–∞—Å–∏, —è–∫–∏—Ö –Ω–µ–º–∞—î –≤ –Ω–∞—à—ñ–π –º–∞–ø—ñ

                # COCO bbox = [x_min, y_min, width, height]
                x_min, y_min, w, h = ann['bbox']
                
                # –ö–æ–Ω–≤–µ—Ä—Ç—É—î–º–æ –≤ [x_min, y_min, x_max, y_max]
                x_max = x_min + w
                y_max = y_min + h
                
                # –ú–∞—Å—à—Ç–∞–±—É—î–º–æ –∫–æ–æ—Ä–¥–∏–Ω–∞—Ç–∏
                x_min_scaled = x_min * w_scale
                y_min_scaled = y_min * h_scale
                x_max_scaled = x_max * w_scale
                y_max_scaled = y_max * h_scale
                
                # –ó–∞—Å—Ç–æ—Å–æ–≤—É—î–º–æ –≤—ñ–¥–¥–∑–µ—Ä–∫–∞–ª–µ–Ω–Ω—è –¥–æ –±–æ–∫—Å—ñ–≤, —è–∫—â–æ –≤–æ–Ω–æ –±—É–ª–æ
                if hflip:
                    img_width_scaled = self.imgsz[1] # –®–∏—Ä–∏–Ω–∞ 'W'
                    # x_min —Å—Ç–∞—î (—à–∏—Ä–∏–Ω–∞ - x_max)
                    # x_max —Å—Ç–∞—î (—à–∏—Ä–∏–Ω–∞ - x_min)
                    x_max_new = img_width_scaled - x_min_scaled
                    x_min_new = img_width_scaled - x_max_scaled
                    x_min_scaled = x_min_new
                    x_max_scaled = x_max_new

                # –û–±—Ä—ñ–∑–∞—î–º–æ –±–æ–∫—Å–∏, —â–æ–± –≤–æ–Ω–∏ –±—É–ª–∏ –≤ –º–µ–∂–∞—Ö –∑–æ–±—Ä–∞–∂–µ–Ω–Ω—è
                x_min_scaled = max(0, x_min_scaled)
                y_min_scaled = max(0, y_min_scaled)
                x_max_scaled = min(self.imgsz[1], x_max_scaled)
                y_max_scaled = min(self.imgsz[0], y_max_scaled)

                # –î–æ–¥–∞—î–º–æ, —Ç—ñ–ª—å–∫–∏ —è–∫—â–æ –±–æ–∫—Å –≤–∞–ª—ñ–¥–Ω–∏–π (–º–∞—î –ø–ª–æ—â—É)
                if x_max_scaled > x_min_scaled and y_max_scaled > y_min_scaled:
                    boxes.append([x_min_scaled, y_min_scaled, x_max_scaled, y_max_scaled])
                    labels.append(label)

        # –ö–æ–Ω–≤–µ—Ä—Ç—É—î–º–æ —É —Ç–µ–Ω–∑–æ—Ä–∏
        boxes = torch.as_tensor(boxes, dtype=torch.float32)
        labels = torch.as_tensor(labels, dtype=torch.int64)

        # –û–±—Ä–æ–±–∫–∞ –≤–∏–ø–∞–¥–∫—ñ–≤, –∫–æ–ª–∏ –∞–Ω–æ—Ç–∞—Ü—ñ–π –Ω–µ–º–∞—î
        if not boxes.numel():
            boxes = torch.zeros((0, 4), dtype=torch.float32)
            
        final_target = {}
        final_target["boxes"] = boxes
        final_target["labels"] = labels
        
        return image, final_target


def _create_model(num_classes, model_name='tf_efficientdet_d0', image_size=(512, 512), pretrained=True):
    """–°—Ç–≤–æ—Ä—é—î –º–æ–¥–µ–ª—å EfficientDet –∑ –∑–∞–¥–∞–Ω–æ—é –∫–æ–Ω—Ñ—ñ–≥—É—Ä–∞—Ü—ñ—î—é."""
    config = get_efficientdet_config(model_name)
    config.num_classes = num_classes
    config.image_size = image_size # effdet –æ—á—ñ–∫—É—î (height, width)

    model = EfficientDet(config, pretrained_backbone=pretrained)
    model.class_net = HeadNet(config, num_outputs=num_classes)
    return model

class EfficientDetTrainer(BaseTrainer):
    """–ö–µ—Ä—É—î –ø—Ä–æ—Ü–µ—Å–æ–º –Ω–∞–≤—á–∞–Ω–Ω—è –º–æ–¥–µ–ª—ñ EfficientDet."""

    def __init__(self, training_params, dataset_dir):
        super().__init__(training_params, dataset_dir)
        self.backbone_choice = None
        self.training_mode = None
        self.image_size = None # –ë—É–¥–µ –∑–±–µ—Ä—ñ–≥–∞—Ç–∏ –æ–±—Ä–∞–Ω–∏–π —Ä–æ–∑–º—ñ—Ä –∑–æ–±—Ä–∞–∂–µ–Ω–Ω—è (H, W)

    def _select_configuration(self):
        """–ó–∞–ø–∏—Ç—É—î —É –∫–æ—Ä–∏—Å—Ç—É–≤–∞—á–∞ backbone —Ç–∞ —Ä–µ–∂–∏–º –Ω–∞–≤—á–∞–Ω–Ω—è."""
        print("\n–ë—É–¥—å –ª–∞—Å–∫–∞, –æ–±–µ—Ä—ñ—Ç—å '—Ö—Ä–µ–±–µ—Ç' (backbone) –¥–ª—è EfficientDet:")
        for key, (name, size) in BACKBONE_CONFIGS.items():
            model_id = name.replace('tf_efficientdet_', '').upper()
            print(f"  {key}: {model_id:<4} (—Ä–µ–∫–æ–º–µ–Ω–¥–æ–≤–∞–Ω–∏–π —Ä–æ–∑–º—ñ—Ä: {size[0]}x{size[1]} [HxW])")

        while self.backbone_choice is None:
            choice = input(f"–í–∞—à –≤–∏–±—ñ—Ä (1-{len(BACKBONE_CONFIGS)}): ").strip()
            if choice in BACKBONE_CONFIGS:
                self.backbone_choice, self.image_size = BACKBONE_CONFIGS[choice]
                print(f"‚úÖ –í–∏ –æ–±—Ä–∞–ª–∏: {self.backbone_choice} –∑ —Ä–æ–∑–º—ñ—Ä–æ–º –∑–æ–±—Ä–∞–∂–µ–Ω–Ω—è {self.image_size} (H x W)")
            else:
                print(f"‚ùå –ù–µ–≤—ñ—Ä–Ω–∏–π –≤–∏–±—ñ—Ä. –ë—É–¥—å –ª–∞—Å–∫–∞, –≤–≤–µ–¥—ñ—Ç—å —á–∏—Å–ª–æ –≤—ñ–¥ 1 –¥–æ {len(BACKBONE_CONFIGS)}.")

        print("\n   –û–±–µ—Ä—ñ—Ç—å —Ä–µ–∂–∏–º –Ω–∞–≤—á–∞–Ω–Ω—è:")
        print("     1: Fine-tuning (–Ω–∞–≤—á–∞—Ç–∏ —Ç—ñ–ª—å–∫–∏ '–≥–æ–ª–æ–≤—É', —à–≤–∏–¥—à–µ, —Ä–µ–∫–æ–º–µ–Ω–¥–æ–≤–∞–Ω–æ)")
        print("     2: Full training (–Ω–∞–≤—á–∞—Ç–∏ –≤—Å—é –º–æ–¥–µ–ª—å, –¥–æ–≤—à–µ)")
        while self.training_mode is None:
            sub_choice = input("   –í–∞—à –≤–∏–±—ñ—Ä —Ä–µ–∂–∏–º—É (1 –∞–±–æ 2): ").strip()
            if sub_choice == '1':
                self.training_mode = '_finetune'
                print("‚úÖ –û–±—Ä–∞–Ω–æ —Ä–µ–∂–∏–º: Fine-tuning.")
            elif sub_choice == '2':
                self.training_mode = '_full'
                print("‚úÖ –û–±—Ä–∞–Ω–æ —Ä–µ–∂–∏–º: Full training.")
            else:
                print("   ‚ùå –ù–µ–≤—ñ—Ä–Ω–∏–π –≤–∏–±—ñ—Ä. –ë—É–¥—å –ª–∞—Å–∫–∞, –≤–≤–µ–¥—ñ—Ç—å 1 –∞–±–æ 2.")

    def _get_model_name(self):
        if not self.backbone_choice:
            return "EfficientDet"
        backbone_str = self.backbone_choice.replace('tf_efficientdet_', '').upper()
        mode_str = "Fine-tune" if self.training_mode == '_finetune' else "Full"
        return f"EfficientDet ({backbone_str} {mode_str})"

    def start_or_resume_training(self, dataset_stats):
        if not self.backbone_choice or not self.training_mode:
            self._select_configuration()

        # –í–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î–º–æ —Ä–æ–∑–º—ñ—Ä –∑–æ–±—Ä–∞–∂–µ–Ω–Ω—è, –æ–±—Ä–∞–Ω–∏–π –∫–æ—Ä–∏—Å—Ç—É–≤–∞—á–µ–º (H, W)
        imgsz = self.image_size
        print(f"\n--- –ó–∞–ø—É—Å–∫ —Ç—Ä–µ–Ω—É–≤–∞–Ω–Ω—è –¥–ª—è {self._get_model_name()} ---")

        device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        print(f"üîå –û–±—Ä–∞–Ω–æ –ø—Ä–∏—Å—Ç—Ä—ñ–π –¥–ª—è –Ω–∞–≤—á–∞–Ω–Ω—è: {str(device).upper()}")

        print(f"üñºÔ∏è –†–æ–∑–º—ñ—Ä –∑–æ–±—Ä–∞–∂–µ–Ω—å –¥–ª—è –Ω–∞–≤—á–∞–Ω–Ω—è –±—É–¥–µ –∑–º—ñ–Ω–µ–Ω–æ –Ω–∞ {imgsz[0]}x{imgsz[1]} (H x W).")

        project_dir = os.path.join(self.params.get('project', 'runs/efficientdet'), f"{self.backbone_choice}{self.training_mode}")
        epochs = self.params.get('epochs', 25)
        batch_size = self.params.get('batch', 8)
        learning_rate = self.params.get('lr', 0.0001)
        step_size = self.params.get('lr_scheduler_step_size', 8)
        gamma = self.params.get('lr_scheduler_gamma', 0.1)
        self.accumulation_steps = self.params.get('accumulation_steps', 1)

        train_loader, val_loader, num_classes = self._prepare_dataloaders(batch_size, imgsz=imgsz)
        print(f"üìä –ó–Ω–∞–π–¥–µ–Ω–æ {num_classes} –∫–ª–∞—Å—ñ–≤. –ù–∞–≤—á–∞–Ω–Ω—è –º–æ–¥–µ–ª—ñ –¥–ª—è —ó—Ö —Ä–æ–∑–ø—ñ–∑–Ω–∞–≤–∞–Ω–Ω—è.")
        
        # --- –î–Ü–ê–ì–ù–û–°–¢–ò–ö–ê: –ü–µ—Ä–µ–≤—ñ—Ä–∫–∞ —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∞—Ü—ñ–π –¥–∞—Ç–∞—Å–µ—Ç—É (–º–æ–∂–Ω–∞ –≤–∏–¥–∞–ª–∏—Ç–∏ –ø—ñ—Å–ª—è —É—Å–ø—ñ—à–Ω–æ—ó –ø–µ—Ä–µ–≤—ñ—Ä–∫–∏) ---
        # –Ü–ú–ü–û–†–¢–£–ô–¢–ï visualize_batch_item –°–Æ–î–ò –ê–ë–û –í utils.py
        # print("\n--- –î–Ü–ê–ì–ù–û–°–¢–ò–ö–ê: –ü–µ—Ä–µ–≤—ñ—Ä–∫–∞ —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∞—Ü—ñ–π –¥–∞—Ç–∞—Å–µ—Ç—É ---")
        # try:
        #     # –û—Ç—Ä–∏–º—É—î–º–æ –æ–¥–∏–Ω –±–∞—Ç—á
        #     for images, targets in train_loader:
        #         break
        #     # –í—ñ–¥–æ–±—Ä–∞–∂–∞—î–º–æ –ª–∏—à–µ –ø–µ—Ä—à–µ –∑–æ–±—Ä–∞–∂–µ–Ω–Ω—è –∑ –±–∞—Ç—á—É
        #     image_to_check = images[0]
        #     target_to_check = targets[0]
        #     visualize_batch_item(image_to_check, target_to_check, 
        #                          class_labels=list(range(num_classes)))
        #     answer = input("–¢—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∞—Ü—ñ—ó –≤–∏–≥–ª—è–¥–∞—é—Ç—å –∫–æ—Ä–µ–∫—Ç–Ω–æ? (y/n): ").strip().lower()
        #     if answer not in ['y', 'Y', '–Ω', '–ù']:
        #         print("‚ùå –¢—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∞—Ü—ñ—ó –Ω–µ–∫–æ—Ä–µ–∫—Ç–Ω—ñ. –ù–∞–≤—á–∞–Ω–Ω—è –∑—É–ø–∏–Ω–µ–Ω–æ.")
        #         return 
        #     else:
        #         print("‚úÖ –¢—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∞—Ü—ñ—ó –∫–æ—Ä–µ–∫—Ç–Ω—ñ. –ü—Ä–æ–¥–æ–≤–∂—É—î–º–æ –Ω–∞–≤—á–∞–Ω–Ω—è.")
        # except Exception as e:
        #     print(f"‚ùå –ü–û–ú–ò–õ–ö–ê –ü–†–ò –í–Ü–ó–£–ê–õ–Ü–ó–ê–¶–Ü–á: {e}. –ü—Ä–æ–¥–æ–≤–∂—É—î–º–æ, –∞–ª–µ –±—É–¥—å—Ç–µ –æ–±–µ—Ä–µ–∂–Ω—ñ.")
        # print("-----------------------------------------------------")
        # --- –ö–Ü–ù–ï–¶–¨ –ë–õ–û–ö–£ –î–Ü–ê–ì–ù–û–°–¢–ò–ö–ò ---

        base_model = self._get_model(num_classes)
        model = DetBenchTrain(base_model, create_labeler=True).to(device)

        optimizer = optim.AdamW(model.parameters(), lr=learning_rate, weight_decay=1e-4)
        lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=step_size, gamma=gamma)

        run_name, checkpoint_path = self._check_for_resume(project_dir)
        start_epoch, best_map, global_step = 0, 0.0, 0

        run_dir = os.path.join(project_dir, run_name)
        os.makedirs(run_dir, exist_ok=True)
        writer = SummaryWriter(log_dir=os.path.join(run_dir, 'tensorboard_logs'))

        print(f"üìÇ –†–µ–∑—É–ª—å—Ç–∞—Ç–∏ –±—É–¥—É—Ç—å –∑–±–µ—Ä–µ–∂–µ–Ω—ñ –≤: {run_dir}")
        print(f"üìà –õ–æ–≥–∏ –¥–ª—è TensorBoard –±—É–¥—É—Ç—å –∑–±–µ—Ä–µ–∂–µ–Ω—ñ –≤: {writer.log_dir}")
        log_dataset_statistics_to_tensorboard(train_loader.dataset, writer)

        if checkpoint_path:
            model.model, optimizer, start_epoch, best_map, lr_scheduler = self._load_checkpoint(
                checkpoint_path, model.model, optimizer, device, lr_scheduler
            )
            # –í—Å—Ç–∞–Ω–æ–≤–ª—é—î–º–æ global_step –Ω–∞ –æ—Å–Ω–æ–≤—ñ –≤—ñ–¥–Ω–æ–≤–ª–µ–Ω–æ—ó –µ–ø–æ—Ö–∏
            global_step = start_epoch * len(train_loader)
            print(f"üöÄ –í—ñ–¥–Ω–æ–≤–ª–µ–Ω–Ω—è –Ω–∞–≤—á–∞–Ω–Ω—è –∑ {start_epoch}-—ó –µ–ø–æ—Ö–∏.")

        # --- –ù–∞–ª–∞—à—Ç—É–≤–∞–Ω–Ω—è Warm-up ---
        warmup_epochs = 1  # –ó–Ω–∞—á–µ–Ω–Ω—è –∑–∞ –∑–∞–º–æ–≤—á—É–≤–∞–Ω–Ω—è–º
        if start_epoch == 0:
            try:
                prompt = f"\n–í–≤–µ–¥—ñ—Ç—å –∫—ñ–ª—å–∫—ñ—Å—Ç—å –µ–ø–æ—Ö –¥–ª—è '–ø—Ä–æ–≥—Ä—ñ–≤—É' (warm-up) [–∞–≤—Ç–æ–º–∞—Ç–∏—á–Ω–æ '1' —á–µ—Ä–µ–∑ 5—Å]: "
                user_input = inputimeout(prompt=prompt, timeout=5).strip()
                if user_input and user_input.isdigit() and int(user_input) > 0:
                    warmup_epochs = int(user_input)
                    print(f"‚úÖ –í—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–æ {warmup_epochs} –µ–ø–æ—Ö –¥–ª—è –ø—Ä–æ–≥—Ä—ñ–≤—É.")
                else:
                    if user_input: # –Ø–∫—â–æ –±—É–ª–æ –≤–≤–µ–¥–µ–Ω–Ω—è, –∞–ª–µ –Ω–µ–∫–æ—Ä–µ–∫—Ç–Ω–µ
                         print(f"‚ö†Ô∏è  –ù–µ–∫–æ—Ä–µ–∫—Ç–Ω–µ –≤–≤–µ–¥–µ–Ω–Ω—è. –í–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î—Ç—å—Å—è –∑–Ω–∞—á–µ–Ω–Ω—è –∑–∞ –∑–∞–º–æ–≤—á—É–≤–∞–Ω–Ω—è–º: {warmup_epochs} –µ–ø–æ—Ö–∞.")
                    else: # –Ø–∫—â–æ –ø—Ä–æ—Å—Ç–æ –Ω–∞—Ç–∏—Å–Ω—É—Ç–æ Enter
                         print(f"‚úÖ –í–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î—Ç—å—Å—è –∑–Ω–∞—á–µ–Ω–Ω—è –∑–∞ –∑–∞–º–æ–≤—á—É–≤–∞–Ω–Ω—è–º: {warmup_epochs} –µ–ø–æ—Ö–∞.")

            except TimeoutOccurred:
                print(f"\n–ß–∞—Å –Ω–∞ –≤–≤–µ–¥–µ–Ω–Ω—è –≤–∏—á–µ—Ä–ø–∞–Ω–æ. –í–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î—Ç—å—Å—è –∑–Ω–∞—á–µ–Ω–Ω—è –∑–∞ –∑–∞–º–æ–≤—á—É–≤–∞–Ω–Ω—è–º: {warmup_epochs} –µ–ø–æ—Ö–∞.")
            except Exception as e:
                print(f"\n–ü–æ–º–∏–ª–∫–∞ –ø—Ä–∏ –∑—á–∏—Ç—É–≤–∞–Ω–Ω—ñ –≤–≤–æ–¥—É ({e}). –í–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î—Ç—å—Å—è –∑–Ω–∞—á–µ–Ω–Ω—è –∑–∞ –∑–∞–º–æ–≤—á—É–≤–∞–Ω–Ω—è–º: {warmup_epochs} –µ–ø–æ—Ö–∞.")

            warmup_steps = warmup_epochs * len(train_loader)
            if warmup_steps > 0:
                print(f"üî• –£–≤—ñ–º–∫–Ω–µ–Ω–æ '–ø—Ä–æ–≥—Ä—ñ–≤' (warm-up) –Ω–∞ {warmup_steps} –∫—Ä–æ–∫—ñ–≤ ({warmup_epochs} –µ–ø–æ—Ö(–∏)).")
            else:
                 warmup_steps = 0 # –ù–∞ –≤–∏–ø–∞–¥–æ–∫ –ø–æ—Ä–æ–∂–Ω—å–æ–≥–æ –¥–∞—Ç–∞—Å–µ—Ç—É
        else:
            warmup_steps = 0  # –ü—Ä–æ–≥—Ä—ñ–≤ –≤–∂–µ –≤—ñ–¥–±—É–≤—Å—è
            
        target_lr = learning_rate
        warmup_start_lr = 1e-7 # –ü–æ—á–∞—Ç–∫–æ–≤–∏–π LR –¥–ª—è –ø—Ä–æ–≥—Ä—ñ–≤—É
        # -----------------------------

        print(f"\nüöÄ –†–æ–∑–ø–æ—á–∏–Ω–∞—î–º–æ —Ç—Ä–µ–Ω—É–≤–∞–Ω–Ω—è –Ω–∞ {epochs} –µ–ø–æ—Ö...")
        for epoch in range(start_epoch, epochs):
            
            global_step = self._train_one_epoch(
                model, optimizer, train_loader, device, epoch, writer, global_step,
                target_lr=target_lr, 
                warmup_steps=warmup_steps, 
                warmup_start_lr=warmup_start_lr
            )

            val_map = self._validate_one_epoch(model, val_loader, device, imgsz=imgsz)
            
            if global_step > warmup_steps:
                lr_scheduler.step()

            current_display_lr = lr_scheduler.get_last_lr()[0] if global_step > warmup_steps else optimizer.param_groups[0]['lr']
            print(f"Epoch {epoch + 1}/{epochs} | Validation mAP: {val_map:.4f} | Current LR: {current_display_lr:.6f}")
            writer.add_scalar('Validation/mAP', val_map, epoch)
            writer.add_scalar('LearningRate/Epoch', current_display_lr, epoch)

            is_best = val_map > best_map
            if is_best:
                best_map = val_map

            self.save_checkpoint({
                'epoch': epoch + 1, 'model_state_dict': model.model.state_dict(),
                'optimizer_state_dict': optimizer.state_dict(), 'best_map': best_map,
                'lr_scheduler_state_dict': lr_scheduler.state_dict()
            }, is_best, run_dir)

        writer.close()
        print("\nüéâ –ù–∞–≤—á–∞–Ω–Ω—è —É—Å–ø—ñ—à–Ω–æ –∑–∞–≤–µ—Ä—à–µ–Ω–æ!")
        
        best_model_path = os.path.join(run_dir, "best_model.pth")
        final_path = None
        if os.path.exists(best_model_path):
            final_path = f"Final-{self._get_model_name()}-best.pth"
            shutil.copy(best_model_path, final_path)
            print(f"\n‚úÖ –ù–∞–π–∫—Ä–∞—â—É –º–æ–¥–µ–ª—å —Å–∫–æ–ø—ñ–π–æ–≤–∞–Ω–æ —É —Ñ–∞–π–ª: {final_path} (mAP: {best_map:.4f})")
        
        summary = { 
            "model_name": self._get_model_name(), 
            "best_map": f"{best_map:.4f}", 
            "best_model_path": final_path, 
            "hyperparameters": self.params 
        }
        return summary

    def _get_model(self, num_classes):
        """–ó–∞–≤–∞–Ω—Ç–∞–∂—É—î —Ç–∞ –Ω–∞–ª–∞—à—Ç–æ–≤—É—î –º–æ–¥–µ–ª—å EfficientDet."""
        print(f"üîß –°—Ç–≤–æ—Ä–µ–Ω–Ω—è –º–æ–¥–µ–ª—ñ: {self._get_model_name()}")
        model = _create_model(
            num_classes,
            self.backbone_choice,
            image_size=self.image_size, # (H, W)
            pretrained=True
        )

        if self.training_mode == '_finetune':
            print("‚ùÑÔ∏è –ó–∞–º–æ—Ä–æ–∂—É–≤–∞–Ω–Ω—è backbone —Ç–∞ FPN. –ù–∞–≤—á–∞–Ω–Ω—è —Ç—ñ–ª—å–∫–∏ '–≥–æ–ª–æ–≤–∏'.")
            for param in model.backbone.parameters():
                param.requires_grad = False
            for param in model.fpn.parameters():
                param.requires_grad = False
            for param in model.class_net.parameters():
                param.requires_grad = True
            for param in model.box_net.parameters():
                param.requires_grad = True
        else:
            print("üî• –£—Å—ñ –≤–∞–≥–∏ –º–æ–¥–µ–ª—ñ —Ä–æ–∑–º–æ—Ä–æ–∂–µ–Ω–æ –¥–ª—è –ø–æ–≤–Ω–æ–≥–æ –Ω–∞–≤—á–∞–Ω–Ω—è.")
            for param in model.parameters():
                param.requires_grad = True

        return model

    def _prepare_dataloaders(self, batch_size, imgsz=None):
        train_img_dir = os.path.join(self.dataset_dir, 'train')
        train_ann_file = os.path.join(self.dataset_dir, 'annotations', 'instances_train.json')
        val_img_dir = os.path.join(self.dataset_dir, 'val')
        val_ann_file = os.path.join(self.dataset_dir, 'annotations', 'instances_val.json')
        temp_dataset = CocoDetection(root=train_img_dir, annFile=train_ann_file)
        coco_cat_ids = sorted(temp_dataset.coco.cats.keys())
        cat_id_to_label = {cat_id: i for i, cat_id in enumerate(coco_cat_ids)}
        num_classes = len(coco_cat_ids)
        
        # –í–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î–º–æ –Ω–∞—à –Ω–æ–≤–∏–π, –≤–±—É–¥–æ–≤–∞–Ω–∏–π –∫–ª–∞—Å DetectionTransforms
        train_dataset = CocoDetection(root=train_img_dir, annFile=train_ann_file,
                                      transforms=DetectionTransforms(is_train=True, cat_id_map=cat_id_to_label, imgsz=imgsz))
        val_dataset = CocoDetection(root=val_img_dir, annFile=val_ann_file,
                                    transforms=DetectionTransforms(is_train=False, cat_id_map=cat_id_to_label, imgsz=imgsz))
        
        train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_fn, num_workers=0, pin_memory=True)
        val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, collate_fn=collate_fn, num_workers=0, pin_memory=True)
        return train_loader, val_loader, num_classes

    def _train_one_epoch(self, model, optimizer, data_loader, device, epoch, writer, global_step,
                     target_lr, warmup_steps, warmup_start_lr):
        model.train()
        progress_bar = tqdm(data_loader, desc=f"Epoch {epoch + 1} [Train]")
        optimizer.zero_grad()

        for i, (images, targets) in enumerate(progress_bar):
            
            # --- –õ–û–ì–Ü–ö–ê –ö–ï–†–£–í–ê–ù–ù–Ø LEARNING RATE ---
            if global_step < warmup_steps:
                lr_scale = global_step / warmup_steps
                new_lr = warmup_start_lr + lr_scale * (target_lr - warmup_start_lr)
                for param_group in optimizer.param_groups:
                    param_group['lr'] = new_lr
            
            elif global_step == warmup_steps:
                print(f"\nüî• Warm-up –∑–∞–≤–µ—Ä—à–µ–Ω–æ. –í—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–æ —Ü—ñ–ª—å–æ–≤–∏–π LR: {target_lr}")
                for param_group in optimizer.param_groups:
                    param_group['lr'] = target_lr
            
            images_tensor = torch.stack(images).to(device)
            boxes = [t['boxes'].to(device) for t in targets]
            cls_ids = [t['labels'].to(device) for t in targets]

            target_for_bench = {
                'bbox': boxes,
                'cls': cls_ids,
                'img_scale': torch.ones(len(images), device=device),
                'img_size': torch.tensor([i.shape[1:] for i in images], device=device)
            }
            loss_dict = model(images_tensor, target_for_bench)
            
            # üí° –î–û–î–ê–ù–û: –í–∏—Ç—è–≥—É—î–º–æ —ñ–Ω–¥–∏–≤—ñ–¥—É–∞–ª—å–Ω—ñ –∑–Ω–∞—á–µ–Ω–Ω—è –≤—Ç—Ä–∞—Ç
            cls_loss = loss_dict['class_loss'].item()
            box_loss = loss_dict['box_loss'].item()
            
            losses = loss_dict['loss']

            if not torch.isfinite(losses):
                print(f"‚ö†Ô∏è –í–∏—è–≤–ª–µ–Ω–æ –Ω–µ—Å–∫—ñ–Ω—á–µ–Ω–Ω–∏–π loss. –ü—Ä–æ–ø—É—Å–∫–∞—î–º–æ –∫—Ä–æ–∫.")
                optimizer.zero_grad()
                continue

            if self.accumulation_steps > 1:
                losses = losses / self.accumulation_steps

            losses.backward()
            
            current_lr = optimizer.param_groups[0]['lr']

            if (i + 1) % self.accumulation_steps == 0 or (i + 1) == len(data_loader):
                # üí° –ó–ú–Ü–ù–ï–ù–û: –û–±—Ä—ñ–∑–∞–Ω–Ω—è –≥—Ä–∞–¥—ñ—î–Ω—Ç—ñ–≤ (max_norm=0.5 –¥–ª—è —Å—Ç–∞–±—ñ–ª—ñ–∑–∞—Ü—ñ—ó)
                torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=0.5) 
                
                optimizer.step()
                optimizer.zero_grad()
                
                display_loss = losses.item() * self.accumulation_steps
                
                writer.add_scalar('Train/Loss_step', display_loss, global_step)
                # üí° –î–û–î–ê–ù–û: –õ–æ–≥—É–≤–∞–Ω–Ω—è Classification Loss
                writer.add_scalar('Train/Classification_Loss', cls_loss, global_step)
                # üí° –î–û–î–ê–ù–û: –õ–æ–≥—É–≤–∞–Ω–Ω—è Box Regression Loss
                writer.add_scalar('Train/Box_Regression_Loss', box_loss, global_step)
                
                writer.add_scalar('LearningRate/Step', current_lr, global_step)
                
                # üí° –ó–ú–Ü–ù–ï–ù–û: –î–æ–¥–∞—î–º–æ —ñ–Ω–¥–∏–≤—ñ–¥—É–∞–ª—å–Ω—ñ –≤—Ç—Ä–∞—Ç–∏ –¥–æ post-fix
                progress_bar.set_postfix(loss=display_loss, cls=cls_loss, box=box_loss, lr=f"{current_lr:.1E}")
                global_step += 1
            else:
                display_loss = losses.item() * self.accumulation_steps
                # üí° –ó–ú–Ü–ù–ï–ù–û: –î–æ–¥–∞—î–º–æ —ñ–Ω–¥–∏–≤—ñ–¥—É–∞–ª—å–Ω—ñ –≤—Ç—Ä–∞—Ç–∏ –¥–æ post-fix
                progress_bar.set_postfix(loss=display_loss, cls=cls_loss, box=box_loss, lr=f"{current_lr:.1E}")

        return global_step

    def _validate_one_epoch(self, model, data_loader, device, imgsz):
        model.eval()
        pred_model = DetBenchPredict(model.model).to(device)
        pred_model.eval()

        metric = MeanAveragePrecision(box_format='xyxy').to(device)
        
        with torch.no_grad():
            progress_bar = tqdm(data_loader, desc="Validating")
            for images, targets in progress_bar:
                images_tensor = torch.stack(images).to(device)
                detections = pred_model(images_tensor)

                preds = []
                for det in detections:
                    preds.append({
                        'boxes': det[:, :4],
                        'scores': det[:, 4],
                        'labels': det[:, 5].int()
                    })
                
                targets_for_metric = [{k: v.to(device) for k, v in t.items()} for t in targets]
                
                metric.update(preds, targets_for_metric)
        try:
            mAP_dict = metric.compute()
            return mAP_dict['map'].item()
        except Exception as e:
            print(f"–ü–æ–º–∏–ª–∫–∞ –ø—Ä–∏ –æ–±—á–∏—Å–ª–µ–Ω–Ω—ñ mAP: {e}")
            return 0.0

    from trainers.FasterRCNNTrainer import FasterRCNNTrainer
    _check_for_resume = FasterRCNNTrainer._check_for_resume_rcnn

    def _load_checkpoint(self, path, model, optimizer, device, lr_scheduler=None):
        checkpoint = torch.load(path, map_location=device)
        model.load_state_dict(checkpoint['model_state_dict'])
        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])
        start_epoch = checkpoint['epoch']
        best_map = checkpoint.get('best_map', 0.0)

        if lr_scheduler and 'lr_scheduler_state_dict' in checkpoint:
            try:
                lr_scheduler.load_state_dict(checkpoint['lr_scheduler_state_dict'])
                print("‚úÖ –°—Ç–∞–Ω –ø–ª–∞–Ω—É–≤–∞–ª—å–Ω–∏–∫–∞ LR —É—Å–ø—ñ—à–Ω–æ –∑–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–æ.")
            except Exception as e:
                print(f"‚ö†Ô∏è –ù–µ –≤–¥–∞–ª–æ—Å—è –∑–∞–≤–∞–Ω—Ç–∞–∂–∏—Ç–∏ —Å—Ç–∞–Ω LR scheduler: {e}. –í–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—é—Ç—å—Å—è –Ω–∞–ª–∞—à—Ç—É–≤–∞–Ω–Ω—è –∑–∞ –∑–∞–º–æ–≤—á—É–≤–∞–Ω–Ω—è–º.")

        return model, optimizer, start_epoch, best_map, lr_scheduler