# trainers/EfficientDet_trainer.py

import os
import datetime as dt
import shutil
import torch
import torch.optim as optim
from torch.utils.data import DataLoader
from torchvision.datasets import CocoDetection
import torchvision.transforms as T
from tqdm import tqdm
from trainers.trainers import BaseTrainer, collate_fn, log_dataset_statistics_to_tensorboard
from torchmetrics.detection import MeanAveragePrecision
from torch.utils.tensorboard import SummaryWriter

# –í–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î–º–æ —Ç–æ–π —Å–∞–º–∏–π –∫–ª–∞—Å —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∞—Ü—ñ–π, —â–æ —ñ –¥–ª—è FCOS/RetinaNet
from trainers.FCOS_trainer import DetectionTransforms

# EfficientDet –≤–∏–º–∞–≥–∞—î —Å—Ç–æ—Ä–æ–Ω–Ω—å–æ—ó –±—ñ–±–ª—ñ–æ—Ç–µ–∫–∏.
# –í—Å—Ç–∞–Ω–æ–≤—ñ—Ç—å —ó—ó –∫–æ–º–∞–Ω–¥–æ—é: pip install effdet
try:
    from effdet import get_efficientdet_config, EfficientDet, DetBenchTrain
    from effdet.efficientdet import HeadNet
except ImportError:
    print("–ü–æ–º–∏–ª–∫–∞: –±—ñ–±–ª—ñ–æ—Ç–µ–∫—É 'effdet' –Ω–µ –∑–Ω–∞–π–¥–µ–Ω–æ.")
    print("–ë—É–¥—å –ª–∞—Å–∫–∞, –≤—Å—Ç–∞–Ω–æ–≤—ñ—Ç—å —ó—ó –∫–æ–º–∞–Ω–¥–æ—é: pip install effdet")
    exit(1)

# –°–ª–æ–≤–Ω–∏–∫ –∑ –∫–æ–Ω—Ñ—ñ–≥—É—Ä–∞—Ü—ñ—è–º–∏ –º–æ–¥–µ–ª–µ–π: –Ω–∞–∑–≤–∞ —Ç–∞ —Ä–µ–∫–æ–º–µ–Ω–¥–æ–≤–∞–Ω–∏–π —Ä–æ–∑–º—ñ—Ä –∑–æ–±—Ä–∞–∂–µ–Ω–Ω—è (—à–∏—Ä–∏–Ω–∞, –≤–∏—Å–æ—Ç–∞)
BACKBONE_CONFIGS = {
    '1': ('tf_efficientdet_d0', (512, 512)),
    '2': ('tf_efficientdet_d1', (640, 640)),
    '3': ('tf_efficientdet_d2', (768, 768)),
    '4': ('tf_efficientdet_d3', (896, 896)),
    '5': ('tf_efficientdet_d4', (1024, 1024)),
    '6': ('tf_efficientdet_d5', (1280, 1280)),
    '7': ('tf_efficientdet_d6', (1536, 1536)),
    '8': ('tf_efficientdet_d7', (1536, 1536)),
}


def _create_model(num_classes, model_name='tf_efficientdet_d0', image_size=(512, 512), pretrained=True):
    """–°—Ç–≤–æ—Ä—é—î –º–æ–¥–µ–ª—å EfficientDet –∑ –∑–∞–¥–∞–Ω–æ—é –∫–æ–Ω—Ñ—ñ–≥—É—Ä–∞—Ü—ñ—î—é."""
    config = get_efficientdet_config(model_name)
    config.num_classes = num_classes
    config.image_size = image_size

    model = EfficientDet(config, pretrained_backbone=pretrained)
    model.class_net = HeadNet(config, num_outputs=num_classes)
    return model

class EfficientDetTrainer(BaseTrainer):
    """–ö–µ—Ä—É—î –ø—Ä–æ—Ü–µ—Å–æ–º –Ω–∞–≤—á–∞–Ω–Ω—è –º–æ–¥–µ–ª—ñ EfficientDet."""

    def __init__(self, training_params, dataset_dir):
        super().__init__(training_params, dataset_dir)
        self.backbone_choice = None
        self.training_mode = None
        self.image_size = None # –ë—É–¥–µ –∑–±–µ—Ä—ñ–≥–∞—Ç–∏ –æ–±—Ä–∞–Ω–∏–π —Ä–æ–∑–º—ñ—Ä –∑–æ–±—Ä–∞–∂–µ–Ω–Ω—è

    def _select_configuration(self):
        """–ó–∞–ø–∏—Ç—É—î —É –∫–æ—Ä–∏—Å—Ç—É–≤–∞—á–∞ backbone —Ç–∞ —Ä–µ–∂–∏–º –Ω–∞–≤—á–∞–Ω–Ω—è."""
        print("\n–ë—É–¥—å –ª–∞—Å–∫–∞, –æ–±–µ—Ä—ñ—Ç—å '—Ö—Ä–µ–±–µ—Ç' (backbone) –¥–ª—è EfficientDet:")
        for key, (name, size) in BACKBONE_CONFIGS.items():
            model_id = name.replace('tf_efficientdet_', '').upper()
            print(f"  {key}: {model_id:<4} (—Ä–µ–∫–æ–º–µ–Ω–¥–æ–≤–∞–Ω–∏–π —Ä–æ–∑–º—ñ—Ä: {size[0]}x{size[1]})")

        while self.backbone_choice is None:
            choice = input(f"–í–∞—à –≤–∏–±—ñ—Ä (1-{len(BACKBONE_CONFIGS)}): ").strip()
            if choice in BACKBONE_CONFIGS:
                self.backbone_choice, self.image_size = BACKBONE_CONFIGS[choice]
                print(f"‚úÖ –í–∏ –æ–±—Ä–∞–ª–∏: {self.backbone_choice} –∑ —Ä–æ–∑–º—ñ—Ä–æ–º –∑–æ–±—Ä–∞–∂–µ–Ω–Ω—è {self.image_size}")
            else:
                print(f"‚ùå –ù–µ–≤—ñ—Ä–Ω–∏–π –≤–∏–±—ñ—Ä. –ë—É–¥—å –ª–∞—Å–∫–∞, –≤–≤–µ–¥—ñ—Ç—å —á–∏—Å–ª–æ –≤—ñ–¥ 1 –¥–æ {len(BACKBONE_CONFIGS)}.")

        print("\n   –û–±–µ—Ä—ñ—Ç—å —Ä–µ–∂–∏–º –Ω–∞–≤—á–∞–Ω–Ω—è:")
        print("     1: Fine-tuning (–Ω–∞–≤—á–∞—Ç–∏ —Ç—ñ–ª—å–∫–∏ '–≥–æ–ª–æ–≤—É', —à–≤–∏–¥—à–µ, —Ä–µ–∫–æ–º–µ–Ω–¥–æ–≤–∞–Ω–æ)")
        print("     2: Full training (–Ω–∞–≤—á–∞—Ç–∏ –≤—Å—é –º–æ–¥–µ–ª—å, –¥–æ–≤—à–µ)")
        while self.training_mode is None:
            sub_choice = input("   –í–∞—à –≤–∏–±—ñ—Ä —Ä–µ–∂–∏–º—É (1 –∞–±–æ 2): ").strip()
            if sub_choice == '1':
                self.training_mode = '_finetune'
                print("‚úÖ –û–±—Ä–∞–Ω–æ —Ä–µ–∂–∏–º: Fine-tuning.")
            elif sub_choice == '2':
                self.training_mode = '_full'
                print("‚úÖ –û–±—Ä–∞–Ω–æ —Ä–µ–∂–∏–º: Full training.")
            else:
                print("   ‚ùå –ù–µ–≤—ñ—Ä–Ω–∏–π –≤–∏–±—ñ—Ä. –ë—É–¥—å –ª–∞—Å–∫–∞, –≤–≤–µ–¥—ñ—Ç—å 1 –∞–±–æ 2.")

    def _get_model_name(self):
        if not self.backbone_choice:
            return "EfficientDet"
        backbone_str = self.backbone_choice.replace('tf_efficientdet_', '').upper()
        mode_str = "Fine-tune" if self.training_mode == '_finetune' else "Full"
        return f"EfficientDet ({backbone_str} {mode_str})"

    def start_or_resume_training(self, dataset_stats):
        if not self.backbone_choice or not self.training_mode:
            self._select_configuration()

        # –í–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î–º–æ —Ä–æ–∑–º—ñ—Ä –∑–æ–±—Ä–∞–∂–µ–Ω–Ω—è, –æ–±—Ä–∞–Ω–∏–π –∫–æ—Ä–∏—Å—Ç—É–≤–∞—á–µ–º
        imgsz = self.image_size
        print(f"\n--- –ó–∞–ø—É—Å–∫ —Ç—Ä–µ–Ω—É–≤–∞–Ω–Ω—è –¥–ª—è {self._get_model_name()} ---")

        device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        print(f"üîå –û–±—Ä–∞–Ω–æ –ø—Ä–∏—Å—Ç—Ä—ñ–π –¥–ª—è –Ω–∞–≤—á–∞–Ω–Ω—è: {str(device).upper()}")

        print(f"üñºÔ∏è –†–æ–∑–º—ñ—Ä –∑–æ–±—Ä–∞–∂–µ–Ω—å –¥–ª—è –Ω–∞–≤—á–∞–Ω–Ω—è –±—É–¥–µ –∑–º—ñ–Ω–µ–Ω–æ –Ω–∞ {imgsz[0]}x{imgsz[1]}.")

        project_dir = os.path.join(self.params.get('project', 'runs/efficientdet'), f"{self.backbone_choice}{self.training_mode}")
        epochs = self.params.get('epochs', 25)
        batch_size = self.params.get('batch', 8)
        learning_rate = self.params.get('lr', 0.0001)
        step_size = self.params.get('lr_scheduler_step_size', 8)
        gamma = self.params.get('lr_scheduler_gamma', 0.1)
        self.accumulation_steps = self.params.get('accumulation_steps', 1)

        train_loader, val_loader, num_classes = self._prepare_dataloaders(batch_size, imgsz=imgsz)
        print(f"üìä –ó–Ω–∞–π–¥–µ–Ω–æ {num_classes} –∫–ª–∞—Å—ñ–≤. –ù–∞–≤—á–∞–Ω–Ω—è –º–æ–¥–µ–ª—ñ –¥–ª—è —ó—Ö —Ä–æ–∑–ø—ñ–∑–Ω–∞–≤–∞–Ω–Ω—è.")

        base_model = self._get_model(num_classes)
        model = DetBenchTrain(base_model, create_labeler=True).to(device)

        optimizer = optim.AdamW(model.parameters(), lr=learning_rate, weight_decay=1e-4)
        lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=step_size, gamma=gamma)

        run_name, checkpoint_path = self._check_for_resume(project_dir)
        start_epoch, best_map, global_step = 0, 0.0, 0

        run_dir = os.path.join(project_dir, run_name)
        os.makedirs(run_dir, exist_ok=True)
        writer = SummaryWriter(log_dir=os.path.join(run_dir, 'tensorboard_logs'))

        print(f"üìÇ –†–µ–∑—É–ª—å—Ç–∞—Ç–∏ –±—É–¥—É—Ç—å –∑–±–µ—Ä–µ–∂–µ–Ω—ñ –≤: {run_dir}")
        print(f"üìà –õ–æ–≥–∏ –¥–ª—è TensorBoard –±—É–¥—É—Ç—å –∑–±–µ—Ä–µ–∂–µ–Ω—ñ –≤: {writer.log_dir}")
        log_dataset_statistics_to_tensorboard(train_loader.dataset, writer)

        if checkpoint_path:
            model.model, optimizer, start_epoch, best_map, lr_scheduler = self._load_checkpoint(
                checkpoint_path, model.model, optimizer, device, lr_scheduler
            )
            print(f"üöÄ –í—ñ–¥–Ω–æ–≤–ª–µ–Ω–Ω—è –Ω–∞–≤—á–∞–Ω–Ω—è –∑ {start_epoch}-—ó –µ–ø–æ—Ö–∏.")

        print(f"\nüöÄ –†–æ–∑–ø–æ—á–∏–Ω–∞—î–º–æ —Ç—Ä–µ–Ω—É–≤–∞–Ω–Ω—è –Ω–∞ {epochs} –µ–ø–æ—Ö...")
        for epoch in range(start_epoch, epochs):
            global_step = self._train_one_epoch(model, optimizer, train_loader, device, epoch, writer, global_step)
            val_map = self._validate_one_epoch(model, val_loader, device, imgsz=imgsz)
            lr_scheduler.step()

            print(f"Epoch {epoch + 1}/{epochs} | Validation mAP: {val_map:.4f} | Current LR: {lr_scheduler.get_last_lr()[0]:.6f}")
            writer.add_scalar('Validation/mAP', val_map, epoch)
            writer.add_scalar('LearningRate/Main', lr_scheduler.get_last_lr()[0], epoch)

            is_best = val_map > best_map
            if is_best:
                best_map = val_map

            self.save_checkpoint({
                'epoch': epoch + 1, 'model_state_dict': model.model.state_dict(),
                'optimizer_state_dict': optimizer.state_dict(), 'best_map': best_map,
                'lr_scheduler_state_dict': lr_scheduler.state_dict()
            }, is_best, run_dir)

        writer.close()
        print("\nüéâ –ù–∞–≤—á–∞–Ω–Ω—è —É—Å–ø—ñ—à–Ω–æ –∑–∞–≤–µ—Ä—à–µ–Ω–æ!")
        # ... (–∫–æ–¥ –¥–ª—è –∑–±–µ—Ä–µ–∂–µ–Ω–Ω—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ñ–≤ —Ç–∞ –ø–æ–≤–µ—Ä–Ω–µ–Ω–Ω—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏) ...
        return {}

    def _get_model(self, num_classes):
        """–ó–∞–≤–∞–Ω—Ç–∞–∂—É—î —Ç–∞ –Ω–∞–ª–∞—à—Ç–æ–≤—É—î –º–æ–¥–µ–ª—å EfficientDet."""
        print(f"üîß –°—Ç–≤–æ—Ä–µ–Ω–Ω—è –º–æ–¥–µ–ª—ñ: {self._get_model_name()}")
        model = _create_model(
            num_classes,
            self.backbone_choice,
            image_size=self.image_size,
            pretrained=True
        )

        if self.training_mode == '_finetune':
            print("‚ùÑÔ∏è –ó–∞–º–æ—Ä–æ–∂—É–≤–∞–Ω–Ω—è backbone —Ç–∞ FPN. –ù–∞–≤—á–∞–Ω–Ω—è —Ç—ñ–ª—å–∫–∏ '–≥–æ–ª–æ–≤–∏'.")
            for param in model.backbone.parameters():
                param.requires_grad = False
            for param in model.fpn.parameters():
                param.requires_grad = False
            for param in model.class_net.parameters():
                param.requires_grad = True
            for param in model.box_net.parameters():
                param.requires_grad = True
        else:
            print("üî• –£—Å—ñ –≤–∞–≥–∏ –º–æ–¥–µ–ª—ñ —Ä–æ–∑–º–æ—Ä–æ–∂–µ–Ω–æ –¥–ª—è –ø–æ–≤–Ω–æ–≥–æ –Ω–∞–≤—á–∞–Ω–Ω—è.")
            for param in model.parameters():
                param.requires_grad = True

        return model

    def _prepare_dataloaders(self, batch_size, imgsz=None):
        train_img_dir = os.path.join(self.dataset_dir, 'train')
        train_ann_file = os.path.join(self.dataset_dir, 'annotations', 'instances_train.json')
        val_img_dir = os.path.join(self.dataset_dir, 'val')
        val_ann_file = os.path.join(self.dataset_dir, 'annotations', 'instances_val.json')
        temp_dataset = CocoDetection(root=train_img_dir, annFile=train_ann_file)
        coco_cat_ids = sorted(temp_dataset.coco.cats.keys())
        cat_id_to_label = {cat_id: i for i, cat_id in enumerate(coco_cat_ids)}
        num_classes = len(coco_cat_ids)
        train_dataset = CocoDetection(root=train_img_dir, annFile=train_ann_file,
                                      transforms=DetectionTransforms(is_train=True, cat_id_map=cat_id_to_label, imgsz=imgsz))
        val_dataset = CocoDetection(root=val_img_dir, annFile=val_ann_file,
                                    transforms=DetectionTransforms(is_train=False, cat_id_map=cat_id_to_label, imgsz=imgsz))
        train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_fn, num_workers=0, pin_memory=True)
        val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, collate_fn=collate_fn, num_workers=0, pin_memory=True)
        return train_loader, val_loader, num_classes

    def _train_one_epoch(self, model, optimizer, data_loader, device, epoch, writer, global_step):
        model.train()
        progress_bar = tqdm(data_loader, desc=f"Epoch {epoch + 1} [Train]")
        optimizer.zero_grad()

        for i, (images, targets) in enumerate(progress_bar):
            # –ü–µ—Ä–µ—Ç–≤–æ—Ä—é—î–º–æ PIL –∑–æ–±—Ä–∞–∂–µ–Ω–Ω—è –≤ —Ç–µ–Ω–∑–æ—Ä–∏ —Ç–∞ –æ–±'—î–¥–Ω—É—î–º–æ –≤ –±–∞—Ç—á
            images_tensor = torch.stack(images).to(device)            
            # –ü–µ—Ä–µ–º—ñ—â—É—î–º–æ –∫–æ–∂–µ–Ω —Ç–µ–Ω–∑–æ—Ä –∑ boxes —Ç–∞ labels –Ω–∞ –ø–æ—Ç—Ä—ñ–±–Ω–∏–π –ø—Ä–∏—Å—Ç—Ä—ñ–π
            boxes = [t['boxes'].to(device) for t in targets]
            cls_ids = [t['labels'].to(device) for t in targets]

            # –°—Ç–≤–æ—Ä—é—î–º–æ —Å–ª–æ–≤–Ω–∏–∫, –æ–¥—Ä–∞–∑—É —Ä–æ–∑–º—ñ—â—É—é—á–∏ –≤—Å—ñ —Ç–µ–Ω–∑–æ—Ä–∏ –Ω–∞ –ø—Ä–∞–≤–∏–ª—å–Ω–æ–º—É –ø—Ä–∏—Å—Ç—Ä–æ—ó
            target_for_bench = {
                'bbox': boxes,
                'cls': cls_ids,
                'img_scale': torch.ones(len(images), device=device),
                'img_size': torch.tensor([i.shape[1:] for i in images], device=device)
            }
            loss_dict = model(images_tensor, target_for_bench)
            losses = loss_dict['loss']

            if not torch.isfinite(losses):
                print(f"‚ö†Ô∏è –í–∏—è–≤–ª–µ–Ω–æ –Ω–µ—Å–∫—ñ–Ω—á–µ–Ω–Ω–∏–π loss. –ü—Ä–æ–ø—É—Å–∫–∞—î–º–æ –∫—Ä–æ–∫.")
                continue

            if self.accumulation_steps > 1:
                losses = losses / self.accumulation_steps

            losses.backward()

            if (i + 1) % self.accumulation_steps == 0 or (i + 1) == len(data_loader):
                optimizer.step()
                optimizer.zero_grad()
                display_loss = losses.item() * self.accumulation_steps if self.accumulation_steps > 1 else losses.item()
                writer.add_scalar('Train/Loss_step', display_loss, global_step)
                global_step += 1
                progress_bar.set_postfix(loss=display_loss)
            else:
                 display_loss = losses.item() * self.accumulation_steps if self.accumulation_steps > 1 else losses.item()
                 progress_bar.set_postfix(loss=display_loss)

        return global_step

    def _validate_one_epoch(self, model, data_loader, device, imgsz):
        model.eval()
        metric = MeanAveragePrecision(box_format='xyxy').to(device)
        
        # –°—Ç–≤–æ—Ä—é—î–º–æ —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∞—Ü—ñ—é –¥–ª—è –≤–∞–ª—ñ–¥–∞—Ü—ñ—ó
        transform = T.Compose([T.Resize(imgsz), T.ToTensor()])

        with torch.no_grad():
            progress_bar = tqdm(data_loader, desc="Validating")
            for images, targets in progress_bar:
                # –û–±'—î–¥–Ω—É—î–º–æ —Ç–µ–Ω–∑–æ—Ä–∏ –∑ DataLoader
                images_tensor = torch.stack(images).to(device)

                # –§–æ—Ä–º—É—î–º–æ —Ü—ñ–ª—ñ (targets) —É —Ñ–æ—Ä–º–∞—Ç—ñ, —è–∫–∏–π –æ—á—ñ–∫—É—î –º–æ–¥–µ–ª—å,
                # –∞–Ω–∞–ª–æ–≥—ñ—á–Ω–æ –¥–æ —Ü–∏–∫–ª—É –Ω–∞–≤—á–∞–Ω–Ω—è.
                boxes = [t['boxes'].to(device) for t in targets]
                cls_ids = [t['labels'].to(device) for t in targets]
                target_for_bench = {
                    'bbox': boxes,
                    'cls': cls_ids,
                    'img_scale': torch.ones(len(images), device=device),
                    'img_size': torch.tensor([i.shape[1:] for i in images], device=device)
                }

                # –¢–µ–ø–µ—Ä –ø–µ—Ä–µ–¥–∞—î–º–æ –≤ –º–æ–¥–µ–ª—å —ñ –∑–æ–±—Ä–∞–∂–µ–Ω–Ω—è, —ñ —Ü—ñ–ª—ñ
                output = model(images_tensor, target_for_bench)
                detections = output['detections']

                # –ö–æ–Ω–≤–µ—Ä—Ç—É—î–º–æ –ø–µ—Ä–µ–¥–±–∞—á–µ–Ω–Ω—è —Ç–∞ ground-truth —É —Ñ–æ—Ä–º–∞—Ç –¥–ª—è torchmetrics
                preds = []
                for det in detections:
                    preds.append({
                        'boxes': det[:, :4],
                        'scores': det[:, 4],
                        'labels': det[:, 5].int()
                    })
                
                # Ground truth –¥–ª—è –º–µ—Ç—Ä–∏–∫–∏
                targets_for_metric = [{k: v.to(device) for k, v in t.items()} for t in targets]
                
                metric.update(preds, targets_for_metric)
        try:
            mAP_dict = metric.compute()
            return mAP_dict['map'].item()
        except Exception as e:
            print(f"–ü–æ–º–∏–ª–∫–∞ –ø—Ä–∏ –æ–±—á–∏—Å–ª–µ–Ω–Ω—ñ mAP: {e}")
            return 0.0

    # –í–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î–º–æ –º–µ—Ç–æ–¥–∏ –∑ FasterRCNNTrainer –¥–ª—è —É–Ω—ñ—Ñ—ñ–∫–∞—Ü—ñ—ó
    from trainers.FasterRCNNTrainer import FasterRCNNTrainer
    _check_for_resume = FasterRCNNTrainer._check_for_resume_rcnn

    def _load_checkpoint(self, path, model, optimizer, device, lr_scheduler=None):
        checkpoint = torch.load(path, map_location=device)
        model.load_state_dict(checkpoint['model_state_dict'])
        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])
        start_epoch = checkpoint['epoch']
        best_map = checkpoint.get('best_map', 0.0)

        if lr_scheduler and 'lr_scheduler_state_dict' in checkpoint:
            lr_scheduler.load_state_dict(checkpoint['lr_scheduler_state_dict'])
            print("‚úÖ –°—Ç–∞–Ω –ø–ª–∞–Ω—É–≤–∞–ª—å–Ω–∏–∫–∞ LR —É—Å–ø—ñ—à–Ω–æ –∑–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–æ.")

        return model, optimizer, start_epoch, best_map, lr_scheduler