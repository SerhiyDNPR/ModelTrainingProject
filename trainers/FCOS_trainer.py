import os
import datetime as dt
import shutil
from glob import glob
import torch
import torch.optim as optim
from torch.utils.data import DataLoader
import torchvision.transforms as T
import torchvision.models as models
from torchvision.datasets import CocoDetection
from tqdm import tqdm
from trainers.trainers import BaseTrainer, collate_fn, log_dataset_statistics_to_tensorboard
from torchmetrics.detection import MeanAveragePrecision
from trainers.FasterRCNNTrainer import FasterRCNNTrainer
from torch.utils.tensorboard import SummaryWriter
from torchvision.models.detection.backbone_utils import BackboneWithFPN

# EfficientNet backbone –≤–∏–º–∞–≥–∞—î –±—ñ–±–ª—ñ–æ—Ç–µ–∫–∏ timm
# –í—Å—Ç–∞–Ω–æ–≤—ñ—Ç—å —ó—ó –∫–æ–º–∞–Ω–¥–æ—é: pip install timm
try:
    import timm
except ImportError:
    print("–ü–æ–ø–µ—Ä–µ–¥–∂–µ–Ω–Ω—è: –±—ñ–±–ª—ñ–æ—Ç–µ–∫—É 'timm' –Ω–µ –∑–Ω–∞–π–¥–µ–Ω–æ. –ë–µ–∫–±–æ–Ω EfficientNet –±—É–¥–µ –Ω–µ–¥–æ—Å—Ç—É–ø–Ω–∏–π.")
    print("–ë—É–¥—å –ª–∞—Å–∫–∞, –≤—Å—Ç–∞–Ω–æ–≤—ñ—Ç—å —ó—ó –∫–æ–º–∞–Ω–¥–æ—é: pip install timm")
    timm = None


# --- –î–æ–ø–æ–º—ñ–∂–Ω–∏–π –∫–ª–∞—Å –¥–ª—è —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∞—Ü—ñ–π ---
class DetectionTransforms:
    def __init__(self, is_train=True, cat_id_map=None, imgsz=None):
        self.cat_id_map = cat_id_map
        transforms_list = []
        if imgsz:
            height, width = imgsz[1], imgsz[0]
            transforms_list.append(T.Resize((height, width)))
        transforms_list.append(T.ToTensor())
        self.transforms = T.Compose(transforms_list)

    def __call__(self, image, target):
        if not target: # –û–±—Ä–æ–±–∫–∞ –∑–æ–±—Ä–∞–∂–µ–Ω—å –±–µ–∑ –∞–Ω–æ—Ç–∞—Ü—ñ–π
            transformed_target = {
                'boxes': torch.empty((0, 4), dtype=torch.float32),
                'labels': torch.empty(0, dtype=torch.int64)
            }
            image = self.transforms(image)
            return image, transformed_target

        boxes = [ann['bbox'] for ann in target]
        boxes = torch.tensor(boxes, dtype=torch.float32)
        # –ü–µ—Ä–µ—Ç–≤–æ—Ä–µ–Ω–Ω—è –∑ [x, y, width, height] –≤ [x1, y1, x2, y2]
        if boxes.numel() > 0:
            boxes[:, 2:] += boxes[:, :2]

        labels = torch.tensor([self.cat_id_map[ann['category_id']] for ann in target], dtype=torch.int64)

        transformed_target = { 'boxes': boxes, 'labels': labels }
        image = self.transforms(image)
        return image, transformed_target

# --- –¢—Ä–µ–Ω–µ—Ä –¥–ª—è FCOS ---
class FCOSTrainer(BaseTrainer):
    """–ö–µ—Ä—É—î –ø—Ä–æ—Ü–µ—Å–æ–º –Ω–∞–≤—á–∞–Ω–Ω—è –º–æ–¥–µ–ª—ñ FCOS –∑ –≤–∏–±–æ—Ä–æ–º —Ä–µ–∂–∏–º—É –Ω–∞–≤—á–∞–Ω–Ω—è."""

    def __init__(self, training_params, dataset_dir):
        super().__init__(training_params, dataset_dir)
        self.training_mode = None # –ó–±–µ—Ä—ñ–≥–∞—Ç–∏–º–µ –æ–±—Ä–∞–Ω–∏–π —Ä–µ–∂–∏–º
        self.backbone_type = None

    def _get_model_name(self):
        """–ü–æ–≤–µ—Ä—Ç–∞—î –ø–æ–≤–Ω—É –Ω–∞–∑–≤—É –º–æ–¥–µ–ª—ñ –¥–ª—è –ª–æ–≥—É–≤–∞–Ω–Ω—è."""
        backbone_map = {'resnet50': 'ResNet-50', 'efficientnet': 'EfficientNet-B0'}
        mode_map = {
            'head_only': 'Fine-tune (Head)',
            'head_fpn': 'Fine-tune (Head+FPN)',
            'full': 'Full Training'
        }
        backbone_str = backbone_map.get(self.backbone_type, "Unknown Backbone")
        mode_str = mode_map.get(self.training_mode, "Unknown Mode")
        return f"FCOS ({backbone_str}) - {mode_str}"

    def _select_configuration(self):
        """–ó–∞–ø–∏—Ç—É—î —É –∫–æ—Ä–∏—Å—Ç—É–≤–∞—á–∞ backbone —Ç–∞ —Ä–µ–∂–∏–º –Ω–∞–≤—á–∞–Ω–Ω—è –¥–ª—è FCOS."""
        print("\n   –û–±–µ—Ä—ñ—Ç—å '—Ö—Ä–µ–±–µ—Ç' (backbone) –¥–ª—è FCOS:")
        print("     1: ResNet-50 (—Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–∏–π, –∑–±–∞–ª–∞–Ω—Å–æ–≤–∞–Ω–∏–π)")
        print("     2: EfficientNet-B0 (–ª–µ–≥–∫–∏–π —Ç–∞ —à–≤–∏–¥–∫–∏–π)")

        while self.backbone_type is None:
            choice = input("   –í–∞—à –≤–∏–±—ñ—Ä backbone (1 –∞–±–æ 2): ").strip()
            if choice == '1':
                self.backbone_type = 'resnet50'
                print("‚úÖ –û–±—Ä–∞–Ω–æ backbone: ResNet-50.")
            elif choice == '2':
                if timm is None:
                    print("‚ùå –ü–æ–º–∏–ª–∫–∞: –±—ñ–±–ª—ñ–æ—Ç–µ–∫–∞ 'timm' –Ω–µ –≤—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∞. –û–±–µ—Ä—ñ—Ç—å —ñ–Ω—à–∏–π backbone.")
                    continue
                self.backbone_type = 'efficientnet'
                print("‚úÖ –û–±—Ä–∞–Ω–æ backbone: EfficientNet-B0.")
            else:
                print("   ‚ùå –ù–µ–≤—ñ—Ä–Ω–∏–π –≤–∏–±—ñ—Ä. –ë—É–¥—å –ª–∞—Å–∫–∞, –≤–≤–µ–¥—ñ—Ç—å 1 –∞–±–æ 2.")

        print("\n   –û–±–µ—Ä—ñ—Ç—å —Ä–µ–∂–∏–º –Ω–∞–≤—á–∞–Ω–Ω—è –¥–ª—è FCOS:")
        print("     1: Fine-tuning (–Ω–∞–≤—á–∞—Ç–∏ —Ç—ñ–ª—å–∫–∏ '–≥–æ–ª–æ–≤—É', –Ω–∞–π—à–≤–∏–¥—à–µ, —Ä–µ–∫–æ–º–µ–Ω–¥–æ–≤–∞–Ω–æ –¥–ª—è —Å—Ç–∞—Ä—Ç—É)")
        # –î–ª—è EfficientNet –∑–∞–º–æ—Ä–æ–∂—É–≤–∞–Ω–Ω—è FPN –æ–∫—Ä–µ–º–æ –Ω–µ –º–∞—î —Å–µ–Ω—Å—É, –±–æ –≤—ñ–Ω —î —á–∞—Å—Ç–∏–Ω–æ—é backbone.
        if self.backbone_type == 'resnet50':
             print("     2: Fine-tuning (–Ω–∞–≤—á–∞—Ç–∏ '–≥–æ–ª–æ–≤—É' —Ç–∞ FPN, –∑–±–∞–ª–∞–Ω—Å–æ–≤–∞–Ω–∏–π –≤–∞—Ä—ñ–∞–Ω—Ç)")
        print("     3: Full training (–Ω–∞–≤—á–∞—Ç–∏ –≤—Å—é –º–æ–¥–µ–ª—å, –Ω–∞–π–¥–æ–≤—à–µ, –ø–æ—Ç–µ–Ω—Ü—ñ–π–Ω–æ –Ω–∞–π–∫—Ä–∞—â–∞ —Ç–æ—á–Ω—ñ—Å—Ç—å)")

        while self.training_mode is None:
            prompt_options = "1, 2 –∞–±–æ 3" if self.backbone_type == 'resnet50' else "1 –∞–±–æ 3"
            choice = input(f"   –í–∞—à –≤–∏–±—ñ—Ä —Ä–µ–∂–∏–º—É ({prompt_options}): ").strip()
            if choice == '1':
                print("‚úÖ –û–±—Ä–∞–Ω–æ —Ä–µ–∂–∏–º: Fine-tuning (—Ç—ñ–ª—å–∫–∏ '–≥–æ–ª–æ–≤–∞').")
                self.training_mode = 'head_only'
            elif choice == '2' and self.backbone_type == 'resnet50':
                print("‚úÖ –û–±—Ä–∞–Ω–æ —Ä–µ–∂–∏–º: Fine-tuning ('–≥–æ–ª–æ–≤–∞' + FPN).")
                self.training_mode = 'head_fpn'
            elif choice == '3':
                print("‚úÖ –û–±—Ä–∞–Ω–æ —Ä–µ–∂–∏–º: Full training (–≤—Å—è –º–æ–¥–µ–ª—å).")
                self.training_mode = 'full'
            else:
                print(f"   ‚ùå –ù–µ–≤—ñ—Ä–Ω–∏–π –≤–∏–±—ñ—Ä. –ë—É–¥—å –ª–∞—Å–∫–∞, –≤–≤–µ–¥—ñ—Ç—å –æ–¥–∏–Ω –∑ –≤–∞—Ä—ñ–∞–Ω—Ç—ñ–≤: {prompt_options}.")

    def start_or_resume_training(self, dataset_stats):
        # –ó–∞–ø–∏—Ç—É—î–º–æ –∫–æ–Ω—Ñ—ñ–≥—É—Ä–∞—Ü—ñ—é, —è–∫—â–æ –≤–æ–Ω–∞ —â–µ –Ω–µ –≤—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∞
        if self.training_mode is None or self.backbone_type is None:
            self._select_configuration()

        imgsz = dataset_stats.get('image_size')
        print(f"\n--- –ó–∞–ø—É—Å–∫ —Ç—Ä–µ–Ω—É–≤–∞–Ω–Ω—è –¥–ª—è {self._get_model_name()} ---")

        device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        print(f"üîå –û–±—Ä–∞–Ω–æ –ø—Ä–∏—Å—Ç—Ä—ñ–π –¥–ª—è –Ω–∞–≤—á–∞–Ω–Ω—è: {str(device).upper()}")

        if imgsz:
            print(f"üñºÔ∏è –†–æ–∑–º—ñ—Ä –∑–æ–±—Ä–∞–∂–µ–Ω—å –¥–ª—è –Ω–∞–≤—á–∞–Ω–Ω—è –±—É–¥–µ –∑–º—ñ–Ω–µ–Ω–æ –Ω–∞ {imgsz[0]}x{imgsz[1]}.")
        else:
            print("‚ö†Ô∏è –†–æ–∑–º—ñ—Ä –∑–æ–±—Ä–∞–∂–µ–Ω–Ω—è (imgsz) –Ω–µ –ø–µ—Ä–µ–¥–∞–Ω–æ, –±—É–¥–µ –≤–∏–∫–æ—Ä–∏—Å—Ç–∞–Ω–æ –æ—Ä–∏–≥—ñ–Ω–∞–ª—å–Ω–∏–π —Ä–æ–∑–º—ñ—Ä.")

        # –ù–∞–∑–≤–∞ –ø—Ä–æ–µ–∫—Ç—É —Ç–µ–ø–µ—Ä –≤–∫–ª—é—á–∞—î backbone —Ç–∞ —Ä–µ–∂–∏–º –Ω–∞–≤—á–∞–Ω–Ω—è
        project_dir = os.path.join(self.params.get('project', 'runs/fcos'), f"{self.backbone_type}_{self.training_mode}")
        epochs = self.params.get('epochs', 25)
        batch_size = self.params.get('batch', 8)
        learning_rate = self.params.get('lr', 0.0001)
        step_size = self.params.get('lr_scheduler_step_size', 8)
        gamma = self.params.get('lr_scheduler_gamma', 0.1)
        self.accumulation_steps = self.params.get('accumulation_steps', 1)

        train_loader, val_loader, num_classes = self._prepare_dataloaders(batch_size, imgsz)
        print(f"üìä –ó–Ω–∞–π–¥–µ–Ω–æ {num_classes} –∫–ª–∞—Å—ñ–≤. –ù–∞–≤—á–∞–Ω–Ω—è –º–æ–¥–µ–ª—ñ –¥–ª—è —ó—Ö —Ä–æ–∑–ø—ñ–∑–Ω–∞–≤–∞–Ω–Ω—è.")

        model = self._get_model(num_classes).to(device)
        optimizer = optim.AdamW(filter(lambda p: p.requires_grad, model.parameters()), lr=learning_rate, weight_decay=1e-4)
        lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=step_size, gamma=gamma)

        run_name, checkpoint_path = self._check_for_resume(project_dir)
        start_epoch, best_map, global_step = 0, 0.0, 0

        run_dir = os.path.join(project_dir, run_name)
        os.makedirs(run_dir, exist_ok=True)
        writer = SummaryWriter(log_dir=os.path.join(run_dir, 'tensorboard_logs'))
        print(f"üìÇ –†–µ–∑—É–ª—å—Ç–∞—Ç–∏ –±—É–¥—É—Ç—å –∑–±–µ—Ä–µ–∂–µ–Ω—ñ –≤: {run_dir}")
        print(f"üìà –õ–æ–≥–∏ –¥–ª—è TensorBoard –±—É–¥—É—Ç—å –∑–±–µ—Ä–µ–∂–µ–Ω—ñ –≤: {writer.log_dir}")

        log_dataset_statistics_to_tensorboard(train_loader.dataset, writer)

        if checkpoint_path:
            model, optimizer, start_epoch, best_map, lr_scheduler = self._load_checkpoint(
                checkpoint_path, model, optimizer, device, lr_scheduler
            )
            print(f"üöÄ –í—ñ–¥–Ω–æ–≤–ª–µ–Ω–Ω—è –Ω–∞–≤—á–∞–Ω–Ω—è –∑ {start_epoch}-—ó –µ–ø–æ—Ö–∏.")

        print(f"\nüöÄ –†–æ–∑–ø–æ—á–∏–Ω–∞—î–º–æ —Ç—Ä–µ–Ω—É–≤–∞–Ω–Ω—è –Ω–∞ {epochs} –µ–ø–æ—Ö...")
        for epoch in range(start_epoch, epochs):
            global_step = self._train_one_epoch(model, optimizer, train_loader, device, epoch, writer, global_step)
            val_map = self._validate_one_epoch(model, val_loader, device)

            lr_scheduler.step()

            print(f"Epoch {epoch + 1}/{epochs} | Validation mAP: {val_map:.4f} | Current LR: {lr_scheduler.get_last_lr()[0]:.6f}")

            writer.add_scalar('Validation/mAP', val_map, epoch)
            writer.add_scalar('LearningRate/Main', lr_scheduler.get_last_lr()[0], epoch)

            is_best = val_map > best_map
            if is_best:
                best_map = val_map

            self._save_checkpoint({
                'epoch': epoch + 1, 'model_state_dict': model.state_dict(),
                'optimizer_state_dict': optimizer.state_dict(), 'best_map': best_map,
                'lr_scheduler_state_dict': lr_scheduler.state_dict()
            }, is_best, run_dir)

        writer.close()
        print("\nüéâ –ù–∞–≤—á–∞–Ω–Ω—è —É—Å–ø—ñ—à–Ω–æ –∑–∞–≤–µ—Ä—à–µ–Ω–æ!")

        best_model_path = os.path.join(run_dir, "best_model.pth")
        final_path = None
        if os.path.exists(best_model_path):
            final_path = f"Final-{self._get_model_name().replace(' - ', '_').replace('(', '').replace(')', '')}-best.pth"
            shutil.copy(best_model_path, final_path)
            print(f"\n‚úÖ –ù–∞–π–∫—Ä–∞—â—É –º–æ–¥–µ–ª—å —Å–∫–æ–ø—ñ–π–æ–≤–∞–Ω–æ —É —Ñ–∞–π–ª: {final_path} (mAP: {best_map:.4f})")

        summary = {
            "model_name": self._get_model_name(),
            "image_count": dataset_stats.get("image_count", "N/A"),
            "negative_count": dataset_stats.get("negative_count", "N/A"),
            "class_count": dataset_stats.get("class_count", num_classes),
            "image_size": dataset_stats.get("image_size", "N/A"),
            "best_map": f"{best_map:.4f}",
            "best_model_path": final_path,
            "hyperparameters": self.params
        }
        return summary

    def _prepare_dataloaders(self, batch_size, imgsz=None):
        """–ì–æ—Ç—É—î –∑–∞–≤–∞–Ω—Ç–∞–∂—É–≤–∞—á—ñ –¥–∞–Ω–∏—Ö –Ω–∞ –æ—Å–Ω–æ–≤—ñ COCO-—Ñ–æ—Ä–º–∞—Ç—É."""
        train_img_dir = os.path.join(self.dataset_dir, 'train')
        train_ann_file = os.path.join(self.dataset_dir, 'annotations', 'instances_train.json')
        val_img_dir = os.path.join(self.dataset_dir, 'val')
        val_ann_file = os.path.join(self.dataset_dir, 'annotations', 'instances_val.json')

        # –í–∏–∑–Ω–∞—á–∞—î–º–æ –∫—ñ–ª—å–∫—ñ—Å—Ç—å –∫–ª–∞—Å—ñ–≤ —ñ —Å—Ç–≤–æ—Ä—é—î–º–æ –º–∞–ø—É
        temp_dataset = CocoDetection(root=train_img_dir, annFile=train_ann_file)
        coco_cat_ids = sorted(temp_dataset.coco.cats.keys())
        # –°—Ç–≤–æ—Ä—é—î–º–æ –º–∞–ø—É: ID –∫–∞—Ç–µ–≥–æ—Ä—ñ—ó COCO -> –Ω–∞—à —ñ–Ω–¥–µ–∫—Å (0, 1, 2...)
        cat_id_to_label = {cat_id: i for i, cat_id in enumerate(coco_cat_ids)}
        num_classes = len(coco_cat_ids)

        train_dataset = CocoDetection(root=train_img_dir, annFile=train_ann_file,
                                      transforms=DetectionTransforms(is_train=True, cat_id_map=cat_id_to_label, imgsz=imgsz))
        val_dataset = CocoDetection(root=val_img_dir, annFile=val_ann_file,
                                    transforms=DetectionTransforms(is_train=False, cat_id_map=cat_id_to_label, imgsz=imgsz))

        # –í–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î–º–æ num_workers=0 –¥–ª—è –∫—Ä–∞—â–æ—ó —Å—É–º—ñ—Å–Ω–æ—Å—Ç—ñ (–æ—Å–æ–±–ª–∏–≤–æ –Ω–∞ Windows)
        train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_fn, num_workers=0, pin_memory=True)
        val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, collate_fn=collate_fn, num_workers=0, pin_memory=True)
        return train_loader, val_loader, num_classes

    def _get_model(self, num_classes):
        """–ó–∞–≤–∞–Ω—Ç–∞–∂—É—î –º–æ–¥–µ–ª—å FCOS, –∞–¥–∞–ø—Ç—É—î —ó—ó –≥–æ–ª–æ–≤—É —Ç–∞ –Ω–∞–ª–∞—à—Ç–æ–≤—É—î –≥—Ä–∞–¥—ñ—î–Ω—Ç–∏."""
        print(f"üîß –°—Ç–≤–æ—Ä–µ–Ω–Ω—è –º–æ–¥–µ–ª—ñ: {self._get_model_name()}")

        if self.backbone_type == 'efficientnet':
            backbone_timm = timm.create_model(
                'efficientnet_b0',
                features_only=True,
                out_indices=(2, 3, 4),
                pretrained=True
            )
            in_channels_list = backbone_timm.feature_info.channels()
            
            # FCOS –æ—á—ñ–∫—É—î 5 –≤–∏—Ö–æ–¥—ñ–≤ –∑ FPN, –∞ efficientnet_b0 –∑ out_indices=(2,3,4) –¥–∞—î 3.
            # –î–ª—è —Å—É–º—ñ—Å–Ω–æ—Å—Ç—ñ, –ø–æ—Ç—Ä—ñ–±–Ω–æ —â–æ–± FPN –≥–µ–Ω–µ—Ä—É–≤–∞–≤ 5 —Ä—ñ–≤–Ω—ñ–≤.
            # –í–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î–º–æ 'extra_blocks' –¥–ª—è —Å—Ç–≤–æ—Ä–µ–Ω–Ω—è –¥–æ–¥–∞—Ç–∫–æ–≤–∏—Ö —Ä—ñ–≤–Ω—ñ–≤ P6, P7.
            backbone = BackboneWithFPN(
                backbone_timm,
                return_layers={'2': '0', '3': '1', '4': '2'}, # –ú–∞–ø—É—î–º–æ —à–∞—Ä–∏ timm –Ω–∞ FPN
                in_channels_list=in_channels_list,
                out_channels=256, # –°—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–∞ –∫—ñ–ª—å–∫—ñ—Å—Ç—å –∫–∞–Ω–∞–ª—ñ–≤ –Ω–∞ –≤–∏—Ö–æ–¥—ñ FPN
                extra_blocks=models.detection.fpn.LastLevelP6P7(256, 256)
            )
            model = models.detection.FCOS(backbone, num_classes=num_classes)
        else: # resnet50
            model = models.detection.fcos_resnet50_fpn(weights=models.detection.FCOS_ResNet50_FPN_Weights.DEFAULT)

        if self.training_mode == 'head_only':
            print("‚ùÑÔ∏è –ó–∞–º–æ—Ä–æ–∂—É–≤–∞–Ω–Ω—è backbone —Ç–∞ FPN. –ù–∞–≤—á–∞–Ω–Ω—è —Ç—ñ–ª—å–∫–∏ '–≥–æ–ª–æ–≤–∏'.")
            for param in model.backbone.parameters():
                param.requires_grad = False
            for param in model.head.parameters():
                param.requires_grad = True

        elif self.training_mode == 'head_fpn' and self.backbone_type == 'resnet50':
            print("‚ùÑÔ∏è –ó–∞–º–æ—Ä–æ–∂—É–≤–∞–Ω–Ω—è backbone. –ù–∞–≤—á–∞–Ω–Ω—è FPN —Ç–∞ '–≥–æ–ª–æ–≤–∏'.")
            for name, param in model.backbone.named_parameters():
                if 'fpn' not in name:
                    param.requires_grad = False
            for param in model.head.parameters():
                param.requires_grad = True

        elif self.training_mode == 'full':
            print("üî• –£—Å—ñ –≤–∞–≥–∏ –º–æ–¥–µ–ª—ñ —Ä–æ–∑–º–æ—Ä–æ–∂–µ–Ω–æ –¥–ª—è –ø–æ–≤–Ω–æ–≥–æ –Ω–∞–≤—á–∞–Ω–Ω—è.")
            for param in model.parameters():
                param.requires_grad = True

        # –ó–∞–º—ñ–Ω–∞ –∫–ª–∞—Å–∏—Ñ—ñ–∫–∞—Ü—ñ–π–Ω–æ–≥–æ —à–∞—Ä—É –≤ –≥–æ–ª–æ–≤—ñ
        in_channels = model.head.classification_head.conv[0].in_channels
        num_anchors = model.head.classification_head.num_anchors # FCOS –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î 1 "–∞–Ω–∫–µ—Ä" –Ω–∞ –ª–æ–∫–∞—Ü—ñ—é
        model.head.classification_head.cls_logits = torch.nn.Conv2d(
            in_channels, num_anchors * num_classes, kernel_size=3, stride=1, padding=1
        )
        # –û–Ω–æ–≤–ª—é—î–º–æ –∫—ñ–ª—å–∫—ñ—Å—Ç—å –∫–ª–∞—Å—ñ–≤ —É –≥–æ–ª–æ–≤—ñ
        model.head.classification_head.num_classes = num_classes

        return model

    def _train_one_epoch(self, model, optimizer, data_loader, device, epoch, writer, global_step):
        model.train()
        progress_bar = tqdm(data_loader, desc=f"Epoch {epoch + 1} [Train]")

        optimizer.zero_grad() # –û—á–∏—â—É—î–º–æ –≥—Ä–∞–¥—ñ—î–Ω—Ç–∏ –Ω–∞ –ø–æ—á–∞—Ç–∫—É –µ–ø–æ—Ö–∏

        for i, (images, targets) in enumerate(progress_bar):
            images = [img.to(device) for img in images]
            targets = [{k: v.to(device) for k, v in t.items()} for t in targets]

            loss_dict = model(images, targets)
            losses = sum(loss for loss in loss_dict.values())

            if not torch.isfinite(losses):
                print(f"‚ö†Ô∏è –í–∏—è–≤–ª–µ–Ω–æ –Ω–µ—Å–∫—ñ–Ω—á–µ–Ω–Ω–∏–π loss –Ω–∞ –µ–ø–æ—Å—ñ {epoch + 1}, –∫—Ä–æ—Ü—ñ {i}. –ü—Ä–æ–ø—É—Å–∫–∞—î–º–æ –∫—Ä–æ–∫. Loss: {losses.item()}")
                continue

            # –õ–æ–≥—ñ–∫–∞ –Ω–∞–∫–æ–ø–∏—á–µ–Ω–Ω—è –≥—Ä–∞–¥—ñ—î–Ω—Ç—ñ–≤
            if self.accumulation_steps > 1:
                losses = losses / self.accumulation_steps

            losses.backward()

            if (i + 1) % self.accumulation_steps == 0 or (i + 1) == len(data_loader):
                optimizer.step()
                optimizer.zero_grad()

                # –õ–æ–≥—É–≤–∞–Ω–Ω—è –≤—Ç—Ä–∞—Ç –ø—ñ—Å–ª—è –∫—Ä–æ–∫—É –æ–ø—Ç–∏–º—ñ–∑–∞—Ç–æ—Ä–∞
                display_loss = losses.item() * self.accumulation_steps if self.accumulation_steps > 1 else losses.item()
                writer.add_scalar('Train/Loss_step', display_loss, global_step)
                global_step += 1

            progress_bar.set_postfix(loss=losses.item())

        return global_step

    def _validate_one_epoch(self, model, data_loader, device):
        model.eval()
        metric = MeanAveragePrecision(box_format='xyxy').to(device)
        with torch.no_grad():
            progress_bar = tqdm(data_loader, desc="Validating")
            for images, targets in progress_bar:
                images = [img.to(device) for img in images]
                targets_for_metric = [{k: v.to(device) for k, v in t.items()} for t in targets]

                predictions = model(images)

                metric.update(predictions, targets_for_metric)
        try:
            mAP_dict = metric.compute()
            return mAP_dict['map'].item()
        except Exception as e:
            print(f"–ü–æ–º–∏–ª–∫–∞ –ø—Ä–∏ –æ–±—á–∏—Å–ª–µ–Ω–Ω—ñ mAP: {e}")
            return 0.0

    # –í–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î–º–æ –º–µ—Ç–æ–¥–∏ –∑ FasterRCNNTrainer –¥–ª—è —É–Ω—ñ—Ñ—ñ–∫–∞—Ü—ñ—ó
    _check_for_resume = FasterRCNNTrainer._check_for_resume_rcnn
    _save_checkpoint = FasterRCNNTrainer._save_checkpoint

    def _load_checkpoint(self, path, model, optimizer, device, lr_scheduler=None):
        checkpoint = torch.load(path, map_location=device)
        model.load_state_dict(checkpoint['model_state_dict'])
        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])
        start_epoch = checkpoint['epoch']
        best_map = checkpoint.get('best_map', 0.0)

        if lr_scheduler and 'lr_scheduler_state_dict' in checkpoint:
            lr_scheduler.load_state_dict(checkpoint['lr_scheduler_state_dict'])
            print("‚úÖ –°—Ç–∞–Ω –ø–ª–∞–Ω—É–≤–∞–ª—å–Ω–∏–∫–∞ LR —É—Å–ø—ñ—à–Ω–æ –∑–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–æ.")

        if lr_scheduler:
            return model, optimizer, start_epoch, best_map, lr_scheduler
        else:
            return model, optimizer, start_epoch, best_map